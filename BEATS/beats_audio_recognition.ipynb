{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Download requirenments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/milad/milad/1/TEZ/5EscModel_beats/2/venv/bin/python\n"
          ]
        }
      ],
      "source": [
        "!which python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'unilm' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/microsoft/unilm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/milad/milad/1/TEZ/5EscModel_beats/2/beats\n"
          ]
        }
      ],
      "source": [
        "!cp -r ./unilm/beats .\n",
        "%cd beats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ry0A9_ZxEfSF",
        "outputId": "6a9c6933-f86a-44c7-f613-eb2598e91563"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "** Resuming transfer from byte position 363147034\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:09 --:--:--     0curl: (6) Could not resolve host: valle.blob.core.windows.net\n"
          ]
        }
      ],
      "source": [
        "!curl -C - -o BEATs_iter3_plus_AS2M_finetuned_on_AS2M_cpt1.pt \"https://valle.blob.core.windows.net/share/BEATs/BEATs_iter3_plus_AS2M_finetuned_on_AS2M_cpt1.pt?sv=2020-08-04&st=2023-03-01T07%3A51%3A05Z&se=2033-03-02T07%3A51%3A00Z&sr=c&sp=rl&sig=QJXmSJG9DbMKf48UDIU1MfzIro8HQOf3sqlNXiflY1I%3D\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "** Resuming transfer from byte position 14675\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0   193    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "curl: (33) HTTP server doesn't seem to support byte ranges. Cannot resume.\n"
          ]
        }
      ],
      "source": [
        "!curl -C - -o class_labels_indices.csv 'http://storage.googleapis.com/us_audioset/youtube_corpus/v1/csv/class_labels_indices.csv'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuFnIzAE7X6E",
        "outputId": "23b3b54b-59cf-4446-d7c2-cf1133eecec9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model loaded!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from BEATs import BEATs, BEATsConfig\n",
        "\n",
        "# Mapping mid to display_name\n",
        "df = pd.read_csv('class_labels_indices.csv')\n",
        "mid_to_display = {mid: display for mid, display in zip(df['mid'], df['display_name'])}\n",
        "\n",
        "# load the fine-tuned checkpoints\n",
        "checkpoint = torch.load('./BEATs_iter3_plus_AS2M_finetuned_on_AS2M_cpt1.pt')\n",
        "\n",
        "cfg = BEATsConfig(checkpoint['cfg'])\n",
        "BEATs_model = BEATs(cfg)\n",
        "BEATs_model.load_state_dict(checkpoint['model'])\n",
        "BEATs_model.eval()\n",
        "\n",
        "print(\"model loaded!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 5 predicted labels of the 0th audio are ['Static', 'Speech', 'White noise', 'Pink noise', 'Waterfall'] with probabilities tensor([0.2092, 0.1707, 0.1452, 0.0852, 0.0729], grad_fn=<UnbindBackward0>)\n",
            "Top 5 predicted labels of the 1th audio are ['White noise', 'Static', 'Speech', 'Spray', 'Pink noise'] with probabilities tensor([0.2516, 0.2028, 0.1648, 0.1044, 0.0888], grad_fn=<UnbindBackward0>)\n",
            "Top 5 predicted labels of the 2th audio are ['Static', 'White noise', 'Speech', 'Noise', 'Pink noise'] with probabilities tensor([0.3231, 0.2448, 0.1637, 0.1039, 0.0542], grad_fn=<UnbindBackward0>)\n",
            "Top 5 predicted labels of the 3th audio are ['Speech', 'White noise', 'Static', 'Pink noise', 'Waterfall'] with probabilities tensor([0.1798, 0.1742, 0.1509, 0.1071, 0.0519], grad_fn=<UnbindBackward0>)\n",
            "Top 5 predicted labels of the 4th audio are ['Static', 'White noise', 'Speech', 'Noise', 'Music'] with probabilities tensor([0.3755, 0.2265, 0.1481, 0.0669, 0.0507], grad_fn=<UnbindBackward0>)\n",
            "Top 5 predicted labels of the 5th audio are ['Static', 'White noise', 'Speech', 'Pink noise', 'Noise'] with probabilities tensor([0.3686, 0.2702, 0.1209, 0.0733, 0.0613], grad_fn=<UnbindBackward0>)\n",
            "Top 5 predicted labels of the 6th audio are ['Static', 'White noise', 'Speech', 'Noise', 'Music'] with probabilities tensor([0.4648, 0.2029, 0.1406, 0.0858, 0.0593], grad_fn=<UnbindBackward0>)\n",
            "Top 5 predicted labels of the 7th audio are ['Static', 'White noise', 'Speech', 'Noise', 'Music'] with probabilities tensor([0.2449, 0.2307, 0.1693, 0.0887, 0.0746], grad_fn=<UnbindBackward0>)\n",
            "Top 5 predicted labels of the 8th audio are ['Speech', 'White noise', 'Static', 'Pink noise', 'Noise'] with probabilities tensor([0.2092, 0.1919, 0.1745, 0.0939, 0.0597], grad_fn=<UnbindBackward0>)\n",
            "Top 5 predicted labels of the 9th audio are ['White noise', 'Speech', 'Static', 'Pink noise', 'Vehicle'] with probabilities tensor([0.1967, 0.1823, 0.1509, 0.1061, 0.0574], grad_fn=<UnbindBackward0>)\n"
          ]
        }
      ],
      "source": [
        "def SED(audio_input_16khz):\n",
        "    padding_mask = torch.zeros(audio_input_16khz.shape[0], audio_input_16khz.shape[1]).bool()\n",
        "    probs = BEATs_model.extract_features(audio_input_16khz, padding_mask=padding_mask)[0]\n",
        "\n",
        "    top5_label_prob_list = []\n",
        "    display_names_list = []\n",
        "\n",
        "    for i, (top5_label_prob, top5_label_idx) in enumerate(zip(*probs.topk(k=5))):\n",
        "        top5_label = [checkpoint['label_dict'][label_idx.item()] for label_idx in top5_label_idx]\n",
        "        display_names = [mid_to_display[mid] for mid in top5_label]\n",
        "\n",
        "        top5_label_prob_list.append(top5_label_prob)\n",
        "        display_names_list.append(display_names)\n",
        "        \n",
        "\n",
        "    return display_names_list, top5_label_prob_list\n",
        "\n",
        "# Example usage\n",
        "audio_input_16khz = torch.randn(10, 10000)  # Example audio input\n",
        "display_names, top5_label_prob = SED(audio_input_16khz)\n",
        "\n",
        "for i, (display_names_i, top5_label_prob_i) in enumerate(zip(display_names, top5_label_prob)):\n",
        "    print(f\"Top 5 predicted labels of the {i}th audio are {display_names_i} with probabilities {top5_label_prob_i}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Random sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoo1-JRCD_H8",
        "outputId": "4d20ed51-ab47-4d76-e0a0-21306439989c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 5 predicted labels of the 0th audio are ['Speech', 'Static', 'White noise', 'Spray', 'Music'] with probabilities tensor([0.2652, 0.2347, 0.1709, 0.0737, 0.0677], grad_fn=<UnbindBackward0>)\n",
            "CPU times: user 679 ms, sys: 9.53 ms, total: 688 ms\n",
            "Wall time: 339 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Example usage\n",
        "audio_input_16khz = torch.randn(1, 10000)  # Example audio input\n",
        "display_names, top5_label_prob = SED(audio_input_16khz)\n",
        "\n",
        "for i, (display_names_i, top5_label_prob_i) in enumerate(zip(display_names, top5_label_prob)):\n",
        "    print(f\"Top 5 predicted labels of the {i}th audio are {display_names_i} with probabilities {top5_label_prob_i}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1-channel sound"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Waveform shape: torch.Size([737280])\n",
            "Sample rate: 44100\n",
            "Matrix shape: torch.Size([16, 44100])\n"
          ]
        }
      ],
      "source": [
        "import torchaudio\n",
        "import torch\n",
        "\n",
        "# Specify the path to the .wav file\n",
        "wav_file_path = '../audio.wav'\n",
        "# Load the .wav file\n",
        "sound, sample_rate = torchaudio.load(wav_file_path)\n",
        "\n",
        "print(\"Waveform shape:\", sound[0].shape)\n",
        "print(\"Sample rate:\", sample_rate)\n",
        "\n",
        "# Calculate the duration of the sound in seconds\n",
        "sound_duration = sound.shape[1] / sample_rate\n",
        "\n",
        "# Define the duration of each chunk in seconds\n",
        "chunk_duration = 1.0  # 1 second\n",
        "\n",
        "# Calculate the number of chunks\n",
        "num_chunks = int(sound_duration / chunk_duration)\n",
        "\n",
        "# Initialize a list to store the chunks\n",
        "sound_chunks = []\n",
        "\n",
        "# Crop the sound into 1-second chunks\n",
        "for i in range(num_chunks):\n",
        "    start_sample = int(i * chunk_duration * sample_rate)\n",
        "    end_sample = int((i + 1) * chunk_duration * sample_rate)\n",
        "    chunk = sound[:, start_sample:end_sample]\n",
        "    sound_chunks.append(chunk.flatten())  # Flatten each chunk into a 1D tensor\n",
        "\n",
        "# Convert the list of flattened chunks to a single tensor\n",
        "sound_matrix = torch.stack(sound_chunks, dim=0)\n",
        "\n",
        "# Print the matrix shape\n",
        "print(\"Matrix shape:\", sound_matrix.shape)\n",
        "\n",
        "# get sound_matrix to the model\n",
        "audio_input_16khz = sound_matrix\n",
        "display_names, top5_label_prob = SED(audio_input_16khz)\n",
        "\n",
        "for i, (display_names_i, top5_label_prob_i) in enumerate(zip(display_names, top5_label_prob)):\n",
        "    print(f\"Top 5 predicted labels of the {i}th audio are {display_names_i} with probabilities {top5_label_prob_i}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2-channel sound"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torchaudio\n",
        "import torch\n",
        "\n",
        "# Specify the path to the .wav file\n",
        "wav_file_path = '../audio.wav'\n",
        "# Load the .wav file\n",
        "sound, sample_rate = torchaudio.load(wav_file_path)\n",
        "sound = sound[0]\n",
        "\n",
        "print(\"Waveform shape:\", sound[0].shape)\n",
        "print(\"Sample rate:\", sample_rate)\n",
        "\n",
        "# Calculate the duration of the sound in seconds\n",
        "sound_duration = sound.shape[1] / sample_rate\n",
        "\n",
        "# Define the duration of each chunk in seconds\n",
        "chunk_duration = 1.0  # 1 second\n",
        "\n",
        "# Calculate the number of chunks\n",
        "num_chunks = int(sound_duration / chunk_duration)\n",
        "\n",
        "# Initialize a list to store the chunks\n",
        "sound_chunks = []\n",
        "\n",
        "# Crop the sound into 1-second chunks\n",
        "for i in range(num_chunks):\n",
        "    start_sample = int(i * chunk_duration * sample_rate)\n",
        "    end_sample = int((i + 1) * chunk_duration * sample_rate)\n",
        "    chunk = sound[:, start_sample:end_sample]\n",
        "    sound_chunks.append(chunk.flatten())  # Flatten each chunk into a 1D tensor\n",
        "\n",
        "# Convert the list of flattened chunks to a single tensor\n",
        "sound_matrix = torch.stack(sound_chunks, dim=0)\n",
        "\n",
        "# Print the matrix shape\n",
        "print(\"Matrix shape:\", sound_matrix.shape)\n",
        "\n",
        "# get sound_matrix to the model\n",
        "audio_input_16khz = sound_matrix\n",
        "display_names, top5_label_prob = SED(audio_input_16khz)\n",
        "\n",
        "for i, (display_names_i, top5_label_prob_i) in enumerate(zip(display_names, top5_label_prob)):\n",
        "    print(f\"Top 5 predicted labels of the {i}th audio are {display_names_i} with probabilities {top5_label_prob_i}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Change samplerate to 16000Hz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Resample and save the sound"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original waveform shape: torch.Size([737280])\n",
            "Original sample rate: 44100\n",
            "Resampled waveform shape: torch.Size([267494])\n",
            "Target sample rate: 16000\n",
            "Resampled audio saved to: ../resampled_audio.wav\n"
          ]
        }
      ],
      "source": [
        "import torchaudio\n",
        "import torch\n",
        "\n",
        "# Specify the path to the .wav file\n",
        "wav_file_path = '../audio.wav'\n",
        "\n",
        "# Load the .wav file\n",
        "sound, sample_rate = torchaudio.load(wav_file_path)\n",
        "\n",
        "# Define the target sample rate\n",
        "target_sample_rate = 16000\n",
        "\n",
        "# Resample the sound to the target sample rate\n",
        "resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=target_sample_rate)\n",
        "resampled_sound = resampler(sound)\n",
        "\n",
        "print(\"Original waveform shape:\", sound[0].shape)\n",
        "print(\"Original sample rate:\", sample_rate)\n",
        "\n",
        "print(\"Resampled waveform shape:\", resampled_sound[0].shape)\n",
        "print(\"Target sample rate:\", target_sample_rate)\n",
        "\n",
        "# Specify the path to save the resampled audio\n",
        "output_file_path = '../resampled_audio.wav'\n",
        "\n",
        "# Save the resampled audio\n",
        "torchaudio.save(output_file_path, resampled_sound, sample_rate=target_sample_rate)\n",
        "\n",
        "print(\"Resampled audio saved to:\", output_file_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Resample and send all chunks to the SED function at same time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Matrix shape: torch.Size([16, 16000])\n",
            "Top 5 predicted labels of the 0th audio are ['Music', 'Speech', 'Mantra', 'Chant', 'Musical instrument'] with probabilities tensor([0.3998, 0.3969, 0.2667, 0.0588, 0.0467], grad_fn=<UnbindBackward0>)\n",
            "Top 5 predicted labels of the 1th audio are ['Speech', 'Music', 'Mantra', 'Inside, small room', 'Musical instrument'] with probabilities tensor([0.4652, 0.2637, 0.0779, 0.0390, 0.0387], grad_fn=<UnbindBackward0>)\n",
            "Top 5 predicted labels of the 2th audio are ['Speech', 'Music', 'Vehicle', 'Gunshot, gunfire', 'Musical instrument'] with probabilities tensor([0.6422, 0.1730, 0.0736, 0.0572, 0.0504], grad_fn=<UnbindBackward0>)\n",
            "Top 5 predicted labels of the 3th audio are ['Speech', 'Music', 'Inside, small room', 'Vehicle', 'Gunshot, gunfire'] with probabilities tensor([0.6924, 0.1111, 0.0532, 0.0487, 0.0400], grad_fn=<UnbindBackward0>)\n",
            "Top 5 predicted labels of the 4th audio are ['Speech', 'Music', 'Inside, small room', 'Vehicle', 'Gunshot, gunfire'] with probabilities tensor([0.6211, 0.1457, 0.0539, 0.0523, 0.0507], grad_fn=<UnbindBackward0>)\n",
            "Top 5 predicted labels of the 5th audio are ['Speech', 'Music', 'Musical instrument', 'Drum', 'Inside, small room'] with probabilities tensor([0.5132, 0.2966, 0.0850, 0.0448, 0.0389], grad_fn=<UnbindBackward0>)\n",
            "Top 5 predicted labels of the 6th audio are ['Speech', 'Machine gun', 'Gunshot, gunfire', 'Music', 'Silence'] with probabilities tensor([0.3139, 0.1294, 0.1285, 0.1280, 0.0484], grad_fn=<UnbindBackward0>)\n",
            "Top 5 predicted labels of the 7th audio are ['Speech', 'Music', 'Silence', 'Inside, small room', 'Musical instrument'] with probabilities tensor([0.2653, 0.2148, 0.0718, 0.0362, 0.0290], grad_fn=<UnbindBackward0>)\n",
            "Top 5 predicted labels of the 8th audio are ['Speech', 'Music', 'Musical instrument', 'Inside, small room', 'Vehicle'] with probabilities tensor([0.3320, 0.1790, 0.0734, 0.0511, 0.0396], grad_fn=<UnbindBackward0>)\n",
            "Top 5 predicted labels of the 9th audio are ['Whistling', 'Speech', 'Music', 'Whistle', 'Musical instrument'] with probabilities tensor([0.3398, 0.3024, 0.1499, 0.0729, 0.0324], grad_fn=<UnbindBackward0>)\n",
            "Top 5 predicted labels of the 10th audio are ['Speech', 'Whistling', 'Music', 'Whistle', 'Outside, rural or natural'] with probabilities tensor([0.2339, 0.2049, 0.1597, 0.0746, 0.0479], grad_fn=<UnbindBackward0>)\n",
            "Top 5 predicted labels of the 11th audio are ['Speech', 'Music', 'Whistling', 'Vehicle', 'Car alarm'] with probabilities tensor([0.2323, 0.1021, 0.0598, 0.0583, 0.0486], grad_fn=<UnbindBackward0>)\n",
            "Top 5 predicted labels of the 12th audio are ['Speech', 'Music', 'Inside, small room', 'Vehicle', 'Animal'] with probabilities tensor([0.3615, 0.1144, 0.0649, 0.0421, 0.0386], grad_fn=<UnbindBackward0>)\n",
            "Top 5 predicted labels of the 13th audio are ['Speech', 'Music', 'Inside, small room', 'Gunshot, gunfire', 'Musical instrument'] with probabilities tensor([0.6510, 0.0982, 0.0554, 0.0323, 0.0256], grad_fn=<UnbindBackward0>)\n",
            "Top 5 predicted labels of the 14th audio are ['Speech', 'Music', 'Gunshot, gunfire', 'Artillery fire', 'Thunder'] with probabilities tensor([0.6303, 0.1387, 0.1062, 0.0754, 0.0435], grad_fn=<UnbindBackward0>)\n",
            "Top 5 predicted labels of the 15th audio are ['Speech', 'Music', 'Inside, small room', 'Musical instrument', 'Domestic animals, pets'] with probabilities tensor([0.4326, 0.1500, 0.0758, 0.0475, 0.0330], grad_fn=<UnbindBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torchaudio\n",
        "import torch\n",
        "\n",
        "# Specify the path to the .wav file\n",
        "wav_file_path = '../audio.wav'\n",
        "# Load the .wav file\n",
        "sound, sample_rate = torchaudio.load(wav_file_path)\n",
        "\n",
        "# Define the target sample rate\n",
        "target_sample_rate = 16000\n",
        "\n",
        "# Resample the sound to the target sample rate\n",
        "resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=target_sample_rate)\n",
        "sound = resampler(sound)\n",
        "\n",
        "# Calculate the duration of the sound in seconds\n",
        "sound_duration = sound.shape[1] / target_sample_rate\n",
        "\n",
        "# Define the duration of each chunk in seconds\n",
        "chunk_duration = 1.0  # 1 second\n",
        "\n",
        "# Calculate the number of chunks\n",
        "num_chunks = int(sound_duration / chunk_duration)\n",
        "\n",
        "# Initialize a list to store the chunks\n",
        "sound_chunks = []\n",
        "\n",
        "# Crop the sound into 1-second chunks\n",
        "for i in range(num_chunks):\n",
        "    start_sample = int(i * chunk_duration * target_sample_rate)\n",
        "    end_sample = int((i + 1) * chunk_duration * target_sample_rate)\n",
        "    chunk = sound[:, start_sample:end_sample]\n",
        "    sound_chunks.append(chunk.flatten())  # Flatten each chunk into a 1D tensor\n",
        "\n",
        "# Convert the list of flattened chunks to a single tensor\n",
        "sound_matrix = torch.stack(sound_chunks, dim=0)\n",
        "\n",
        "# Print the matrix shape\n",
        "print(\"Matrix shape:\", sound_matrix.shape)\n",
        "\n",
        "# get sound_matrix to the model\n",
        "audio_input_16khz = sound_matrix\n",
        "display_names, top5_label_prob = SED(audio_input_16khz)\n",
        "\n",
        "for i, (display_names_i, top5_label_prob_i) in enumerate(zip(display_names, top5_label_prob)):\n",
        "    print(f\"Top 5 predicted labels of the {i}th audio are {display_names_i} with probabilities {top5_label_prob_i}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Resample and send each chunk to the SED function one after the other"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 5 predicted labels of the 0th audio are [['Music', 'Speech', 'Mantra', 'Chant', 'Musical instrument']] with probabilities [tensor([0.3998, 0.3969, 0.2667, 0.0588, 0.0467], grad_fn=<UnbindBackward0>)]\n",
            "Chunk 1 saved to: ../chunked_audio/chunk_1.wav\n",
            "Top 5 predicted labels of the 1th audio are [['Speech', 'Music', 'Mantra', 'Inside, small room', 'Musical instrument']] with probabilities [tensor([0.4652, 0.2637, 0.0779, 0.0390, 0.0387], grad_fn=<UnbindBackward0>)]\n",
            "Chunk 2 saved to: ../chunked_audio/chunk_2.wav\n",
            "Top 5 predicted labels of the 2th audio are [['Speech', 'Music', 'Vehicle', 'Gunshot, gunfire', 'Musical instrument']] with probabilities [tensor([0.6422, 0.1730, 0.0736, 0.0572, 0.0504], grad_fn=<UnbindBackward0>)]\n",
            "Chunk 3 saved to: ../chunked_audio/chunk_3.wav\n",
            "Top 5 predicted labels of the 3th audio are [['Speech', 'Music', 'Inside, small room', 'Vehicle', 'Gunshot, gunfire']] with probabilities [tensor([0.6924, 0.1111, 0.0532, 0.0487, 0.0400], grad_fn=<UnbindBackward0>)]\n",
            "Chunk 4 saved to: ../chunked_audio/chunk_4.wav\n",
            "Top 5 predicted labels of the 4th audio are [['Speech', 'Music', 'Inside, small room', 'Vehicle', 'Gunshot, gunfire']] with probabilities [tensor([0.6211, 0.1457, 0.0539, 0.0523, 0.0507], grad_fn=<UnbindBackward0>)]\n",
            "Chunk 5 saved to: ../chunked_audio/chunk_5.wav\n",
            "Top 5 predicted labels of the 5th audio are [['Speech', 'Music', 'Musical instrument', 'Drum', 'Inside, small room']] with probabilities [tensor([0.5132, 0.2966, 0.0850, 0.0448, 0.0389], grad_fn=<UnbindBackward0>)]\n",
            "Chunk 6 saved to: ../chunked_audio/chunk_6.wav\n",
            "Top 5 predicted labels of the 6th audio are [['Speech', 'Machine gun', 'Gunshot, gunfire', 'Music', 'Silence']] with probabilities [tensor([0.3139, 0.1294, 0.1285, 0.1280, 0.0484], grad_fn=<UnbindBackward0>)]\n",
            "Chunk 7 saved to: ../chunked_audio/chunk_7.wav\n",
            "Top 5 predicted labels of the 7th audio are [['Speech', 'Music', 'Silence', 'Inside, small room', 'Musical instrument']] with probabilities [tensor([0.2653, 0.2148, 0.0718, 0.0362, 0.0290], grad_fn=<UnbindBackward0>)]\n",
            "Chunk 8 saved to: ../chunked_audio/chunk_8.wav\n",
            "Top 5 predicted labels of the 8th audio are [['Speech', 'Music', 'Musical instrument', 'Inside, small room', 'Vehicle']] with probabilities [tensor([0.3320, 0.1790, 0.0734, 0.0511, 0.0396], grad_fn=<UnbindBackward0>)]\n",
            "Chunk 9 saved to: ../chunked_audio/chunk_9.wav\n",
            "Top 5 predicted labels of the 9th audio are [['Whistling', 'Speech', 'Music', 'Whistle', 'Musical instrument']] with probabilities [tensor([0.3398, 0.3024, 0.1499, 0.0729, 0.0324], grad_fn=<UnbindBackward0>)]\n",
            "Chunk 10 saved to: ../chunked_audio/chunk_10.wav\n",
            "Top 5 predicted labels of the 10th audio are [['Speech', 'Whistling', 'Music', 'Whistle', 'Outside, rural or natural']] with probabilities [tensor([0.2339, 0.2049, 0.1597, 0.0746, 0.0479], grad_fn=<UnbindBackward0>)]\n",
            "Chunk 11 saved to: ../chunked_audio/chunk_11.wav\n",
            "Top 5 predicted labels of the 11th audio are [['Speech', 'Music', 'Whistling', 'Vehicle', 'Car alarm']] with probabilities [tensor([0.2323, 0.1021, 0.0598, 0.0583, 0.0486], grad_fn=<UnbindBackward0>)]\n",
            "Chunk 12 saved to: ../chunked_audio/chunk_12.wav\n",
            "Top 5 predicted labels of the 12th audio are [['Speech', 'Music', 'Inside, small room', 'Vehicle', 'Animal']] with probabilities [tensor([0.3615, 0.1144, 0.0649, 0.0421, 0.0386], grad_fn=<UnbindBackward0>)]\n",
            "Chunk 13 saved to: ../chunked_audio/chunk_13.wav\n",
            "Top 5 predicted labels of the 13th audio are [['Speech', 'Music', 'Inside, small room', 'Gunshot, gunfire', 'Musical instrument']] with probabilities [tensor([0.6510, 0.0982, 0.0554, 0.0323, 0.0256], grad_fn=<UnbindBackward0>)]\n",
            "Chunk 14 saved to: ../chunked_audio/chunk_14.wav\n",
            "Top 5 predicted labels of the 14th audio are [['Speech', 'Music', 'Gunshot, gunfire', 'Artillery fire', 'Thunder']] with probabilities [tensor([0.6303, 0.1387, 0.1062, 0.0754, 0.0435], grad_fn=<UnbindBackward0>)]\n",
            "Chunk 15 saved to: ../chunked_audio/chunk_15.wav\n",
            "Top 5 predicted labels of the 15th audio are [['Speech', 'Music', 'Inside, small room', 'Musical instrument', 'Domestic animals, pets']] with probabilities [tensor([0.4326, 0.1500, 0.0758, 0.0475, 0.0330], grad_fn=<UnbindBackward0>)]\n",
            "Chunk 16 saved to: ../chunked_audio/chunk_16.wav\n"
          ]
        }
      ],
      "source": [
        "import torchaudio\n",
        "import torch\n",
        "\n",
        "# Specify the path to the .wav file\n",
        "wav_file_path = '../audio.wav'\n",
        "\n",
        "# Load the .wav file\n",
        "sound, sample_rate = torchaudio.load(wav_file_path)\n",
        "\n",
        "# Define the target sample rate\n",
        "target_sample_rate = 16000\n",
        "\n",
        "# Resample the sound to the target sample rate\n",
        "resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=target_sample_rate)\n",
        "sound = resampler(sound)\n",
        "\n",
        "# Calculate the duration of the sound in seconds\n",
        "sound_duration = sound.shape[1] / target_sample_rate\n",
        "\n",
        "# Define the duration of each chunk in seconds\n",
        "chunk_duration = 1.0  # 1 second\n",
        "\n",
        "# Calculate the number of chunks\n",
        "num_chunks = int(sound_duration / chunk_duration)\n",
        "\n",
        "# Specify the directory to save the chunked audio files\n",
        "output_directory = '../chunked_audio/'\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "import os\n",
        "os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "# Crop the sound into 1-second chunks and save each chunk\n",
        "for i in range(num_chunks):\n",
        "    start_sample = int(i * chunk_duration * target_sample_rate)\n",
        "    end_sample = int((i + 1) * chunk_duration * target_sample_rate)\n",
        "    chunk = sound[:, start_sample:end_sample]\n",
        "\n",
        "    # get sound_matrix to the model\n",
        "    audio_input_16khz = chunk\n",
        "    display_names, top5_label_prob = SED(audio_input_16khz)\n",
        "    print(f\"Top 5 predicted labels of the {i}th audio are {display_names} with probabilities {top5_label_prob}\")\n",
        "    \n",
        "    # Create a file name for the chunk\n",
        "    output_file_path = os.path.join(output_directory, f'chunk_{i+1}.wav')\n",
        "    \n",
        "    # Save the chunk as a separate audio file\n",
        "    torchaudio.save(output_file_path, chunk, sample_rate=target_sample_rate)\n",
        "\n",
        "    print(f\"Chunk {i+1} saved to:\", output_file_path)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
