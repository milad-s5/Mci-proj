{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fXpFjiht0bg"
      },
      "source": [
        "# Login to Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mRXsDffet4nW"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bikP354nuWkx"
      },
      "outputs": [],
      "source": [
        "%cd drive/MyDrive"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Teacher Student model"
      ],
      "metadata": {
        "id": "AgFynumQp9U1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## codes"
      ],
      "metadata": {
        "id": "lKNopN2H43uT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython.display as display\n",
        "\n",
        "import glob\n",
        "from collections import Counter\n",
        "\n",
        "import math\n",
        "import pandas as pd\n",
        "\n",
        "import librosa\n",
        "import librosa.display\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import torch\n",
        "import torchaudio\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import soundfile as sf\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import models, transforms\n",
        "\n",
        "import tensorflow as tf\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import datetime\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "\n",
        "def model_size(model):\n",
        "    param_size = 0\n",
        "    for param in model.parameters():\n",
        "        param_size += param.nelement() * param.element_size()\n",
        "    buffer_size = 0\n",
        "    for buffer in model.buffers():\n",
        "        buffer_size += buffer.nelement() * buffer.element_size()\n",
        "\n",
        "    size_all_mb = (param_size + buffer_size) / 1024**2\n",
        "    # print('model size: {:.3f}MB'.format(size_all_mb))\n",
        "    return size_all_mb\n",
        "\n",
        "def evaluate(model, test_loader, device=\"cpu\"):\n",
        "    model.eval()\n",
        "    num_correct = 0\n",
        "    num_examples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            inputs, targets = batch\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "            output = model(inputs)\n",
        "            correct = torch.eq(torch.max(F.softmax(output, dim=1), dim=1)[1], targets).view(-1)\n",
        "            num_correct += torch.sum(correct).item()\n",
        "            num_examples += correct.shape[0]\n",
        "\n",
        "    accuracy = num_correct / num_examples\n",
        "    return accuracy\n",
        "\n",
        "class FrequencyMask(object):\n",
        "    \"\"\"\n",
        "      Example:\n",
        "        >>> transforms.Compose([\n",
        "        >>>     transforms.ToTensor(),\n",
        "        >>>     FrequencyMask(max_width=10, use_mean=False),\n",
        "        >>> ])\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, max_width, use_mean=True):\n",
        "        self.max_width = max_width\n",
        "        self.use_mean = use_mean\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            tensor (Tensor): Tensor image of\n",
        "            size (C, H, W) where the frequency\n",
        "            mask is to be applied.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Transformed image with Frequency Mask.\n",
        "        \"\"\"\n",
        "        start = random.randrange(0, tensor.shape[2])\n",
        "        end = start + random.randrange(1, self.max_width)\n",
        "        if self.use_mean:\n",
        "            tensor[:, start:end, :] = tensor.mean()\n",
        "        else:\n",
        "            tensor[:, start:end, :] = 0\n",
        "        return tensor\n",
        "\n",
        "    def __repr__(self):\n",
        "        format_string = self.__class__.__name__ + \"(max_width=\"\n",
        "        format_string += str(self.max_width) + \")\"\n",
        "        format_string += 'use_mean=' + (str(self.use_mean) + ')')\n",
        "\n",
        "        return format_string\n",
        "\n",
        "\n",
        "class TimeMask(object):\n",
        "    \"\"\"\n",
        "      Example:\n",
        "        >>> transforms.Compose([\n",
        "        >>>     transforms.ToTensor(),\n",
        "        >>>     TimeMask(max_width=10, use_mean=False),\n",
        "        >>> ])\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, max_width, use_mean=True):\n",
        "        self.max_width = max_width\n",
        "        self.use_mean = use_mean\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            tensor (Tensor): Tensor image of\n",
        "            size (C, H, W) where the time mask\n",
        "            is to be applied.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Transformed image with Time Mask.\n",
        "        \"\"\"\n",
        "        start = random.randrange(0, tensor.shape[1])\n",
        "        end = start + random.randrange(0, self.max_width)\n",
        "        if self.use_mean:\n",
        "            tensor[:, :, start:end] = tensor.mean()\n",
        "        else:\n",
        "            tensor[:, :, start:end] = 0\n",
        "        return tensor\n",
        "\n",
        "    def __repr__(self):\n",
        "        format_string = self.__class__.__name__ + \"(max_width=\"\n",
        "        format_string += str(self.max_width) + \")\"\n",
        "        format_string += 'use_mean=' + (str(self.use_mean) + ')')\n",
        "        return format_string\n",
        "\n",
        "\n",
        "class PrecomputedESC50(Dataset):\n",
        "    def __init__(self,path, max_freqmask_width, max_timemask_width, use_mean=True, dpi=50):\n",
        "        files = Path(path).glob('*.png')\n",
        "        self.items = [(f,int(f.name.split(\"-\")[-1].replace(\".wav.png\",\"\"))) for f in files]\n",
        "        self.length = len(self.items)\n",
        "        self.max_freqmask_width = max_freqmask_width\n",
        "        self.max_timemask_width = max_timemask_width\n",
        "        self.use_mean = use_mean\n",
        "        self.img_transforms = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225]),\n",
        "            transforms.RandomApply([FrequencyMask(self.max_freqmask_width, self.use_mean)], p=0.5),\n",
        "            transforms.RandomApply([TimeMask(self.max_timemask_width, self.use_mean)], p=0.5)])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        filename, label = self.items[index]\n",
        "        img = Image.open(filename).convert('RGB')\n",
        "        return (self.img_transforms(img), label)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "# Define a function to plot and log confusion matrix to TensorBoard\n",
        "def plot_confusion_matrix(model, test_loader, device=\"cpu\"):\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            inputs, targets = batch\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "            output = model(inputs)\n",
        "            predictions = torch.max(F.softmax(output, dim=1), dim=1)[1].cpu().numpy()\n",
        "            all_predictions.extend(predictions)\n",
        "            all_labels.extend(targets.cpu().numpy())\n",
        "\n",
        "    # Generate confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "\n",
        "    # Create a heatmap of the confusion matrix\n",
        "    plt.figure(figsize=(20, 16))\n",
        "    sn.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=True, yticklabels=True)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Confusion Matrix')\n",
        "\n",
        "    figure = plt.gcf()\n",
        "    return figure\n",
        "\n",
        "# Define a function to log predictions vs. actuals as images to TensorBoard\n",
        "def log_predictions_vs_actuals(model, data_loader, device=\"cpu\", num_batches=5):\n",
        "    model.eval()\n",
        "\n",
        "    batch_counter = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            if batch_counter >= num_batches:\n",
        "                break\n",
        "\n",
        "            inputs, targets = batch\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "            output = model(inputs)\n",
        "            predictions = torch.max(F.softmax(output, dim=1), dim=1)\n",
        "            predicted_labels = predictions[1]\n",
        "            probabilities = predictions[0]\n",
        "\n",
        "            # Convert PyTorch tensors to NumPy arrays\n",
        "            inputs_np = inputs.permute(0, 2, 3, 1).cpu().numpy()\n",
        "\n",
        "            # Create a figure for each batch\n",
        "            fig, axes = plt.subplots(nrows=4, ncols=4, figsize=(12, 12))\n",
        "\n",
        "            for i, ax in enumerate(axes.flat):\n",
        "                ax.imshow(inputs_np[i])\n",
        "                ax.axis(\"off\")\n",
        "\n",
        "                actual_label = targets[i].item()\n",
        "                predicted_label = predicted_labels[i].item()\n",
        "                probability = probabilities[i].item()\n",
        "\n",
        "                # Color the title based on correctness\n",
        "                title_color = 'green' if actual_label == predicted_label else 'red'\n",
        "\n",
        "                ax.set_title(f\"Actual: {actual_label}\\nPredicted: {predicted_label}\\nProb: {probability:.2f}\", color=title_color)\n",
        "\n",
        "            plt.tight_layout()\n",
        "            batch_counter += 1\n",
        "    return fig\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience, verbose=False):\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_valid_accuracy = 0.0\n",
        "        self.early_stop = False\n",
        "\n",
        "    def step(self, valid_accuracy):\n",
        "        if valid_accuracy > self.best_valid_accuracy:\n",
        "            self.best_valid_accuracy = valid_accuracy\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter > self.patience:\n",
        "                self.early_stop = True\n",
        "                if self.verbose:\n",
        "                    print(\"Early stopping activated.\")\n",
        "        return self.early_stop\n",
        "\n",
        "class LearningRateScheduler(lr_scheduler._LRScheduler):\n",
        "    def __init__(self, optimizer, patience, factor=0.1, verbose=False):\n",
        "        self.optimizer = optimizer\n",
        "        self.patience = patience\n",
        "        self.factor = factor\n",
        "        self.verbose = verbose\n",
        "        self.lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, patience=self.patience, factor=self.factor, verbose=self.verbose)\n",
        "\n",
        "    def step(self, valid_accuracy):\n",
        "        self.lr_scheduler.step(valid_accuracy)\n",
        "        return self.optimizer.param_groups[0]['lr']\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "PATH_ESC50_TRAIN=\"./train1/\"\n",
        "PATH_ESC50_VALID=\"./valid1/\"\n",
        "PATH_ESC50_TEST=\"./test/\"\n",
        "\n",
        "bs=16\n",
        "esc50pre_train = PrecomputedESC50(PATH_ESC50_TRAIN, max_freqmask_width=10, max_timemask_width=10 )\n",
        "esc50pre_valid = PrecomputedESC50(PATH_ESC50_VALID,max_freqmask_width=10, max_timemask_width=10 )\n",
        "esc50pre_test = PrecomputedESC50(PATH_ESC50_TEST,max_freqmask_width=10, max_timemask_width=10 )\n",
        "\n",
        "esc50_train_loader = torch.utils.data.DataLoader(esc50pre_train, bs, shuffle=True)\n",
        "esc50_val_loader = torch.utils.data.DataLoader(esc50pre_valid, bs, shuffle=True)\n",
        "esc50_test_loader = torch.utils.data.DataLoader(esc50pre_test, bs, shuffle=True)"
      ],
      "metadata": {
        "id": "4uQ8iGLFqA7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate model latency (replace with your actual latency calculation)\n",
        "def estimate_latency(model, device=\"cpu\"):\n",
        "    input_tensor = torch.randn(1, 3, 224, 224).to(device)\n",
        "    # This is a simplified estimation of model latency and may not be accurate for all models.\n",
        "    model = model.to(device)\n",
        "    input_tensor = input_tensor.to(device)\n",
        "\n",
        "    # Warm-up to reduce variability\n",
        "    for _ in range(10):\n",
        "        _ = model(input_tensor)\n",
        "\n",
        "    # Measure execution time\n",
        "    start_time = torch.cuda.Event(enable_timing=True)\n",
        "    end_time = torch.cuda.Event(enable_timing=True)\n",
        "    start_time.record()\n",
        "    _ = model(input_tensor)\n",
        "    end_time.record()\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    latency_ms = start_time.elapsed_time(end_time)\n",
        "    return latency_ms\n",
        "\n",
        "\n",
        "def train(model, optimizer, loss_fn, train_loader, val_loader, test_loader, load_model_name, epochs=20, device=\"cpu\", log_dir='tensorboard_logs', patience=5, early_stopping_patience=10, classifier_type='MLP'):\n",
        "    ## Create folders and writer\n",
        "    # Create a directory to store TensorBoard logs\n",
        "    # log_dir = 'tensorboard_logs'\n",
        "\n",
        "    # Create a TensorBoard SummaryWriter\n",
        "    # load_model_name = \"resnet50\"\n",
        "    model_name = \"best_model_\" + load_model_name + \".pth\"\n",
        "\n",
        "    current_datetime = datetime.datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n",
        "    unique_folder_name = f\"{current_datetime}_{load_model_name}\"\n",
        "    unique_log_dir = os.path.join(log_dir, unique_folder_name)\n",
        "    model_path = os.path.join(unique_log_dir, model_name)\n",
        "\n",
        "    layout = {\n",
        "        \"Train and validation at same time\": {\n",
        "            \"Loss\": [\"Multiline\", [\"Loss/Train\", \"Loss/Validation\"]],\n",
        "            \"Accuracy\": [\"Multiline\", [\"Accuracy/Train\", \"Accuracy/Validation\"]],\n",
        "        },\n",
        "    }\n",
        "\n",
        "    writer = SummaryWriter(log_dir=unique_log_dir)\n",
        "    writer.add_custom_scalars(layout)\n",
        "\n",
        "    ## use early_stopping and scheduler learning rate\n",
        "    early_stopping = EarlyStopping(patience=early_stopping_patience, verbose=True)\n",
        "    lr_scheduler = LearningRateScheduler(optimizer, patience=patience, factor=0.1, verbose=True)\n",
        "\n",
        "    ## start training\n",
        "    best_valid_accuracy = 0.0\n",
        "    best_model_state = None\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        training_loss = 0.0\n",
        "        valid_loss = 0.0\n",
        "        model.train()\n",
        "\n",
        "        # Initialize variables for train accuracy calculation\n",
        "        num_correct_train = 0\n",
        "        num_examples_train = 0\n",
        "\n",
        "        for batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            inputs, targets = batch\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "            output = model(inputs)\n",
        "            loss = loss_fn(output, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            training_loss += loss.data.item() * inputs.size(0)\n",
        "\n",
        "            # Calculate the number of correct predictions in the current batch\n",
        "            correct = torch.eq(torch.max(F.softmax(output, dim=1), dim=1)[1], targets).view(-1)\n",
        "            num_correct_train += torch.sum(correct).item()\n",
        "            num_examples_train += correct.shape[0]\n",
        "\n",
        "        training_loss /= len(train_loader.dataset)\n",
        "        train_accuracy = num_correct_train / num_examples_train\n",
        "\n",
        "        model.eval()\n",
        "        num_correct = 0\n",
        "        num_examples = 0\n",
        "\n",
        "        for batch in val_loader:\n",
        "            inputs, targets = batch\n",
        "            inputs = inputs.to(device)\n",
        "            output = model(inputs)\n",
        "            targets = targets.to(device)\n",
        "            loss = loss_fn(output, targets)\n",
        "            valid_loss += loss.data.item() * inputs.size(0)\n",
        "            correct = torch.eq(torch.max(F.softmax(output, dim=1), dim=1)[1], targets).view(-1)\n",
        "\n",
        "            num_correct += torch.sum(correct).item()\n",
        "            num_examples += correct.shape[0]\n",
        "\n",
        "        valid_loss /= len(val_loader.dataset)\n",
        "        valid_accuracy = num_correct / num_examples\n",
        "\n",
        "        # Get the current learning rate from the optimizer\n",
        "        current_lr = lr_scheduler.step(valid_accuracy)\n",
        "\n",
        "        print('Epoch: {}, Learning Rate: {}, Training Loss: {:.2f}, Training Accuracy: {:.4f}, Validation Loss: {:.2f}, Validation Accuracy: {:.4f}'.format(epoch, current_lr, training_loss, train_accuracy, valid_loss, valid_accuracy))\n",
        "\n",
        "        # Log training accuracy to TensorBoard\n",
        "        writer.add_scalar('Learning Rate', current_lr, epoch)\n",
        "        writer.add_scalar('Loss/Train', training_loss, epoch)\n",
        "        writer.add_scalar('Accuracy/Train', train_accuracy, epoch)\n",
        "        writer.add_scalar('Loss/Validation', valid_loss, epoch)\n",
        "        writer.add_scalar('Accuracy/Validation', valid_accuracy, epoch)\n",
        "\n",
        "        early_stop = early_stopping.step(valid_accuracy)\n",
        "        if early_stop:\n",
        "            break  # Stop training if early stopping is activated\n",
        "\n",
        "        # Save the best model based on validation accuracy\n",
        "        if valid_accuracy > best_valid_accuracy:\n",
        "            best_valid_accuracy = valid_accuracy\n",
        "            best_model_state = model.state_dict()\n",
        "            # Save the best model state to a file\n",
        "            torch.save(best_model_state, model_path)\n",
        "\n",
        "    print(f\"\\n Model has been saved to {model_path} \\n\")\n",
        "\n",
        "    # Inspect the model\n",
        "    writer.add_graph(model, inputs)\n",
        "    writer.add_figure('Confusion Matrix', plot_confusion_matrix(model, val_loader, device))\n",
        "    # writer.add_figure(f\"Predictions vs. Actuals\", log_predictions_vs_actuals(model, val_loader, device=device, num_batches=1))\n",
        "\n",
        "    # Add hyperparameters to TensorBoard\n",
        "    hyperparameters = {\n",
        "        'Feature Extractor': load_model_name,\n",
        "        'Model Accuracy': best_valid_accuracy,\n",
        "        'Params (M)': sum(p.numel() for p in model.parameters()) / 1e6,  # Convert to million parameters\n",
        "        'Size of model (MB)': os.path.getsize(model_path) / (1024 * 1024),  # Size in MB\n",
        "        'Latency of model (ms)': estimate_latency(model, device),  # Calculate latency with a dummy input\n",
        "        'Classifier type': classifier_type,\n",
        "        'Training type': 'Normal',\n",
        "    }\n",
        "\n",
        "    writer.add_hparams(hparam_dict=hyperparameters, metric_dict={})\n",
        "    # Print hyperparameters with .4f\n",
        "    for key, value in hyperparameters.items():\n",
        "        if isinstance(value, float):\n",
        "            print(f'{key}: {value:.4f}')\n",
        "        else:\n",
        "            print(f'{key}: {value}')\n",
        "\n",
        "    # Close the TensorBoard SummaryWriter\n",
        "    writer.close()\n",
        "\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    test_accuracy = evaluate(model, test_loader, device=device)\n",
        "    print(f\"\\n Test Accuracy: {test_accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "otMAjsLacJxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_knowledge_distillation(teacher, student, optimizer, ce_loss, train_loader, val_loader, test_loader, load_model_name, epochs, T, soft_target_loss_weight, ce_loss_weight, device, log_dir='tensorboard_logs', patience=5, early_stopping_patience=10, classifier_type='MLP'):\n",
        "    ## Create folders and writer\n",
        "    # Create a directory to store TensorBoard logs\n",
        "    # log_dir = 'tensorboard_logs'\n",
        "\n",
        "    # Create a TensorBoard SummaryWriter\n",
        "    # load_model_name = \"resnet50\"\n",
        "    model_name = \"best_model_\" + load_model_name + \".pth\"\n",
        "\n",
        "    current_datetime = datetime.datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n",
        "    unique_folder_name = f\"{current_datetime}_{load_model_name}\"\n",
        "    unique_log_dir = os.path.join(log_dir, unique_folder_name)\n",
        "    model_path = os.path.join(unique_log_dir, model_name)\n",
        "\n",
        "    layout = {\n",
        "        \"Train and validation at same time\": {\n",
        "            \"Loss\": [\"Multiline\", [\"Loss/Train\", \"Loss/Validation\"]],\n",
        "            \"Accuracy\": [\"Multiline\", [\"Accuracy/Train\", \"Accuracy/Validation\"]],\n",
        "        },\n",
        "    }\n",
        "\n",
        "    writer = SummaryWriter(log_dir=unique_log_dir)\n",
        "    writer.add_custom_scalars(layout)\n",
        "\n",
        "    ## use early_stopping and scheduler learning rate\n",
        "    early_stopping = EarlyStopping(patience=early_stopping_patience, verbose=True)\n",
        "    lr_scheduler = LearningRateScheduler(optimizer, patience=patience, factor=0.1, verbose=True)\n",
        "\n",
        "    ## start training\n",
        "    best_valid_accuracy = 0.0\n",
        "    best_model_state = None\n",
        "    teacher.eval()  # Teacher set to evaluation mode\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        running_loss = 0.0\n",
        "        num_correct_train = 0\n",
        "        num_examples_train = 0\n",
        "        student.train()\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass with the teacher model - do not save gradients here as we do not change the teacher's weights\n",
        "            with torch.no_grad():\n",
        "                teacher_logits = teacher(inputs)\n",
        "\n",
        "            # Forward pass with the student model\n",
        "            student_logits = student(inputs)\n",
        "\n",
        "            # Soften the student logits by applying softmax first and log() second\n",
        "            soft_targets = nn.functional.softmax(teacher_logits / T, dim=-1)\n",
        "            soft_prob = nn.functional.log_softmax(student_logits / T, dim=-1)\n",
        "\n",
        "            # Calculate the soft targets loss. Scaled by T**2 as suggested by the authors of the paper \"Distilling the knowledge in a neural network\"\n",
        "            soft_targets_loss = -torch.sum(soft_targets * soft_prob) / soft_prob.size()[0] * (T**2)\n",
        "\n",
        "            # Calculate the true label loss\n",
        "            label_loss = ce_loss(student_logits, labels)\n",
        "\n",
        "            # Weighted sum of the two losses\n",
        "            loss = soft_target_loss_weight * soft_targets_loss + ce_loss_weight * label_loss\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # Calculate the number of correct predictions in the current batch\n",
        "            correct = torch.eq(torch.max(F.softmax(student_logits, dim=1), dim=1)[1], labels).view(-1)\n",
        "            num_correct_train += torch.sum(correct).item()\n",
        "            num_examples_train += correct.shape[0]\n",
        "\n",
        "        # Calculate training accuracy\n",
        "        train_accuracy = num_correct_train / num_examples_train\n",
        "\n",
        "        # Validation\n",
        "        student.eval()\n",
        "        num_correct_val = 0\n",
        "        num_examples_val = 0\n",
        "        valid_loss = 0.0\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                student_logits = student(inputs)\n",
        "\n",
        "                # Calculate the true label loss\n",
        "                label_loss = ce_loss(student_logits, labels)\n",
        "\n",
        "                valid_loss += label_loss.item() * inputs.size(0)\n",
        "\n",
        "                # Calculate the number of correct predictions in the current batch\n",
        "                correct = torch.eq(torch.max(F.softmax(student_logits, dim=1), dim=1)[1], labels).view(-1)\n",
        "                num_correct_val += torch.sum(correct).item()\n",
        "                num_examples_val += correct.shape[0]\n",
        "\n",
        "        # Calculate validation accuracy\n",
        "        valid_accuracy = num_correct_val / num_examples_val\n",
        "        valid_loss /= len(val_loader.dataset)\n",
        "\n",
        "        # Get the current learning rate from the optimizer\n",
        "        current_lr = lr_scheduler.step(valid_accuracy)\n",
        "\n",
        "        # Print metrics\n",
        "        print(f\"Epoch {epoch}, Learning Rate: {current_lr}, Training Loss: {running_loss / len(train_loader):.2f}, Training Accuracy: {train_accuracy:.4f}, Validation Loss: {valid_loss:.2f}, Validation Accuracy: {valid_accuracy:.4f}\")\n",
        "\n",
        "        # Log training accuracy to TensorBoard\n",
        "        writer.add_scalar('Learning Rate', current_lr, epoch)\n",
        "        writer.add_scalar('Loss/Train', running_loss / len(train_loader), epoch)\n",
        "        writer.add_scalar('Accuracy/Train', train_accuracy, epoch)\n",
        "        writer.add_scalar('Loss/Validation', valid_loss, epoch)\n",
        "        writer.add_scalar('Accuracy/Validation', valid_accuracy, epoch)\n",
        "\n",
        "        early_stop = early_stopping.step(valid_accuracy)\n",
        "        if early_stop:\n",
        "            break  # Stop training if early stopping is activated\n",
        "\n",
        "        # Save the best model based on validation accuracy\n",
        "        if valid_accuracy > best_valid_accuracy:\n",
        "            best_valid_accuracy = valid_accuracy\n",
        "            best_model_state = student.state_dict()\n",
        "            # Save the best model state to a file\n",
        "            torch.save(best_model_state, model_path)\n",
        "\n",
        "    print(f\"\\n Model has been saved to {model_path}\\n\")\n",
        "\n",
        "    # Inspect the model\n",
        "    writer.add_graph(student, inputs)\n",
        "    writer.add_figure('Confusion Matrix', plot_confusion_matrix(student, val_loader, device))\n",
        "    # writer.add_figure(f\"Predictions vs. Actuals\", log_predictions_vs_actuals(student, val_loader, device=device, num_batches=1))\n",
        "\n",
        "    # Add hyperparameters to TensorBoard\n",
        "    hyperparameters = {\n",
        "        'Feature Extractor': load_model_name,\n",
        "        'Model Accuracy': best_valid_accuracy,\n",
        "        'Params (M)': sum(p.numel() for p in student.parameters()) / 1e6,  # Convert to million parameters\n",
        "        'Size of model (MB)': os.path.getsize(model_path) / (1024 * 1024),  # Size in MB\n",
        "        'Latency of model (ms)': estimate_latency(student, device),  # Calculate latency with a dummy input\n",
        "        'Classifier type': classifier_type,\n",
        "        'Training type': 'Teacher Student Model',\n",
        "    }\n",
        "\n",
        "    writer.add_hparams(hparam_dict=hyperparameters, metric_dict={})\n",
        "    # Print hyperparameters with .4f\n",
        "    for key, value in hyperparameters.items():\n",
        "        if isinstance(value, float):\n",
        "            print(f'{key}: {value:.4f}')\n",
        "        else:\n",
        "            print(f'{key}: {value}')\n",
        "\n",
        "    # Close the TensorBoard SummaryWriter\n",
        "    writer.close()\n",
        "\n",
        "    student.load_state_dict(torch.load(model_path))\n",
        "    test_accuracy = evaluate(student, test_loader, device=device)\n",
        "    print(f\"\\n Test Accuracy: {test_accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "MEiwXbu_cMLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load model"
      ],
      "metadata": {
        "id": "rFctSyNMdeO2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load teacher model\n",
        "# model = models.resnext101_32x8d(pretrained=False)\n",
        "model = models.resnet50(pretrained=False)\n",
        "# Replace the last fully connected layer\n",
        "num_features = model.fc.in_features\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Linear(num_features, 500),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(),\n",
        "    nn.Linear(500, 50)\n",
        ")\n",
        "# model_path = 'tensorboard_logs/2023-11-27_062728_resnet50/best_model_resnet50.pth'\n",
        "# model_path = 'tensorboard_logs/2023-12-01_104615_resnet50/best_model_resnet50.pth ' # 86.5 %\n",
        "model_path = 'tensorboard_logs/2023-12-01_130404_resnet50/best_model_resnet50.pth' # 87.25%\n",
        "# model_path = 'tensorboard_logs/2023-11-13_075020_resnext101_32x8d/best_model_resnext101_32x8d.pth'\n",
        "# model_path = 'tensorboard_logs/2023-11-28_071915_resnext101_32x8d/best_model_resnext101_32x8d.pth '\n",
        "# model_path = 'tensorboard_logs/2023-11-29_060228_resnext101_32x8d/best_model_resnext101_32x8d.pth'\n",
        "# model.load_state_dict(torch.load(model_path))\n",
        "model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "model.to(device)\n",
        "print(f'model from {model_path} loaded')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGVAUoOFdgjE",
        "outputId": "4a8608d7-e64d-4340-96dc-f681bde31016"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model from tensorboard_logs/2023-12-01_130404_resnet50/best_model_resnet50.pth loaded\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8575"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(model, esc50_test_loader, device)"
      ],
      "metadata": {
        "id": "1MLbQJLkYlsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ResNet18 raw"
      ],
      "metadata": {
        "id": "2Hm8da-16rZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epoches = 20\n",
        "patience = 5\n",
        "early_stopping_patience=10\n",
        "load_model_name = \"resnet18\"\n",
        "classifier_type='MLP'\n",
        "\n",
        "model_raw1 = models.resnet18(pretrained=True)\n",
        "\n",
        "# Replace the last fully connected layer\n",
        "num_features = model_raw1.fc.in_features\n",
        "model_raw1.fc = nn.Sequential(\n",
        "    nn.Linear(num_features, 500),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(),\n",
        "    nn.Linear(500, 50)\n",
        ")\n",
        "\n",
        "model_raw1.to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.Adam([\n",
        "                        {'params': model_raw1.conv1.parameters(), 'lr': 1e-4},\n",
        "                        {'params': model_raw1.layer1.parameters(), 'lr': 1e-4},\n",
        "                        {'params': model_raw1.layer2.parameters(), 'lr': 1e-4},\n",
        "                        {'params': model_raw1.layer3.parameters(), 'lr': 1e-4},\n",
        "                        {'params': model_raw1.layer4.parameters(), 'lr': 1e-4},\n",
        "                        {'params': model_raw1.fc.parameters(), 'lr': 1e-8}\n",
        "                        ], lr=1e-2)\n",
        "\n",
        "\n",
        "train(model_raw1, optimizer, loss_fn, esc50_train_loader, esc50_val_loader, esc50_test_loader, load_model_name, epochs=epoches, device=device,\n",
        "      log_dir = 'tensorboard_logs', patience=patience, early_stopping_patience=early_stopping_patience, classifier_type=classifier_type)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xt7UQxvjlmE0",
        "outputId": "fb8e7984-42eb-4003-a20a-a98215a11130"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 51.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Learning Rate: 0.0001, Training Loss: 3.60, Training Accuracy: 0.14, Validation Loss: 3.17, Validation Accuracy: 0.38\n",
            "Epoch: 2, Learning Rate: 0.0001, Training Loss: 2.99, Training Accuracy: 0.40, Validation Loss: 2.73, Validation Accuracy: 0.49\n",
            "Epoch: 3, Learning Rate: 0.0001, Training Loss: 2.55, Training Accuracy: 0.58, Validation Loss: 2.31, Validation Accuracy: 0.58\n",
            "Epoch: 4, Learning Rate: 0.0001, Training Loss: 2.19, Training Accuracy: 0.69, Validation Loss: 2.09, Validation Accuracy: 0.67\n",
            "Epoch: 5, Learning Rate: 0.0001, Training Loss: 1.90, Training Accuracy: 0.79, Validation Loss: 1.90, Validation Accuracy: 0.70\n",
            "Epoch: 6, Learning Rate: 0.0001, Training Loss: 1.63, Training Accuracy: 0.86, Validation Loss: 1.71, Validation Accuracy: 0.78\n",
            "Epoch 00007: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch 00007: reducing learning rate of group 1 to 1.0000e-05.\n",
            "Epoch 00007: reducing learning rate of group 2 to 1.0000e-05.\n",
            "Epoch 00007: reducing learning rate of group 3 to 1.0000e-05.\n",
            "Epoch 00007: reducing learning rate of group 4 to 1.0000e-05.\n",
            "Epoch: 7, Learning Rate: 1e-05, Training Loss: 1.40, Training Accuracy: 0.92, Validation Loss: 1.55, Validation Accuracy: 0.80\n",
            "Epoch: 8, Learning Rate: 1e-05, Training Loss: 1.19, Training Accuracy: 0.96, Validation Loss: 1.53, Validation Accuracy: 0.80\n",
            "Epoch: 9, Learning Rate: 1e-05, Training Loss: 1.14, Training Accuracy: 0.97, Validation Loss: 1.50, Validation Accuracy: 0.81\n",
            "Epoch: 10, Learning Rate: 1e-05, Training Loss: 1.12, Training Accuracy: 0.97, Validation Loss: 1.50, Validation Accuracy: 0.81\n",
            "Epoch: 11, Learning Rate: 1e-05, Training Loss: 1.08, Training Accuracy: 0.98, Validation Loss: 1.49, Validation Accuracy: 0.80\n",
            "Epoch: 12, Learning Rate: 1e-05, Training Loss: 1.06, Training Accuracy: 0.98, Validation Loss: 1.46, Validation Accuracy: 0.80\n",
            "Epoch 00013: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Epoch 00013: reducing learning rate of group 1 to 1.0000e-06.\n",
            "Epoch 00013: reducing learning rate of group 2 to 1.0000e-06.\n",
            "Epoch 00013: reducing learning rate of group 3 to 1.0000e-06.\n",
            "Epoch 00013: reducing learning rate of group 4 to 1.0000e-06.\n",
            "Epoch: 13, Learning Rate: 1.0000000000000002e-06, Training Loss: 1.04, Training Accuracy: 0.98, Validation Loss: 1.47, Validation Accuracy: 0.81\n",
            "Epoch: 14, Learning Rate: 1.0000000000000002e-06, Training Loss: 1.00, Training Accuracy: 0.98, Validation Loss: 1.47, Validation Accuracy: 0.82\n",
            "Epoch: 15, Learning Rate: 1.0000000000000002e-06, Training Loss: 1.03, Training Accuracy: 0.98, Validation Loss: 1.45, Validation Accuracy: 0.82\n",
            "Epoch: 16, Learning Rate: 1.0000000000000002e-06, Training Loss: 1.01, Training Accuracy: 0.99, Validation Loss: 1.45, Validation Accuracy: 0.81\n",
            "Epoch: 17, Learning Rate: 1.0000000000000002e-06, Training Loss: 1.02, Training Accuracy: 0.98, Validation Loss: 1.47, Validation Accuracy: 0.81\n",
            "Epoch: 18, Learning Rate: 1.0000000000000002e-06, Training Loss: 1.01, Training Accuracy: 0.98, Validation Loss: 1.45, Validation Accuracy: 0.81\n",
            "Epoch 00019: reducing learning rate of group 0 to 1.0000e-07.\n",
            "Epoch 00019: reducing learning rate of group 1 to 1.0000e-07.\n",
            "Epoch 00019: reducing learning rate of group 2 to 1.0000e-07.\n",
            "Epoch 00019: reducing learning rate of group 3 to 1.0000e-07.\n",
            "Epoch 00019: reducing learning rate of group 4 to 1.0000e-07.\n",
            "Epoch: 19, Learning Rate: 1.0000000000000002e-07, Training Loss: 1.03, Training Accuracy: 0.98, Validation Loss: 1.46, Validation Accuracy: 0.82\n",
            "Epoch: 20, Learning Rate: 1.0000000000000002e-07, Training Loss: 1.01, Training Accuracy: 0.99, Validation Loss: 1.47, Validation Accuracy: 0.82\n",
            "\n",
            " Model has been saved to tensorboard_logs/2023-11-27_070408_resnet18/best_model_resnet18.pth \n",
            "\n",
            "Feature Extractor: resnet18\n",
            "Model Accuracy: 0.82\n",
            "Params (M): 11.46\n",
            "Size of model (MB): 43.79\n",
            "Latency of model (ms): 4.02\n",
            "Classifier type: MLP\n",
            "Training type: Normal\n",
            "\n",
            " Test Accuracy: 81.75%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epoches = 50\n",
        "patience = 10\n",
        "early_stopping_patience=15\n",
        "load_model_name = \"resnet18\"\n",
        "classifier_type='MLP'\n",
        "\n",
        "model_raw1 = models.resnet18(pretrained=True)\n",
        "\n",
        "# Replace the last fully connected layer\n",
        "num_features = model_raw1.fc.in_features\n",
        "model_raw1.fc = nn.Sequential(\n",
        "    nn.Linear(num_features, 500),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(),\n",
        "    nn.Linear(500, 50)\n",
        ")\n",
        "\n",
        "model_raw1.to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.Adam([\n",
        "                        {'params': model_raw1.conv1.parameters(), 'lr': 1e-4},\n",
        "                        {'params': model_raw1.layer1.parameters(), 'lr': 1e-4},\n",
        "                        {'params': model_raw1.layer2.parameters(), 'lr': 1e-4},\n",
        "                        {'params': model_raw1.layer3.parameters(), 'lr': 1e-4},\n",
        "                        {'params': model_raw1.layer4.parameters(), 'lr': 1e-4},\n",
        "                        {'params': model_raw1.fc.parameters(), 'lr': 1e-8}\n",
        "                        ], lr=1e-2)\n",
        "\n",
        "\n",
        "train(model_raw1, optimizer, loss_fn, esc50_train_loader, esc50_val_loader, esc50_test_loader, load_model_name, epochs=epoches, device=device,\n",
        "      log_dir = 'tensorboard_logs', patience=patience, early_stopping_patience=early_stopping_patience, classifier_type=classifier_type)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkRbqIawZbC8",
        "outputId": "a2beb719-2713-4ea3-efad-c08577dc0b75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 67.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Learning Rate: 0.0001, Training Loss: 3.60, Training Accuracy: 0.1313, Validation Loss: 3.12, Validation Accuracy: 0.3450\n",
            "Epoch: 2, Learning Rate: 0.0001, Training Loss: 2.97, Training Accuracy: 0.4081, Validation Loss: 2.68, Validation Accuracy: 0.5675\n",
            "Epoch: 3, Learning Rate: 0.0001, Training Loss: 2.51, Training Accuracy: 0.5962, Validation Loss: 2.29, Validation Accuracy: 0.6450\n",
            "Epoch: 4, Learning Rate: 0.0001, Training Loss: 2.15, Training Accuracy: 0.7281, Validation Loss: 2.01, Validation Accuracy: 0.7075\n",
            "Epoch: 5, Learning Rate: 0.0001, Training Loss: 1.83, Training Accuracy: 0.8087, Validation Loss: 1.87, Validation Accuracy: 0.7400\n",
            "Epoch: 6, Learning Rate: 0.0001, Training Loss: 1.58, Training Accuracy: 0.8681, Validation Loss: 1.74, Validation Accuracy: 0.7700\n",
            "Epoch: 7, Learning Rate: 0.0001, Training Loss: 1.37, Training Accuracy: 0.9113, Validation Loss: 1.58, Validation Accuracy: 0.7900\n",
            "Epoch: 8, Learning Rate: 0.0001, Training Loss: 1.20, Training Accuracy: 0.9463, Validation Loss: 1.43, Validation Accuracy: 0.8250\n",
            "Epoch: 9, Learning Rate: 0.0001, Training Loss: 1.01, Training Accuracy: 0.9762, Validation Loss: 1.43, Validation Accuracy: 0.8250\n",
            "Epoch: 10, Learning Rate: 0.0001, Training Loss: 0.89, Training Accuracy: 0.9819, Validation Loss: 1.38, Validation Accuracy: 0.8075\n",
            "Epoch: 11, Learning Rate: 0.0001, Training Loss: 0.81, Training Accuracy: 0.9906, Validation Loss: 1.38, Validation Accuracy: 0.8025\n",
            "Epoch 00012: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch 00012: reducing learning rate of group 1 to 1.0000e-05.\n",
            "Epoch 00012: reducing learning rate of group 2 to 1.0000e-05.\n",
            "Epoch 00012: reducing learning rate of group 3 to 1.0000e-05.\n",
            "Epoch 00012: reducing learning rate of group 4 to 1.0000e-05.\n",
            "Epoch: 12, Learning Rate: 1e-05, Training Loss: 0.74, Training Accuracy: 0.9912, Validation Loss: 1.30, Validation Accuracy: 0.8225\n",
            "Epoch: 13, Learning Rate: 1e-05, Training Loss: 0.64, Training Accuracy: 0.9962, Validation Loss: 1.25, Validation Accuracy: 0.8550\n",
            "Epoch: 14, Learning Rate: 1e-05, Training Loss: 0.60, Training Accuracy: 0.9988, Validation Loss: 1.27, Validation Accuracy: 0.8400\n",
            "Epoch: 15, Learning Rate: 1e-05, Training Loss: 0.58, Training Accuracy: 0.9988, Validation Loss: 1.24, Validation Accuracy: 0.8450\n",
            "Epoch: 16, Learning Rate: 1e-05, Training Loss: 0.58, Training Accuracy: 0.9981, Validation Loss: 1.25, Validation Accuracy: 0.8475\n",
            "Epoch: 17, Learning Rate: 1e-05, Training Loss: 0.58, Training Accuracy: 0.9969, Validation Loss: 1.22, Validation Accuracy: 0.8425\n",
            "Epoch: 18, Learning Rate: 1e-05, Training Loss: 0.55, Training Accuracy: 0.9981, Validation Loss: 1.25, Validation Accuracy: 0.8325\n",
            "Epoch: 19, Learning Rate: 1e-05, Training Loss: 0.56, Training Accuracy: 0.9975, Validation Loss: 1.24, Validation Accuracy: 0.8400\n",
            "Epoch: 20, Learning Rate: 1e-05, Training Loss: 0.57, Training Accuracy: 0.9994, Validation Loss: 1.24, Validation Accuracy: 0.8400\n",
            "Epoch: 21, Learning Rate: 1e-05, Training Loss: 0.54, Training Accuracy: 0.9988, Validation Loss: 1.27, Validation Accuracy: 0.8325\n",
            "Epoch: 22, Learning Rate: 1e-05, Training Loss: 0.55, Training Accuracy: 0.9981, Validation Loss: 1.24, Validation Accuracy: 0.8325\n",
            "Epoch 00023: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Epoch 00023: reducing learning rate of group 1 to 1.0000e-06.\n",
            "Epoch 00023: reducing learning rate of group 2 to 1.0000e-06.\n",
            "Epoch 00023: reducing learning rate of group 3 to 1.0000e-06.\n",
            "Epoch 00023: reducing learning rate of group 4 to 1.0000e-06.\n",
            "Epoch: 23, Learning Rate: 1.0000000000000002e-06, Training Loss: 0.53, Training Accuracy: 1.0000, Validation Loss: 1.22, Validation Accuracy: 0.8425\n",
            "Epoch: 24, Learning Rate: 1.0000000000000002e-06, Training Loss: 0.54, Training Accuracy: 0.9994, Validation Loss: 1.23, Validation Accuracy: 0.8450\n",
            "Epoch: 25, Learning Rate: 1.0000000000000002e-06, Training Loss: 0.51, Training Accuracy: 0.9988, Validation Loss: 1.23, Validation Accuracy: 0.8475\n",
            "Epoch: 26, Learning Rate: 1.0000000000000002e-06, Training Loss: 0.54, Training Accuracy: 0.9994, Validation Loss: 1.24, Validation Accuracy: 0.8375\n",
            "Epoch: 27, Learning Rate: 1.0000000000000002e-06, Training Loss: 0.51, Training Accuracy: 0.9994, Validation Loss: 1.24, Validation Accuracy: 0.8425\n",
            "Epoch: 28, Learning Rate: 1.0000000000000002e-06, Training Loss: 0.53, Training Accuracy: 0.9994, Validation Loss: 1.23, Validation Accuracy: 0.8475\n",
            "Epoch: 29, Learning Rate: 1.0000000000000002e-06, Training Loss: 0.52, Training Accuracy: 0.9975, Validation Loss: 1.22, Validation Accuracy: 0.8300\n",
            "Early stopping activated.\n",
            "\n",
            " Model has been saved to tensorboard_logs/2023-12-02_120014_resnet18/best_model_resnet18.pth \n",
            "\n",
            "Feature Extractor: resnet18\n",
            "Model Accuracy: 0.8550\n",
            "Params (M): 11.4581\n",
            "Size of model (MB): 43.7863\n",
            "Latency of model (ms): 4.5199\n",
            "Classifier type: MLP\n",
            "Training type: Normal\n",
            "\n",
            " Test Accuracy: 85.50%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ResNet50 - Teacher"
      ],
      "metadata": {
        "id": "xkVz_gpH6RZj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 50\n",
        "patience = 10\n",
        "early_stopping_patience=15\n",
        "load_model_name = \"resnet50\"\n",
        "classifier_type='MLP'\n",
        "\n",
        "model = models.resnet50(pretrained=True)\n",
        "\n",
        "# Replace the last fully connected layer\n",
        "num_features = model.fc.in_features\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Linear(num_features, 500),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(),\n",
        "    nn.Linear(500, 50)\n",
        ")\n",
        "\n",
        "model.to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.Adam([\n",
        "                        {'params': model.conv1.parameters(), 'lr': 1e-4},\n",
        "                        {'params': model.layer1.parameters(), 'lr': 1e-4},\n",
        "                        {'params': model.layer2.parameters(), 'lr': 1e-4},\n",
        "                        {'params': model.layer3.parameters(), 'lr': 1e-4},\n",
        "                        {'params': model.layer4.parameters(), 'lr': 1e-4},\n",
        "                        {'params': model.fc.parameters(), 'lr': 1e-8}\n",
        "                        ], lr=1e-2)\n",
        "\n",
        "\n",
        "train(model, optimizer, loss_fn, esc50_train_loader, esc50_val_loader, esc50_test_loader, load_model_name, epochs=epochs, device=device,\n",
        "      log_dir = 'tensorboard_logs', patience=patience, early_stopping_patience=early_stopping_patience, classifier_type=classifier_type)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46BpKBYrehZZ",
        "outputId": "6bc6bd89-dce2-4e25-f00a-3b6155789349"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Learning Rate: 0.0001, Training Loss: 3.62, Training Accuracy: 0.1631, Validation Loss: 3.13, Validation Accuracy: 0.3700\n",
            "Epoch: 2, Learning Rate: 0.0001, Training Loss: 3.01, Training Accuracy: 0.4113, Validation Loss: 2.56, Validation Accuracy: 0.5275\n",
            "Epoch: 3, Learning Rate: 0.0001, Training Loss: 2.50, Training Accuracy: 0.5519, Validation Loss: 2.15, Validation Accuracy: 0.6075\n",
            "Epoch: 4, Learning Rate: 0.0001, Training Loss: 2.00, Training Accuracy: 0.7262, Validation Loss: 1.74, Validation Accuracy: 0.6900\n",
            "Epoch: 5, Learning Rate: 0.0001, Training Loss: 1.65, Training Accuracy: 0.8013, Validation Loss: 1.57, Validation Accuracy: 0.7125\n",
            "Epoch: 6, Learning Rate: 0.0001, Training Loss: 1.32, Training Accuracy: 0.8606, Validation Loss: 1.34, Validation Accuracy: 0.7500\n",
            "Epoch: 7, Learning Rate: 0.0001, Training Loss: 1.06, Training Accuracy: 0.9169, Validation Loss: 1.23, Validation Accuracy: 0.7675\n",
            "Epoch: 8, Learning Rate: 0.0001, Training Loss: 0.85, Training Accuracy: 0.9387, Validation Loss: 1.20, Validation Accuracy: 0.7500\n",
            "Epoch: 9, Learning Rate: 0.0001, Training Loss: 0.69, Training Accuracy: 0.9569, Validation Loss: 1.05, Validation Accuracy: 0.7925\n",
            "Epoch: 10, Learning Rate: 0.0001, Training Loss: 0.58, Training Accuracy: 0.9725, Validation Loss: 1.01, Validation Accuracy: 0.8050\n",
            "Epoch: 11, Learning Rate: 0.0001, Training Loss: 0.50, Training Accuracy: 0.9744, Validation Loss: 0.94, Validation Accuracy: 0.8075\n",
            "Epoch 00012: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch 00012: reducing learning rate of group 1 to 1.0000e-05.\n",
            "Epoch 00012: reducing learning rate of group 2 to 1.0000e-05.\n",
            "Epoch 00012: reducing learning rate of group 3 to 1.0000e-05.\n",
            "Epoch 00012: reducing learning rate of group 4 to 1.0000e-05.\n",
            "Epoch: 12, Learning Rate: 1e-05, Training Loss: 0.42, Training Accuracy: 0.9856, Validation Loss: 0.93, Validation Accuracy: 0.8125\n",
            "Epoch: 13, Learning Rate: 1e-05, Training Loss: 0.32, Training Accuracy: 0.9962, Validation Loss: 0.78, Validation Accuracy: 0.8500\n",
            "Epoch: 14, Learning Rate: 1e-05, Training Loss: 0.29, Training Accuracy: 0.9988, Validation Loss: 0.78, Validation Accuracy: 0.8400\n",
            "Epoch: 15, Learning Rate: 1e-05, Training Loss: 0.27, Training Accuracy: 0.9981, Validation Loss: 0.75, Validation Accuracy: 0.8575\n",
            "Epoch: 16, Learning Rate: 1e-05, Training Loss: 0.28, Training Accuracy: 0.9988, Validation Loss: 0.73, Validation Accuracy: 0.8700\n",
            "Epoch: 17, Learning Rate: 1e-05, Training Loss: 0.26, Training Accuracy: 0.9994, Validation Loss: 0.74, Validation Accuracy: 0.8500\n",
            "Epoch: 18, Learning Rate: 1e-05, Training Loss: 0.25, Training Accuracy: 0.9994, Validation Loss: 0.74, Validation Accuracy: 0.8675\n",
            "Epoch: 19, Learning Rate: 1e-05, Training Loss: 0.24, Training Accuracy: 0.9994, Validation Loss: 0.73, Validation Accuracy: 0.8675\n",
            "Epoch: 20, Learning Rate: 1e-05, Training Loss: 0.24, Training Accuracy: 1.0000, Validation Loss: 0.71, Validation Accuracy: 0.8625\n",
            "Epoch: 21, Learning Rate: 1e-05, Training Loss: 0.24, Training Accuracy: 0.9994, Validation Loss: 0.74, Validation Accuracy: 0.8550\n",
            "Epoch: 22, Learning Rate: 1e-05, Training Loss: 0.23, Training Accuracy: 1.0000, Validation Loss: 0.71, Validation Accuracy: 0.8675\n",
            "Epoch 00023: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Epoch 00023: reducing learning rate of group 1 to 1.0000e-06.\n",
            "Epoch 00023: reducing learning rate of group 2 to 1.0000e-06.\n",
            "Epoch 00023: reducing learning rate of group 3 to 1.0000e-06.\n",
            "Epoch 00023: reducing learning rate of group 4 to 1.0000e-06.\n",
            "Epoch: 23, Learning Rate: 1.0000000000000002e-06, Training Loss: 0.22, Training Accuracy: 0.9994, Validation Loss: 0.72, Validation Accuracy: 0.8575\n",
            "Epoch: 24, Learning Rate: 1.0000000000000002e-06, Training Loss: 0.22, Training Accuracy: 1.0000, Validation Loss: 0.69, Validation Accuracy: 0.8750\n",
            "Epoch: 25, Learning Rate: 1.0000000000000002e-06, Training Loss: 0.23, Training Accuracy: 0.9994, Validation Loss: 0.70, Validation Accuracy: 0.8700\n",
            "Epoch: 26, Learning Rate: 1.0000000000000002e-06, Training Loss: 0.22, Training Accuracy: 1.0000, Validation Loss: 0.71, Validation Accuracy: 0.8675\n",
            "Epoch: 27, Learning Rate: 1.0000000000000002e-06, Training Loss: 0.22, Training Accuracy: 0.9994, Validation Loss: 0.70, Validation Accuracy: 0.8575\n",
            "Epoch: 28, Learning Rate: 1.0000000000000002e-06, Training Loss: 0.21, Training Accuracy: 0.9994, Validation Loss: 0.72, Validation Accuracy: 0.8675\n",
            "Epoch: 29, Learning Rate: 1.0000000000000002e-06, Training Loss: 0.21, Training Accuracy: 1.0000, Validation Loss: 0.71, Validation Accuracy: 0.8625\n",
            "Epoch: 30, Learning Rate: 1.0000000000000002e-06, Training Loss: 0.21, Training Accuracy: 1.0000, Validation Loss: 0.69, Validation Accuracy: 0.8700\n",
            "Epoch: 31, Learning Rate: 1.0000000000000002e-06, Training Loss: 0.21, Training Accuracy: 0.9981, Validation Loss: 0.72, Validation Accuracy: 0.8525\n",
            "Epoch: 32, Learning Rate: 1.0000000000000002e-06, Training Loss: 0.22, Training Accuracy: 0.9988, Validation Loss: 0.71, Validation Accuracy: 0.8650\n",
            "Epoch: 33, Learning Rate: 1.0000000000000002e-06, Training Loss: 0.21, Training Accuracy: 1.0000, Validation Loss: 0.70, Validation Accuracy: 0.8600\n",
            "Epoch 00034: reducing learning rate of group 0 to 1.0000e-07.\n",
            "Epoch 00034: reducing learning rate of group 1 to 1.0000e-07.\n",
            "Epoch 00034: reducing learning rate of group 2 to 1.0000e-07.\n",
            "Epoch 00034: reducing learning rate of group 3 to 1.0000e-07.\n",
            "Epoch 00034: reducing learning rate of group 4 to 1.0000e-07.\n",
            "Epoch: 34, Learning Rate: 1.0000000000000002e-07, Training Loss: 0.21, Training Accuracy: 0.9994, Validation Loss: 0.71, Validation Accuracy: 0.8525\n",
            "Epoch: 35, Learning Rate: 1.0000000000000002e-07, Training Loss: 0.21, Training Accuracy: 0.9994, Validation Loss: 0.70, Validation Accuracy: 0.8650\n",
            "Epoch: 36, Learning Rate: 1.0000000000000002e-07, Training Loss: 0.21, Training Accuracy: 1.0000, Validation Loss: 0.69, Validation Accuracy: 0.8625\n",
            "Epoch: 37, Learning Rate: 1.0000000000000002e-07, Training Loss: 0.22, Training Accuracy: 0.9994, Validation Loss: 0.68, Validation Accuracy: 0.8675\n",
            "Epoch: 38, Learning Rate: 1.0000000000000002e-07, Training Loss: 0.21, Training Accuracy: 1.0000, Validation Loss: 0.71, Validation Accuracy: 0.8550\n",
            "Epoch: 39, Learning Rate: 1.0000000000000002e-07, Training Loss: 0.21, Training Accuracy: 0.9994, Validation Loss: 0.68, Validation Accuracy: 0.8650\n",
            "Epoch: 40, Learning Rate: 1.0000000000000002e-07, Training Loss: 0.20, Training Accuracy: 1.0000, Validation Loss: 0.70, Validation Accuracy: 0.8650\n",
            "Early stopping activated.\n",
            "\n",
            " Model has been saved to tensorboard_logs/2023-12-01_130404_resnet50/best_model_resnet50.pth \n",
            "\n",
            "Feature Extractor: resnet50\n",
            "Model Accuracy: 0.8750\n",
            "Params (M): 24.5576\n",
            "Size of model (MB): 93.9870\n",
            "Latency of model (ms): 13.5424\n",
            "Classifier type: MLP\n",
            "Training type: Normal\n",
            "\n",
            " Test Accuracy: 87.25%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## resnext101_32x8d - Teacher"
      ],
      "metadata": {
        "id": "l2v9BjUJ5Vo3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 50\n",
        "patience = 5\n",
        "early_stopping_patience=10\n",
        "load_model_name = \"resnext101_32x8d\"\n",
        "classifier_type='MLP'\n",
        "\n",
        "model = models.resnext101_32x8d(pretrained=True)\n",
        "\n",
        "# Replace the last fully connected layer\n",
        "num_features = model.fc.in_features\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Linear(num_features, 500),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(),\n",
        "    nn.Linear(500, 50)\n",
        ")\n",
        "\n",
        "model.to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.Adam([\n",
        "                        {'params': model.conv1.parameters(), 'lr': 1e-4},\n",
        "                        {'params': model.layer1.parameters(), 'lr': 1e-4},\n",
        "                        {'params': model.layer2.parameters(), 'lr': 1e-4},\n",
        "                        {'params': model.layer3.parameters(), 'lr': 1e-4},\n",
        "                        {'params': model.layer4.parameters(), 'lr': 1e-4},\n",
        "                        {'params': model.fc.parameters(), 'lr': 1e-8}\n",
        "                        ], lr=1e-2)\n",
        "\n",
        "\n",
        "train(model, optimizer, loss_fn, esc50_train_loader, esc50_val_loader, esc50_test_loader, load_model_name, epochs=epochs, device=device,\n",
        "      log_dir = 'tensorboard_logs', patience=patience, early_stopping_patience=early_stopping_patience, classifier_type=classifier_type)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzh0pDJSq1Hb",
        "outputId": "5cc01304-d25f-4212-f5a4-de3ee017886d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNeXt101_32X8D_Weights.IMAGENET1K_V1`. You can also use `weights=ResNeXt101_32X8D_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth\" to /root/.cache/torch/hub/checkpoints/resnext101_32x8d-8ba56ff5.pth\n",
            "100%|██████████| 340M/340M [00:05<00:00, 65.4MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Learning Rate: 0.0001, Training Loss: 3.36, Training Accuracy: 0.2544, Validation Loss: 2.69, Validation Accuracy: 0.4800\n",
            "Epoch: 2, Learning Rate: 0.0001, Training Loss: 2.43, Training Accuracy: 0.5775, Validation Loss: 1.85, Validation Accuracy: 0.6750\n",
            "Epoch: 3, Learning Rate: 0.0001, Training Loss: 1.74, Training Accuracy: 0.7619, Validation Loss: 1.53, Validation Accuracy: 0.6825\n",
            "Epoch: 4, Learning Rate: 0.0001, Training Loss: 1.26, Training Accuracy: 0.8494, Validation Loss: 1.17, Validation Accuracy: 0.7800\n",
            "Epoch: 5, Learning Rate: 0.0001, Training Loss: 0.91, Training Accuracy: 0.9169, Validation Loss: 1.06, Validation Accuracy: 0.7825\n",
            "Epoch: 6, Learning Rate: 0.0001, Training Loss: 0.71, Training Accuracy: 0.9387, Validation Loss: 0.95, Validation Accuracy: 0.8300\n",
            "Epoch 00007: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch 00007: reducing learning rate of group 1 to 1.0000e-05.\n",
            "Epoch 00007: reducing learning rate of group 2 to 1.0000e-05.\n",
            "Epoch 00007: reducing learning rate of group 3 to 1.0000e-05.\n",
            "Epoch 00007: reducing learning rate of group 4 to 1.0000e-05.\n",
            "Epoch: 7, Learning Rate: 1e-05, Training Loss: 0.59, Training Accuracy: 0.9556, Validation Loss: 0.89, Validation Accuracy: 0.8175\n",
            "Epoch: 8, Learning Rate: 1e-05, Training Loss: 0.38, Training Accuracy: 0.9888, Validation Loss: 0.70, Validation Accuracy: 0.8625\n",
            "Epoch: 9, Learning Rate: 1e-05, Training Loss: 0.33, Training Accuracy: 0.9969, Validation Loss: 0.70, Validation Accuracy: 0.8725\n",
            "Epoch: 10, Learning Rate: 1e-05, Training Loss: 0.29, Training Accuracy: 1.0000, Validation Loss: 0.64, Validation Accuracy: 0.8825\n",
            "Epoch: 11, Learning Rate: 1e-05, Training Loss: 0.28, Training Accuracy: 0.9988, Validation Loss: 0.64, Validation Accuracy: 0.8825\n",
            "Epoch: 12, Learning Rate: 1e-05, Training Loss: 0.26, Training Accuracy: 0.9994, Validation Loss: 0.63, Validation Accuracy: 0.8750\n",
            "Epoch 00013: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Epoch 00013: reducing learning rate of group 1 to 1.0000e-06.\n",
            "Epoch 00013: reducing learning rate of group 2 to 1.0000e-06.\n",
            "Epoch 00013: reducing learning rate of group 3 to 1.0000e-06.\n",
            "Epoch 00013: reducing learning rate of group 4 to 1.0000e-06.\n",
            "Epoch: 13, Learning Rate: 1.0000000000000002e-06, Training Loss: 0.25, Training Accuracy: 0.9994, Validation Loss: 0.63, Validation Accuracy: 0.8700\n",
            "Epoch: 14, Learning Rate: 1.0000000000000002e-06, Training Loss: 0.24, Training Accuracy: 0.9988, Validation Loss: 0.62, Validation Accuracy: 0.8800\n",
            "Epoch: 15, Learning Rate: 1.0000000000000002e-06, Training Loss: 0.23, Training Accuracy: 0.9981, Validation Loss: 0.63, Validation Accuracy: 0.8725\n",
            "Epoch: 16, Learning Rate: 1.0000000000000002e-06, Training Loss: 0.23, Training Accuracy: 0.9981, Validation Loss: 0.61, Validation Accuracy: 0.8800\n",
            "Epoch: 17, Learning Rate: 1.0000000000000002e-06, Training Loss: 0.24, Training Accuracy: 0.9994, Validation Loss: 0.61, Validation Accuracy: 0.8800\n",
            "Epoch: 18, Learning Rate: 1.0000000000000002e-06, Training Loss: 0.23, Training Accuracy: 0.9981, Validation Loss: 0.60, Validation Accuracy: 0.8750\n",
            "Epoch 00019: reducing learning rate of group 0 to 1.0000e-07.\n",
            "Epoch 00019: reducing learning rate of group 1 to 1.0000e-07.\n",
            "Epoch 00019: reducing learning rate of group 2 to 1.0000e-07.\n",
            "Epoch 00019: reducing learning rate of group 3 to 1.0000e-07.\n",
            "Epoch 00019: reducing learning rate of group 4 to 1.0000e-07.\n",
            "Epoch: 19, Learning Rate: 1.0000000000000002e-07, Training Loss: 0.22, Training Accuracy: 1.0000, Validation Loss: 0.60, Validation Accuracy: 0.8825\n",
            "Epoch: 20, Learning Rate: 1.0000000000000002e-07, Training Loss: 0.22, Training Accuracy: 0.9994, Validation Loss: 0.61, Validation Accuracy: 0.8875\n",
            "Epoch: 21, Learning Rate: 1.0000000000000002e-07, Training Loss: 0.22, Training Accuracy: 0.9994, Validation Loss: 0.63, Validation Accuracy: 0.8775\n",
            "Epoch: 22, Learning Rate: 1.0000000000000002e-07, Training Loss: 0.22, Training Accuracy: 0.9988, Validation Loss: 0.63, Validation Accuracy: 0.8850\n",
            "Epoch: 23, Learning Rate: 1.0000000000000002e-07, Training Loss: 0.23, Training Accuracy: 0.9994, Validation Loss: 0.62, Validation Accuracy: 0.8800\n",
            "Epoch: 24, Learning Rate: 1.0000000000000002e-07, Training Loss: 0.23, Training Accuracy: 1.0000, Validation Loss: 0.59, Validation Accuracy: 0.8875\n",
            "Epoch 00025: reducing learning rate of group 0 to 1.0000e-08.\n",
            "Epoch 00025: reducing learning rate of group 1 to 1.0000e-08.\n",
            "Epoch 00025: reducing learning rate of group 2 to 1.0000e-08.\n",
            "Epoch 00025: reducing learning rate of group 3 to 1.0000e-08.\n",
            "Epoch 00025: reducing learning rate of group 4 to 1.0000e-08.\n",
            "Epoch: 25, Learning Rate: 1.0000000000000004e-08, Training Loss: 0.23, Training Accuracy: 0.9988, Validation Loss: 0.61, Validation Accuracy: 0.8775\n",
            "Epoch: 26, Learning Rate: 1.0000000000000004e-08, Training Loss: 0.22, Training Accuracy: 0.9994, Validation Loss: 0.60, Validation Accuracy: 0.8850\n",
            "Epoch: 27, Learning Rate: 1.0000000000000004e-08, Training Loss: 0.23, Training Accuracy: 0.9994, Validation Loss: 0.60, Validation Accuracy: 0.8700\n",
            "Epoch: 28, Learning Rate: 1.0000000000000004e-08, Training Loss: 0.23, Training Accuracy: 0.9994, Validation Loss: 0.60, Validation Accuracy: 0.8850\n",
            "Epoch: 29, Learning Rate: 1.0000000000000004e-08, Training Loss: 0.23, Training Accuracy: 0.9994, Validation Loss: 0.61, Validation Accuracy: 0.8875\n",
            "Epoch: 30, Learning Rate: 1.0000000000000004e-08, Training Loss: 0.23, Training Accuracy: 0.9994, Validation Loss: 0.59, Validation Accuracy: 0.8900\n",
            "Epoch: 31, Learning Rate: 1.0000000000000004e-08, Training Loss: 0.23, Training Accuracy: 0.9988, Validation Loss: 0.63, Validation Accuracy: 0.8725\n",
            "Epoch: 32, Learning Rate: 1.0000000000000004e-08, Training Loss: 0.21, Training Accuracy: 0.9994, Validation Loss: 0.62, Validation Accuracy: 0.8825\n",
            "Epoch: 33, Learning Rate: 1.0000000000000004e-08, Training Loss: 0.23, Training Accuracy: 0.9988, Validation Loss: 0.62, Validation Accuracy: 0.8700\n",
            "Epoch: 34, Learning Rate: 1.0000000000000004e-08, Training Loss: 0.22, Training Accuracy: 0.9988, Validation Loss: 0.65, Validation Accuracy: 0.8675\n",
            "Epoch: 35, Learning Rate: 1.0000000000000004e-08, Training Loss: 0.22, Training Accuracy: 0.9981, Validation Loss: 0.62, Validation Accuracy: 0.8850\n",
            "Epoch: 36, Learning Rate: 1.0000000000000004e-08, Training Loss: 0.22, Training Accuracy: 1.0000, Validation Loss: 0.58, Validation Accuracy: 0.8850\n",
            "Epoch: 37, Learning Rate: 1.0000000000000004e-08, Training Loss: 0.23, Training Accuracy: 0.9994, Validation Loss: 0.60, Validation Accuracy: 0.8825\n",
            "Epoch: 38, Learning Rate: 1.0000000000000004e-08, Training Loss: 0.22, Training Accuracy: 1.0000, Validation Loss: 0.59, Validation Accuracy: 0.8825\n",
            "Epoch: 39, Learning Rate: 1.0000000000000004e-08, Training Loss: 0.22, Training Accuracy: 0.9994, Validation Loss: 0.61, Validation Accuracy: 0.8800\n",
            "Epoch: 40, Learning Rate: 1.0000000000000004e-08, Training Loss: 0.22, Training Accuracy: 0.9988, Validation Loss: 0.59, Validation Accuracy: 0.8900\n",
            "Epoch: 41, Learning Rate: 1.0000000000000004e-08, Training Loss: 0.22, Training Accuracy: 0.9994, Validation Loss: 0.60, Validation Accuracy: 0.8825\n",
            "Early stopping activated.\n",
            "\n",
            " Model has been saved to tensorboard_logs/2023-11-29_060228_resnext101_32x8d/best_model_resnext101_32x8d.pth \n",
            "\n",
            "Feature Extractor: resnext101_32x8d\n",
            "Model Accuracy: 0.8900\n",
            "Params (M): 87.7919\n",
            "Size of model (MB): 335.8824\n",
            "Latency of model (ms): 123.3936\n",
            "Classifier type: MLP\n",
            "Training type: Normal\n",
            "\n",
            " Test Accuracy: 88.25%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ResNet18 - Student"
      ],
      "metadata": {
        "id": "6WImy5Ud6kQq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 50\n",
        "patience = 15\n",
        "early_stopping_patience=20\n",
        "load_model_name = \"resnet18\"\n",
        "classifier_type='MLP'\n",
        "\n",
        "#spec_resnet = models.resnet50(pretrained=True)\n",
        "student_model= models.resnet18(pretrained=False)\n",
        "\n",
        "student_model.fc = nn.Sequential(nn.Linear(student_model.fc.in_features,500),\n",
        "                               nn.ReLU(),\n",
        "                               nn.Dropout(),\n",
        "                               nn.Linear(500,50))\n",
        "student_model.to(device)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.Adam([\n",
        "                        {'params': student_model.conv1.parameters(), 'lr': 1e-4},\n",
        "                        {'params': student_model.layer1.parameters(), 'lr': 1e-4},\n",
        "                        {'params': student_model.layer2.parameters(), 'lr': 1e-4},\n",
        "                        {'params': student_model.layer3.parameters(), 'lr': 1e-4},\n",
        "                        {'params': student_model.layer4.parameters(), 'lr': 1e-4},\n",
        "                        {'params': student_model.fc.parameters(), 'lr': 1e-8}\n",
        "                        ], lr=1e-2)\n",
        "\n",
        "# load teacher model\n",
        "model = models.resnext101_32x8d(pretrained=False)\n",
        "# Replace the last fully connected layer\n",
        "num_features = model.fc.in_features\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Linear(num_features, 500),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(),\n",
        "    nn.Linear(500, 50)\n",
        ")\n",
        "model_path = 'tensorboard_logs/2023-11-27_062728_resnet50/best_model_resnet50.pth'\n",
        "# model_path = 'tensorboard_logs/2023-11-13_075020_resnext101_32x8d/best_model_resnext101_32x8d.pth'\n",
        "model_path = 'tensorboard_logs/2023-11-28_071915_resnext101_32x8d/best_model_resnext101_32x8d.pth '\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "model.to(device)\n",
        "\n",
        "nn_deep = model\n",
        "\n",
        "new_nn_light = student_model\n",
        "\n",
        "train_knowledge_distillation(teacher=nn_deep, student=new_nn_light, optimizer=optimizer, ce_loss=loss_fn, train_loader=esc50_train_loader,\n",
        "                             val_loader=esc50_val_loader, test_loader=esc50_test_loader, load_model_name=load_model_name,\n",
        "                             epochs=epochs, T=2, soft_target_loss_weight=0.25, ce_loss_weight=0.75, device=device,\n",
        "                             log_dir = 'tensorboard_logs', patience=patience, early_stopping_patience=early_stopping_patience,\n",
        "                             classifier_type=classifier_type)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaeWRABCwvt1",
        "outputId": "f3ff1ea5-e323-486c-e2ac-2120312224bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Learning Rate: 0.0001, Training Loss: 6.69, Training Accuracy: 0.0606, Validation Loss: 3.53, Validation Accuracy: 0.1075\n",
            "Epoch 2, Learning Rate: 0.0001, Training Loss: 6.48, Training Accuracy: 0.1044, Validation Loss: 3.37, Validation Accuracy: 0.1575\n",
            "Epoch 3, Learning Rate: 0.0001, Training Loss: 6.35, Training Accuracy: 0.1556, Validation Loss: 3.25, Validation Accuracy: 0.2175\n",
            "Epoch 4, Learning Rate: 0.0001, Training Loss: 6.20, Training Accuracy: 0.2181, Validation Loss: 3.12, Validation Accuracy: 0.2600\n",
            "Epoch 5, Learning Rate: 0.0001, Training Loss: 6.10, Training Accuracy: 0.2406, Validation Loss: 3.03, Validation Accuracy: 0.2525\n",
            "Epoch 6, Learning Rate: 0.0001, Training Loss: 5.96, Training Accuracy: 0.3056, Validation Loss: 2.90, Validation Accuracy: 0.3125\n",
            "Epoch 7, Learning Rate: 0.0001, Training Loss: 5.82, Training Accuracy: 0.3613, Validation Loss: 2.76, Validation Accuracy: 0.3675\n",
            "Epoch 8, Learning Rate: 0.0001, Training Loss: 5.68, Training Accuracy: 0.4175, Validation Loss: 2.67, Validation Accuracy: 0.3775\n",
            "Epoch 9, Learning Rate: 0.0001, Training Loss: 5.57, Training Accuracy: 0.4625, Validation Loss: 2.62, Validation Accuracy: 0.4350\n",
            "Epoch 10, Learning Rate: 0.0001, Training Loss: 5.43, Training Accuracy: 0.5194, Validation Loss: 2.52, Validation Accuracy: 0.4500\n",
            "Epoch 11, Learning Rate: 0.0001, Training Loss: 5.30, Training Accuracy: 0.5656, Validation Loss: 2.44, Validation Accuracy: 0.4600\n",
            "Epoch 12, Learning Rate: 0.0001, Training Loss: 5.19, Training Accuracy: 0.6062, Validation Loss: 2.44, Validation Accuracy: 0.4425\n",
            "Epoch 13, Learning Rate: 0.0001, Training Loss: 5.05, Training Accuracy: 0.6456, Validation Loss: 2.17, Validation Accuracy: 0.5325\n",
            "Epoch 14, Learning Rate: 0.0001, Training Loss: 4.95, Training Accuracy: 0.6963, Validation Loss: 2.33, Validation Accuracy: 0.5225\n",
            "Epoch 15, Learning Rate: 0.0001, Training Loss: 4.79, Training Accuracy: 0.7500, Validation Loss: 2.18, Validation Accuracy: 0.5325\n",
            "Epoch 16, Learning Rate: 0.0001, Training Loss: 4.70, Training Accuracy: 0.7675, Validation Loss: 2.09, Validation Accuracy: 0.5500\n",
            "Epoch 00017: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch 00017: reducing learning rate of group 1 to 1.0000e-05.\n",
            "Epoch 00017: reducing learning rate of group 2 to 1.0000e-05.\n",
            "Epoch 00017: reducing learning rate of group 3 to 1.0000e-05.\n",
            "Epoch 00017: reducing learning rate of group 4 to 1.0000e-05.\n",
            "Epoch 17, Learning Rate: 1e-05, Training Loss: 4.58, Training Accuracy: 0.8006, Validation Loss: 2.04, Validation Accuracy: 0.5400\n",
            "Epoch 18, Learning Rate: 1e-05, Training Loss: 4.39, Training Accuracy: 0.8675, Validation Loss: 1.86, Validation Accuracy: 0.6425\n",
            "Epoch 19, Learning Rate: 1e-05, Training Loss: 4.30, Training Accuracy: 0.8969, Validation Loss: 1.87, Validation Accuracy: 0.6400\n",
            "Epoch 20, Learning Rate: 1e-05, Training Loss: 4.30, Training Accuracy: 0.8931, Validation Loss: 1.87, Validation Accuracy: 0.6250\n",
            "Epoch 21, Learning Rate: 1e-05, Training Loss: 4.26, Training Accuracy: 0.9125, Validation Loss: 1.85, Validation Accuracy: 0.6450\n",
            "Epoch 22, Learning Rate: 1e-05, Training Loss: 4.24, Training Accuracy: 0.9175, Validation Loss: 1.86, Validation Accuracy: 0.6425\n",
            "Epoch 23, Learning Rate: 1e-05, Training Loss: 4.23, Training Accuracy: 0.9181, Validation Loss: 1.84, Validation Accuracy: 0.6450\n",
            "Epoch 24, Learning Rate: 1e-05, Training Loss: 4.19, Training Accuracy: 0.9131, Validation Loss: 1.86, Validation Accuracy: 0.6325\n",
            "Epoch 25, Learning Rate: 1e-05, Training Loss: 4.17, Training Accuracy: 0.9313, Validation Loss: 1.85, Validation Accuracy: 0.6425\n",
            "Epoch 26, Learning Rate: 1e-05, Training Loss: 4.16, Training Accuracy: 0.9231, Validation Loss: 1.85, Validation Accuracy: 0.6425\n",
            "Epoch 27, Learning Rate: 1e-05, Training Loss: 4.15, Training Accuracy: 0.9325, Validation Loss: 1.82, Validation Accuracy: 0.6525\n",
            "Epoch 28, Learning Rate: 1e-05, Training Loss: 4.14, Training Accuracy: 0.9331, Validation Loss: 1.81, Validation Accuracy: 0.6400\n",
            "Epoch 29, Learning Rate: 1e-05, Training Loss: 4.13, Training Accuracy: 0.9437, Validation Loss: 1.81, Validation Accuracy: 0.6400\n",
            "Epoch 30, Learning Rate: 1e-05, Training Loss: 4.10, Training Accuracy: 0.9537, Validation Loss: 1.83, Validation Accuracy: 0.6700\n",
            "Epoch 31, Learning Rate: 1e-05, Training Loss: 4.11, Training Accuracy: 0.9431, Validation Loss: 1.82, Validation Accuracy: 0.6475\n",
            "Epoch 32, Learning Rate: 1e-05, Training Loss: 4.08, Training Accuracy: 0.9506, Validation Loss: 1.83, Validation Accuracy: 0.6650\n",
            "Epoch 00033: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Epoch 00033: reducing learning rate of group 1 to 1.0000e-06.\n",
            "Epoch 00033: reducing learning rate of group 2 to 1.0000e-06.\n",
            "Epoch 00033: reducing learning rate of group 3 to 1.0000e-06.\n",
            "Epoch 00033: reducing learning rate of group 4 to 1.0000e-06.\n",
            "Epoch 33, Learning Rate: 1.0000000000000002e-06, Training Loss: 4.04, Training Accuracy: 0.9613, Validation Loss: 1.80, Validation Accuracy: 0.6500\n",
            "Epoch 34, Learning Rate: 1.0000000000000002e-06, Training Loss: 4.05, Training Accuracy: 0.9569, Validation Loss: 1.79, Validation Accuracy: 0.6400\n",
            "Epoch 35, Learning Rate: 1.0000000000000002e-06, Training Loss: 4.03, Training Accuracy: 0.9606, Validation Loss: 1.79, Validation Accuracy: 0.6650\n",
            "Epoch 36, Learning Rate: 1.0000000000000002e-06, Training Loss: 4.03, Training Accuracy: 0.9619, Validation Loss: 1.80, Validation Accuracy: 0.6525\n",
            "Epoch 37, Learning Rate: 1.0000000000000002e-06, Training Loss: 4.04, Training Accuracy: 0.9569, Validation Loss: 1.79, Validation Accuracy: 0.6750\n",
            "Epoch 38, Learning Rate: 1.0000000000000002e-06, Training Loss: 4.04, Training Accuracy: 0.9556, Validation Loss: 1.79, Validation Accuracy: 0.6700\n",
            "Epoch 39, Learning Rate: 1.0000000000000002e-06, Training Loss: 4.04, Training Accuracy: 0.9550, Validation Loss: 1.79, Validation Accuracy: 0.6550\n",
            "Epoch 40, Learning Rate: 1.0000000000000002e-06, Training Loss: 4.04, Training Accuracy: 0.9581, Validation Loss: 1.81, Validation Accuracy: 0.6525\n",
            "Epoch 41, Learning Rate: 1.0000000000000002e-06, Training Loss: 4.03, Training Accuracy: 0.9613, Validation Loss: 1.81, Validation Accuracy: 0.6525\n",
            "Epoch 42, Learning Rate: 1.0000000000000002e-06, Training Loss: 4.04, Training Accuracy: 0.9619, Validation Loss: 1.83, Validation Accuracy: 0.6350\n",
            "Epoch 43, Learning Rate: 1.0000000000000002e-06, Training Loss: 4.03, Training Accuracy: 0.9537, Validation Loss: 1.80, Validation Accuracy: 0.6625\n",
            "Epoch 44, Learning Rate: 1.0000000000000002e-06, Training Loss: 4.03, Training Accuracy: 0.9587, Validation Loss: 1.80, Validation Accuracy: 0.6425\n",
            "Epoch 45, Learning Rate: 1.0000000000000002e-06, Training Loss: 4.02, Training Accuracy: 0.9656, Validation Loss: 1.80, Validation Accuracy: 0.6575\n",
            "Epoch 46, Learning Rate: 1.0000000000000002e-06, Training Loss: 4.02, Training Accuracy: 0.9587, Validation Loss: 1.82, Validation Accuracy: 0.6425\n",
            "Epoch 47, Learning Rate: 1.0000000000000002e-06, Training Loss: 4.03, Training Accuracy: 0.9625, Validation Loss: 1.79, Validation Accuracy: 0.6525\n",
            "Epoch 48, Learning Rate: 1.0000000000000002e-06, Training Loss: 4.02, Training Accuracy: 0.9637, Validation Loss: 1.79, Validation Accuracy: 0.6650\n",
            "Epoch 00049: reducing learning rate of group 0 to 1.0000e-07.\n",
            "Epoch 00049: reducing learning rate of group 1 to 1.0000e-07.\n",
            "Epoch 00049: reducing learning rate of group 2 to 1.0000e-07.\n",
            "Epoch 00049: reducing learning rate of group 3 to 1.0000e-07.\n",
            "Epoch 00049: reducing learning rate of group 4 to 1.0000e-07.\n",
            "Epoch 49, Learning Rate: 1.0000000000000002e-07, Training Loss: 4.02, Training Accuracy: 0.9650, Validation Loss: 1.78, Validation Accuracy: 0.6650\n",
            "Epoch 50, Learning Rate: 1.0000000000000002e-07, Training Loss: 4.02, Training Accuracy: 0.9650, Validation Loss: 1.78, Validation Accuracy: 0.6500\n",
            "\n",
            " Model has been saved to tensorboard_logs/2023-11-28_100236_resnet18/best_model_resnet18.pth\n",
            "\n",
            "Feature Extractor: resnet18\n",
            "Model Accuracy: 0.6750\n",
            "Params (M): 11.4581\n",
            "Size of model (MB): 43.7863\n",
            "Latency of model (ms): 3.8067\n",
            "Classifier type: MLP\n",
            "Training type: Teacher Student Model\n",
            "\n",
            " Test Accuracy: 63.50%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Res Block - Student"
      ],
      "metadata": {
        "id": "xulrIPqcYnNs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def model_size(model):\n",
        "    param_size = 0\n",
        "    for param in model.parameters():\n",
        "        param_size += param.nelement() * param.element_size()\n",
        "    buffer_size = 0\n",
        "    for buffer in model.buffers():\n",
        "        buffer_size += buffer.nelement() * buffer.element_size()\n",
        "\n",
        "    size_all_mb = (param_size + buffer_size) / 1024**2\n",
        "    # print('model size: {:.3f}MB'.format(size_all_mb))\n",
        "    return size_all_mb\n",
        "\n",
        "\n",
        "class block(nn.Module):\n",
        "    def __init__(\n",
        "        self, in_channels, intermediate_channels, identity_downsample=None, stride=1\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.expansion = 4\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels,\n",
        "            intermediate_channels,\n",
        "            kernel_size=1,\n",
        "            stride=1,\n",
        "            padding=0,\n",
        "            bias=False,\n",
        "        )\n",
        "        self.bn1 = nn.BatchNorm2d(intermediate_channels)\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            intermediate_channels,\n",
        "            intermediate_channels,\n",
        "            kernel_size=3,\n",
        "            stride=stride,\n",
        "            padding=1,\n",
        "            bias=False,\n",
        "        )\n",
        "        self.bn2 = nn.BatchNorm2d(intermediate_channels)\n",
        "        self.conv3 = nn.Conv2d(\n",
        "            intermediate_channels,\n",
        "            intermediate_channels * self.expansion,\n",
        "            kernel_size=1,\n",
        "            stride=1,\n",
        "            padding=0,\n",
        "            bias=False,\n",
        "        )\n",
        "        self.bn3 = nn.BatchNorm2d(intermediate_channels * self.expansion)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.identity_downsample = identity_downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x.clone()\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "\n",
        "        if self.identity_downsample is not None:\n",
        "            identity = self.identity_downsample(identity)\n",
        "\n",
        "        x += identity\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, layers, image_channels, num_classes):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_channels = 64\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            image_channels, 64, kernel_size=7, stride=2, padding=3, bias=False\n",
        "        )\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        # Essentially the entire ResNet architecture are in these 4 lines below\n",
        "        self.layer1 = self._make_layer(\n",
        "            block, layers[0], intermediate_channels=64, stride=1\n",
        "        )\n",
        "        self.layer2 = self._make_layer(\n",
        "            block, layers[1], intermediate_channels=128, stride=2\n",
        "        )\n",
        "        self.layer3 = self._make_layer(\n",
        "            block, layers[2], intermediate_channels=256, stride=2\n",
        "        )\n",
        "        self.layer4 = self._make_layer(\n",
        "            block, layers[3], intermediate_channels=512, stride=2\n",
        "        )\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * 4, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def _make_layer(self, block, num_residual_blocks, intermediate_channels, stride):\n",
        "        identity_downsample = None\n",
        "        layers = []\n",
        "\n",
        "        # Either if we half the input space for ex, 56x56 -> 28x28 (stride=2), or channels changes\n",
        "        # we need to adapt the Identity (skip connection) so it will be able to be added\n",
        "        # to the layer that's ahead\n",
        "        if stride != 1 or self.in_channels != intermediate_channels * 4:\n",
        "            identity_downsample = nn.Sequential(\n",
        "                nn.Conv2d(\n",
        "                    self.in_channels,\n",
        "                    intermediate_channels * 4,\n",
        "                    kernel_size=1,\n",
        "                    stride=stride,\n",
        "                    bias=False,\n",
        "                ),\n",
        "                nn.BatchNorm2d(intermediate_channels * 4),\n",
        "            )\n",
        "\n",
        "        layers.append(\n",
        "            block(self.in_channels, intermediate_channels, identity_downsample, stride)\n",
        "        )\n",
        "\n",
        "        # The expansion size is always 4 for ResNet 50,101,152\n",
        "        self.in_channels = intermediate_channels * 4\n",
        "\n",
        "        # For example for first resnet layer: 256 will be mapped to 64 as intermediate layer,\n",
        "        # then finally back to 256. Hence no identity downsample is needed, since stride = 1,\n",
        "        # and also same amount of channels.\n",
        "        for i in range(num_residual_blocks - 1):\n",
        "            layers.append(block(self.in_channels, intermediate_channels))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "def ResNet50(img_channel=3, num_classes=1000):\n",
        "    return ResNet(block, [3, 4, 6, 3], img_channel, num_classes)\n",
        "\n",
        "def ResNetRaw(img_channel=3, num_classes=50):\n",
        "    return ResNet(block, [1, 2, 2, 1], img_channel, num_classes)\n",
        "\n",
        "def ResNetRaw1(img_channel=3, num_classes=50):\n",
        "    return ResNet(block, [1, 1, 1, 1], img_channel, num_classes)\n",
        "\n",
        "net = ResNetRaw1(img_channel=3, num_classes=50).to(device)\n",
        "print(model_size(net))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79VqozAqt64n",
        "outputId": "8f963428-f63a-45b0-d497-ebef0952889e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "31.043045043945312\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 50\n",
        "patience = 20\n",
        "early_stopping_patience=30\n",
        "load_model_name = \"ResNetRaw\"\n",
        "classifier_type='MLP'\n",
        "\n",
        "#spec_resnet = models.resnet50(pretrained=True)\n",
        "# student_model= models.resnet18(pretrained=False)\n",
        "student_model = ResNetRaw(img_channel=3, num_classes=50)\n",
        "\n",
        "student_model.fc = nn.Sequential(nn.Linear(student_model.fc.in_features,500),\n",
        "                               nn.ReLU(),\n",
        "                               nn.Dropout(),\n",
        "                               nn.Linear(500,50))\n",
        "student_model.to(device)\n",
        "print(model_size(student_model))\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.Adam([\n",
        "                        {'params': student_model.conv1.parameters(), 'lr': 1e-4},\n",
        "                        {'params': student_model.layer1.parameters(), 'lr': 1e-4},\n",
        "                        {'params': student_model.layer2.parameters(), 'lr': 1e-4},\n",
        "                        {'params': student_model.layer3.parameters(), 'lr': 1e-4},\n",
        "                        {'params': student_model.layer4.parameters(), 'lr': 1e-4},\n",
        "                        {'params': student_model.fc.parameters(), 'lr': 1e-8}\n",
        "                        ], lr=1e-2)\n",
        "\n",
        "nn_deep = model\n",
        "new_nn_light = student_model\n",
        "\n",
        "train_knowledge_distillation(teacher=nn_deep, student=new_nn_light, optimizer=optimizer, ce_loss=loss_fn, train_loader=esc50_train_loader,\n",
        "                             val_loader=esc50_val_loader, test_loader=esc50_test_loader, load_model_name=load_model_name,\n",
        "                             epochs=epochs, T=2, soft_target_loss_weight=0.25, ce_loss_weight=0.75, device=device,\n",
        "                             log_dir = 'tensorboard_logs', patience=patience, early_stopping_patience=early_stopping_patience,\n",
        "                             classifier_type=classifier_type)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGjIGI1Kim8g",
        "outputId": "ba67bf60-441e-47f7-c3cf-0f5df2915f54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40.00364685058594\n",
            "Epoch 1, Learning Rate: 0.0001, Training Loss: 6.69, Training Accuracy: 0.0737, Validation Loss: 3.53, Validation Accuracy: 0.1000\n",
            "Epoch 2, Learning Rate: 0.0001, Training Loss: 6.45, Training Accuracy: 0.1250, Validation Loss: 3.33, Validation Accuracy: 0.1450\n",
            "Epoch 3, Learning Rate: 0.0001, Training Loss: 6.24, Training Accuracy: 0.1819, Validation Loss: 3.17, Validation Accuracy: 0.1950\n",
            "Epoch 4, Learning Rate: 0.0001, Training Loss: 6.10, Training Accuracy: 0.2263, Validation Loss: 3.17, Validation Accuracy: 0.2200\n",
            "Epoch 5, Learning Rate: 0.0001, Training Loss: 5.95, Training Accuracy: 0.2769, Validation Loss: 2.85, Validation Accuracy: 0.2850\n",
            "Epoch 6, Learning Rate: 0.0001, Training Loss: 5.84, Training Accuracy: 0.3162, Validation Loss: 2.93, Validation Accuracy: 0.2925\n",
            "Epoch 7, Learning Rate: 0.0001, Training Loss: 5.70, Training Accuracy: 0.3731, Validation Loss: 2.69, Validation Accuracy: 0.3600\n",
            "Epoch 8, Learning Rate: 0.0001, Training Loss: 5.59, Training Accuracy: 0.4256, Validation Loss: 2.72, Validation Accuracy: 0.2950\n",
            "Epoch 9, Learning Rate: 0.0001, Training Loss: 5.45, Training Accuracy: 0.4800, Validation Loss: 2.57, Validation Accuracy: 0.4025\n",
            "Epoch 10, Learning Rate: 0.0001, Training Loss: 5.30, Training Accuracy: 0.5350, Validation Loss: 2.38, Validation Accuracy: 0.4575\n",
            "Epoch 11, Learning Rate: 0.0001, Training Loss: 5.21, Training Accuracy: 0.5850, Validation Loss: 2.40, Validation Accuracy: 0.4475\n",
            "Epoch 00012: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch 00012: reducing learning rate of group 1 to 1.0000e-05.\n",
            "Epoch 00012: reducing learning rate of group 2 to 1.0000e-05.\n",
            "Epoch 00012: reducing learning rate of group 3 to 1.0000e-05.\n",
            "Epoch 00012: reducing learning rate of group 4 to 1.0000e-05.\n",
            "Epoch 12, Learning Rate: 1e-05, Training Loss: 5.08, Training Accuracy: 0.5944, Validation Loss: 2.24, Validation Accuracy: 0.4950\n",
            "Epoch 13, Learning Rate: 1e-05, Training Loss: 4.85, Training Accuracy: 0.7044, Validation Loss: 2.12, Validation Accuracy: 0.5575\n",
            "Epoch 14, Learning Rate: 1e-05, Training Loss: 4.79, Training Accuracy: 0.7169, Validation Loss: 2.08, Validation Accuracy: 0.5575\n",
            "Epoch 15, Learning Rate: 1e-05, Training Loss: 4.76, Training Accuracy: 0.7369, Validation Loss: 2.08, Validation Accuracy: 0.5600\n",
            "Epoch 16, Learning Rate: 1e-05, Training Loss: 4.71, Training Accuracy: 0.7556, Validation Loss: 2.05, Validation Accuracy: 0.5675\n",
            "Epoch 17, Learning Rate: 1e-05, Training Loss: 4.68, Training Accuracy: 0.7600, Validation Loss: 2.06, Validation Accuracy: 0.5575\n",
            "Epoch 18, Learning Rate: 1e-05, Training Loss: 4.66, Training Accuracy: 0.7825, Validation Loss: 2.02, Validation Accuracy: 0.5775\n",
            "Epoch 19, Learning Rate: 1e-05, Training Loss: 4.63, Training Accuracy: 0.7794, Validation Loss: 2.03, Validation Accuracy: 0.5750\n",
            "Epoch 20, Learning Rate: 1e-05, Training Loss: 4.59, Training Accuracy: 0.7913, Validation Loss: 2.02, Validation Accuracy: 0.5800\n",
            "Epoch 21, Learning Rate: 1e-05, Training Loss: 4.56, Training Accuracy: 0.8050, Validation Loss: 2.03, Validation Accuracy: 0.5800\n",
            "Epoch 22, Learning Rate: 1e-05, Training Loss: 4.56, Training Accuracy: 0.8113, Validation Loss: 2.01, Validation Accuracy: 0.5750\n",
            "Epoch 00023: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Epoch 00023: reducing learning rate of group 1 to 1.0000e-06.\n",
            "Epoch 00023: reducing learning rate of group 2 to 1.0000e-06.\n",
            "Epoch 00023: reducing learning rate of group 3 to 1.0000e-06.\n",
            "Epoch 00023: reducing learning rate of group 4 to 1.0000e-06.\n",
            "Epoch 23, Learning Rate: 1.0000000000000002e-06, Training Loss: 4.56, Training Accuracy: 0.8069, Validation Loss: 2.01, Validation Accuracy: 0.5725\n",
            "Epoch 24, Learning Rate: 1.0000000000000002e-06, Training Loss: 4.50, Training Accuracy: 0.8275, Validation Loss: 2.01, Validation Accuracy: 0.5900\n",
            "Epoch 25, Learning Rate: 1.0000000000000002e-06, Training Loss: 4.49, Training Accuracy: 0.8438, Validation Loss: 2.00, Validation Accuracy: 0.5975\n",
            "Epoch 26, Learning Rate: 1.0000000000000002e-06, Training Loss: 4.49, Training Accuracy: 0.8363, Validation Loss: 1.99, Validation Accuracy: 0.5900\n",
            "Epoch 27, Learning Rate: 1.0000000000000002e-06, Training Loss: 4.49, Training Accuracy: 0.8387, Validation Loss: 2.00, Validation Accuracy: 0.5800\n",
            "Epoch 28, Learning Rate: 1.0000000000000002e-06, Training Loss: 4.48, Training Accuracy: 0.8263, Validation Loss: 1.98, Validation Accuracy: 0.5875\n",
            "Epoch 29, Learning Rate: 1.0000000000000002e-06, Training Loss: 4.49, Training Accuracy: 0.8300, Validation Loss: 1.97, Validation Accuracy: 0.6050\n",
            "Epoch 30, Learning Rate: 1.0000000000000002e-06, Training Loss: 4.48, Training Accuracy: 0.8350, Validation Loss: 1.97, Validation Accuracy: 0.5800\n",
            "Epoch 31, Learning Rate: 1.0000000000000002e-06, Training Loss: 4.47, Training Accuracy: 0.8350, Validation Loss: 1.97, Validation Accuracy: 0.6000\n",
            "Epoch 32, Learning Rate: 1.0000000000000002e-06, Training Loss: 4.47, Training Accuracy: 0.8444, Validation Loss: 1.98, Validation Accuracy: 0.5975\n",
            "Epoch 33, Learning Rate: 1.0000000000000002e-06, Training Loss: 4.46, Training Accuracy: 0.8431, Validation Loss: 1.98, Validation Accuracy: 0.5950\n",
            "Epoch 00034: reducing learning rate of group 0 to 1.0000e-07.\n",
            "Epoch 00034: reducing learning rate of group 1 to 1.0000e-07.\n",
            "Epoch 00034: reducing learning rate of group 2 to 1.0000e-07.\n",
            "Epoch 00034: reducing learning rate of group 3 to 1.0000e-07.\n",
            "Epoch 00034: reducing learning rate of group 4 to 1.0000e-07.\n",
            "Epoch 34, Learning Rate: 1.0000000000000002e-07, Training Loss: 4.48, Training Accuracy: 0.8413, Validation Loss: 2.00, Validation Accuracy: 0.5925\n",
            "Epoch 35, Learning Rate: 1.0000000000000002e-07, Training Loss: 4.45, Training Accuracy: 0.8575, Validation Loss: 2.00, Validation Accuracy: 0.5900\n",
            "Epoch 36, Learning Rate: 1.0000000000000002e-07, Training Loss: 4.45, Training Accuracy: 0.8444, Validation Loss: 1.98, Validation Accuracy: 0.5850\n",
            "Epoch 37, Learning Rate: 1.0000000000000002e-07, Training Loss: 4.47, Training Accuracy: 0.8313, Validation Loss: 1.99, Validation Accuracy: 0.5925\n",
            "Epoch 38, Learning Rate: 1.0000000000000002e-07, Training Loss: 4.47, Training Accuracy: 0.8400, Validation Loss: 1.98, Validation Accuracy: 0.5875\n",
            "Epoch 39, Learning Rate: 1.0000000000000002e-07, Training Loss: 4.46, Training Accuracy: 0.8375, Validation Loss: 1.97, Validation Accuracy: 0.5925\n",
            "Epoch 40, Learning Rate: 1.0000000000000002e-07, Training Loss: 4.45, Training Accuracy: 0.8444, Validation Loss: 1.98, Validation Accuracy: 0.5950\n",
            "Epoch 41, Learning Rate: 1.0000000000000002e-07, Training Loss: 4.47, Training Accuracy: 0.8375, Validation Loss: 1.98, Validation Accuracy: 0.5950\n",
            "Epoch 42, Learning Rate: 1.0000000000000002e-07, Training Loss: 4.44, Training Accuracy: 0.8450, Validation Loss: 1.98, Validation Accuracy: 0.5975\n",
            "Epoch 43, Learning Rate: 1.0000000000000002e-07, Training Loss: 4.44, Training Accuracy: 0.8356, Validation Loss: 1.96, Validation Accuracy: 0.6025\n",
            "Epoch 44, Learning Rate: 1.0000000000000002e-07, Training Loss: 4.46, Training Accuracy: 0.8350, Validation Loss: 1.98, Validation Accuracy: 0.6050\n",
            "Epoch 00045: reducing learning rate of group 0 to 1.0000e-08.\n",
            "Epoch 00045: reducing learning rate of group 1 to 1.0000e-08.\n",
            "Epoch 00045: reducing learning rate of group 2 to 1.0000e-08.\n",
            "Epoch 00045: reducing learning rate of group 3 to 1.0000e-08.\n",
            "Epoch 00045: reducing learning rate of group 4 to 1.0000e-08.\n",
            "Epoch 45, Learning Rate: 1.0000000000000004e-08, Training Loss: 4.45, Training Accuracy: 0.8356, Validation Loss: 1.99, Validation Accuracy: 0.5925\n",
            "Epoch 46, Learning Rate: 1.0000000000000004e-08, Training Loss: 4.46, Training Accuracy: 0.8294, Validation Loss: 1.98, Validation Accuracy: 0.6025\n",
            "Epoch 47, Learning Rate: 1.0000000000000004e-08, Training Loss: 4.46, Training Accuracy: 0.8456, Validation Loss: 1.98, Validation Accuracy: 0.6075\n",
            "Epoch 48, Learning Rate: 1.0000000000000004e-08, Training Loss: 4.44, Training Accuracy: 0.8644, Validation Loss: 1.99, Validation Accuracy: 0.5925\n",
            "Epoch 49, Learning Rate: 1.0000000000000004e-08, Training Loss: 4.47, Training Accuracy: 0.8394, Validation Loss: 1.98, Validation Accuracy: 0.5825\n",
            "Epoch 50, Learning Rate: 1.0000000000000004e-08, Training Loss: 4.44, Training Accuracy: 0.8500, Validation Loss: 1.98, Validation Accuracy: 0.5850\n",
            "\n",
            " Model has been saved to tensorboard_logs/2023-11-28_104956_ResNetRaw/best_model_ResNetRaw.pth\n",
            "\n",
            "Feature Extractor: ResNetRaw\n",
            "Model Accuracy: 0.6075\n",
            "Params (M): 10.4627\n",
            "Size of model (MB): 40.0504\n",
            "Latency of model (ms): 4.1533\n",
            "Classifier type: MLP\n",
            "Training type: Teacher Student Model\n",
            "\n",
            " Test Accuracy: 59.75%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 50\n",
        "patience = 20\n",
        "early_stopping_patience=30\n",
        "load_model_name = \"ResNetRaw\"\n",
        "classifier_type='MLP'\n",
        "\n",
        "#spec_resnet = models.resnet50(pretrained=True)\n",
        "# student_model= models.resnet18(pretrained=False)\n",
        "student_model = ResNetRaw(img_channel=3, num_classes=50)\n",
        "\n",
        "student_model.fc = nn.Sequential(nn.Linear(student_model.fc.in_features,500),\n",
        "                               nn.ReLU(),\n",
        "                               nn.Dropout(),\n",
        "                               nn.Linear(500,50))\n",
        "student_model.to(device)\n",
        "print(model_size(student_model))\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.Adam([\n",
        "                        {'params': student_model.conv1.parameters(), 'lr': 1e-4},\n",
        "                        {'params': student_model.layer1.parameters(), 'lr': 1e-4},\n",
        "                        {'params': student_model.layer2.parameters(), 'lr': 1e-4},\n",
        "                        {'params': student_model.layer3.parameters(), 'lr': 1e-4},\n",
        "                        {'params': student_model.layer4.parameters(), 'lr': 1e-4},\n",
        "                        {'params': student_model.fc.parameters(), 'lr': 1e-8}\n",
        "                        ], lr=1e-2)\n",
        "\n",
        "nn_deep = model\n",
        "new_nn_light = student_model\n",
        "\n",
        "train_knowledge_distillation(teacher=nn_deep, student=new_nn_light, optimizer=optimizer, ce_loss=loss_fn, train_loader=esc50_train_loader,\n",
        "                             val_loader=esc50_val_loader, test_loader=esc50_test_loader, load_model_name=load_model_name,\n",
        "                             epochs=epochs, T=2, soft_target_loss_weight=0.25, ce_loss_weight=0.75, device=device,\n",
        "                             log_dir = 'tensorboard_logs', patience=patience, early_stopping_patience=early_stopping_patience,\n",
        "                             classifier_type=classifier_type)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WeKbOOljF6a",
        "outputId": "b9bcfe24-d55d-465a-bab6-fe2cd47e8b31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40.00364685058594\n",
            "Epoch 1, Learning Rate: 0.0001, Training Loss: 6.67, Training Accuracy: 0.0688, Validation Loss: 3.51, Validation Accuracy: 0.1100\n",
            "Epoch 2, Learning Rate: 0.0001, Training Loss: 6.41, Training Accuracy: 0.1150, Validation Loss: 3.30, Validation Accuracy: 0.2250\n",
            "Epoch 3, Learning Rate: 0.0001, Training Loss: 6.23, Training Accuracy: 0.1819, Validation Loss: 3.10, Validation Accuracy: 0.2100\n",
            "Epoch 4, Learning Rate: 0.0001, Training Loss: 6.06, Training Accuracy: 0.2469, Validation Loss: 2.98, Validation Accuracy: 0.2075\n",
            "Epoch 5, Learning Rate: 0.0001, Training Loss: 5.92, Training Accuracy: 0.2994, Validation Loss: 2.82, Validation Accuracy: 0.3250\n",
            "Epoch 6, Learning Rate: 0.0001, Training Loss: 5.78, Training Accuracy: 0.3450, Validation Loss: 2.82, Validation Accuracy: 0.2925\n",
            "Epoch 7, Learning Rate: 0.0001, Training Loss: 5.66, Training Accuracy: 0.3869, Validation Loss: 2.70, Validation Accuracy: 0.3200\n",
            "Epoch 8, Learning Rate: 0.0001, Training Loss: 5.55, Training Accuracy: 0.4225, Validation Loss: 2.53, Validation Accuracy: 0.4075\n",
            "Epoch 9, Learning Rate: 0.0001, Training Loss: 5.40, Training Accuracy: 0.4850, Validation Loss: 2.41, Validation Accuracy: 0.4550\n",
            "Epoch 10, Learning Rate: 0.0001, Training Loss: 5.27, Training Accuracy: 0.5381, Validation Loss: 2.41, Validation Accuracy: 0.4325\n",
            "Epoch 11, Learning Rate: 0.0001, Training Loss: 5.15, Training Accuracy: 0.6056, Validation Loss: 2.34, Validation Accuracy: 0.4675\n",
            "Epoch 12, Learning Rate: 0.0001, Training Loss: 5.03, Training Accuracy: 0.6331, Validation Loss: 2.28, Validation Accuracy: 0.5050\n",
            "Epoch 13, Learning Rate: 0.0001, Training Loss: 4.91, Training Accuracy: 0.6700, Validation Loss: 2.21, Validation Accuracy: 0.5150\n",
            "Epoch 14, Learning Rate: 0.0001, Training Loss: 4.75, Training Accuracy: 0.7262, Validation Loss: 2.16, Validation Accuracy: 0.5075\n",
            "Epoch 15, Learning Rate: 0.0001, Training Loss: 4.63, Training Accuracy: 0.7725, Validation Loss: 2.08, Validation Accuracy: 0.5550\n",
            "Epoch 16, Learning Rate: 0.0001, Training Loss: 4.49, Training Accuracy: 0.7987, Validation Loss: 2.18, Validation Accuracy: 0.4900\n",
            "Epoch 17, Learning Rate: 0.0001, Training Loss: 4.38, Training Accuracy: 0.8237, Validation Loss: 2.00, Validation Accuracy: 0.5375\n",
            "Epoch 18, Learning Rate: 0.0001, Training Loss: 4.24, Training Accuracy: 0.8650, Validation Loss: 2.02, Validation Accuracy: 0.5625\n",
            "Epoch 19, Learning Rate: 0.0001, Training Loss: 4.17, Training Accuracy: 0.8969, Validation Loss: 1.93, Validation Accuracy: 0.5925\n",
            "Epoch 20, Learning Rate: 0.0001, Training Loss: 4.06, Training Accuracy: 0.9200, Validation Loss: 1.95, Validation Accuracy: 0.5450\n",
            "Epoch 21, Learning Rate: 0.0001, Training Loss: 3.98, Training Accuracy: 0.9363, Validation Loss: 2.02, Validation Accuracy: 0.5350\n",
            "Epoch 00022: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch 00022: reducing learning rate of group 1 to 1.0000e-05.\n",
            "Epoch 00022: reducing learning rate of group 2 to 1.0000e-05.\n",
            "Epoch 00022: reducing learning rate of group 3 to 1.0000e-05.\n",
            "Epoch 00022: reducing learning rate of group 4 to 1.0000e-05.\n",
            "Epoch 22, Learning Rate: 1e-05, Training Loss: 3.85, Training Accuracy: 0.9613, Validation Loss: 1.84, Validation Accuracy: 0.6150\n",
            "Epoch 23, Learning Rate: 1e-05, Training Loss: 3.68, Training Accuracy: 0.9781, Validation Loss: 1.71, Validation Accuracy: 0.6925\n",
            "Epoch 24, Learning Rate: 1e-05, Training Loss: 3.62, Training Accuracy: 0.9888, Validation Loss: 1.71, Validation Accuracy: 0.6950\n",
            "Epoch 25, Learning Rate: 1e-05, Training Loss: 3.60, Training Accuracy: 0.9906, Validation Loss: 1.70, Validation Accuracy: 0.7100\n",
            "Epoch 26, Learning Rate: 1e-05, Training Loss: 3.58, Training Accuracy: 0.9906, Validation Loss: 1.68, Validation Accuracy: 0.7050\n",
            "Epoch 27, Learning Rate: 1e-05, Training Loss: 3.57, Training Accuracy: 0.9925, Validation Loss: 1.69, Validation Accuracy: 0.7075\n",
            "Epoch 28, Learning Rate: 1e-05, Training Loss: 3.54, Training Accuracy: 0.9919, Validation Loss: 1.71, Validation Accuracy: 0.6850\n",
            "Epoch 29, Learning Rate: 1e-05, Training Loss: 3.54, Training Accuracy: 0.9956, Validation Loss: 1.69, Validation Accuracy: 0.7125\n",
            "Epoch 30, Learning Rate: 1e-05, Training Loss: 3.53, Training Accuracy: 0.9950, Validation Loss: 1.70, Validation Accuracy: 0.7025\n",
            "Epoch 31, Learning Rate: 1e-05, Training Loss: 3.51, Training Accuracy: 0.9925, Validation Loss: 1.69, Validation Accuracy: 0.7075\n",
            "Epoch 32, Learning Rate: 1e-05, Training Loss: 3.52, Training Accuracy: 0.9938, Validation Loss: 1.73, Validation Accuracy: 0.6950\n",
            "Epoch 33, Learning Rate: 1e-05, Training Loss: 3.49, Training Accuracy: 0.9925, Validation Loss: 1.73, Validation Accuracy: 0.7050\n",
            "Epoch 34, Learning Rate: 1e-05, Training Loss: 3.49, Training Accuracy: 0.9956, Validation Loss: 1.71, Validation Accuracy: 0.7075\n",
            "Epoch 35, Learning Rate: 1e-05, Training Loss: 3.48, Training Accuracy: 0.9938, Validation Loss: 1.71, Validation Accuracy: 0.7050\n",
            "Epoch 36, Learning Rate: 1e-05, Training Loss: 3.46, Training Accuracy: 0.9962, Validation Loss: 1.71, Validation Accuracy: 0.7025\n",
            "Epoch 37, Learning Rate: 1e-05, Training Loss: 3.46, Training Accuracy: 0.9956, Validation Loss: 1.70, Validation Accuracy: 0.6850\n",
            "Epoch 38, Learning Rate: 1e-05, Training Loss: 3.45, Training Accuracy: 0.9975, Validation Loss: 1.71, Validation Accuracy: 0.6900\n",
            "Epoch 39, Learning Rate: 1e-05, Training Loss: 3.44, Training Accuracy: 0.9975, Validation Loss: 1.71, Validation Accuracy: 0.7050\n",
            "Epoch 40, Learning Rate: 1e-05, Training Loss: 3.43, Training Accuracy: 0.9962, Validation Loss: 1.72, Validation Accuracy: 0.6850\n",
            "Epoch 41, Learning Rate: 1e-05, Training Loss: 3.43, Training Accuracy: 0.9975, Validation Loss: 1.72, Validation Accuracy: 0.7050\n",
            "Epoch 42, Learning Rate: 1e-05, Training Loss: 3.42, Training Accuracy: 0.9969, Validation Loss: 1.72, Validation Accuracy: 0.6925\n",
            "Epoch 00043: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Epoch 00043: reducing learning rate of group 1 to 1.0000e-06.\n",
            "Epoch 00043: reducing learning rate of group 2 to 1.0000e-06.\n",
            "Epoch 00043: reducing learning rate of group 3 to 1.0000e-06.\n",
            "Epoch 00043: reducing learning rate of group 4 to 1.0000e-06.\n",
            "Epoch 43, Learning Rate: 1.0000000000000002e-06, Training Loss: 3.43, Training Accuracy: 0.9988, Validation Loss: 1.72, Validation Accuracy: 0.6825\n",
            "Epoch 44, Learning Rate: 1.0000000000000002e-06, Training Loss: 3.42, Training Accuracy: 0.9994, Validation Loss: 1.71, Validation Accuracy: 0.7100\n",
            "Epoch 45, Learning Rate: 1.0000000000000002e-06, Training Loss: 3.41, Training Accuracy: 0.9988, Validation Loss: 1.71, Validation Accuracy: 0.6900\n",
            "Epoch 46, Learning Rate: 1.0000000000000002e-06, Training Loss: 3.39, Training Accuracy: 1.0000, Validation Loss: 1.72, Validation Accuracy: 0.7075\n",
            "Epoch 47, Learning Rate: 1.0000000000000002e-06, Training Loss: 3.40, Training Accuracy: 0.9981, Validation Loss: 1.71, Validation Accuracy: 0.7025\n",
            "Epoch 48, Learning Rate: 1.0000000000000002e-06, Training Loss: 3.39, Training Accuracy: 1.0000, Validation Loss: 1.71, Validation Accuracy: 0.7150\n",
            "Epoch 49, Learning Rate: 1.0000000000000002e-06, Training Loss: 3.38, Training Accuracy: 0.9988, Validation Loss: 1.71, Validation Accuracy: 0.6925\n",
            "Epoch 50, Learning Rate: 1.0000000000000002e-06, Training Loss: 3.39, Training Accuracy: 0.9988, Validation Loss: 1.71, Validation Accuracy: 0.7150\n",
            "\n",
            " Model has been saved to tensorboard_logs/2023-11-28_113618_ResNetRaw/best_model_ResNetRaw.pth\n",
            "\n",
            "Feature Extractor: ResNetRaw\n",
            "Model Accuracy: 0.7150\n",
            "Params (M): 10.4627\n",
            "Size of model (MB): 40.0504\n",
            "Latency of model (ms): 4.1492\n",
            "Classifier type: MLP\n",
            "Training type: Teacher Student Model\n",
            "\n",
            " Test Accuracy: 70.25%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 50\n",
        "patience = 20\n",
        "early_stopping_patience=30\n",
        "load_model_name = \"ResNetRaw\"\n",
        "classifier_type='MLP'\n",
        "\n",
        "#spec_resnet = models.resnet50(pretrained=True)\n",
        "# student_model= models.resnet18(pretrained=False)\n",
        "student_model = ResNetRaw(img_channel=3, num_classes=50)\n",
        "student_model.fc = nn.Sequential(nn.Linear(student_model.fc.in_features,500),\n",
        "                               nn.ReLU(),\n",
        "                               nn.Dropout(),\n",
        "                               nn.Linear(500,50))\n",
        "student_model.to(device)\n",
        "# print(model_size(student_model))\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.Adam([\n",
        "                        {'params': student_model.conv1.parameters(), 'lr': 1e-4},\n",
        "                        {'params': student_model.layer1.parameters(), 'lr': 1e-4},\n",
        "                        {'params': student_model.layer2.parameters(), 'lr': 1e-4},\n",
        "                        {'params': student_model.layer3.parameters(), 'lr': 1e-4},\n",
        "                        {'params': student_model.layer4.parameters(), 'lr': 1e-4},\n",
        "                        {'params': student_model.fc.parameters(), 'lr': 1e-8}\n",
        "                        ], lr=1e-2)\n",
        "\n",
        "nn_deep = model\n",
        "new_nn_light = student_model\n",
        "\n",
        "train_knowledge_distillation(teacher=nn_deep, student=new_nn_light, optimizer=optimizer, ce_loss=loss_fn, train_loader=esc50_train_loader,\n",
        "                             val_loader=esc50_val_loader, test_loader=esc50_test_loader, load_model_name=load_model_name,\n",
        "                             epochs=epochs, T=2, soft_target_loss_weight=0.25, ce_loss_weight=0.75, device=device,\n",
        "                             log_dir = 'tensorboard_logs', patience=patience, early_stopping_patience=early_stopping_patience,\n",
        "                             classifier_type=classifier_type)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khM8TF2bqzUd",
        "outputId": "717cd227-7f66-4cbf-fd33-5ebc598784e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Learning Rate: 0.0001, Training Loss: 6.69, Training Accuracy: 0.0663, Validation Loss: 3.51, Validation Accuracy: 0.1250\n",
            "Epoch 2, Learning Rate: 0.0001, Training Loss: 6.45, Training Accuracy: 0.1263, Validation Loss: 3.36, Validation Accuracy: 0.1900\n",
            "Epoch 3, Learning Rate: 0.0001, Training Loss: 6.27, Training Accuracy: 0.2037, Validation Loss: 3.18, Validation Accuracy: 0.2225\n",
            "Epoch 4, Learning Rate: 0.0001, Training Loss: 6.09, Training Accuracy: 0.2313, Validation Loss: 3.05, Validation Accuracy: 0.2425\n",
            "Epoch 5, Learning Rate: 0.0001, Training Loss: 5.99, Training Accuracy: 0.2706, Validation Loss: 2.93, Validation Accuracy: 0.2925\n",
            "Epoch 6, Learning Rate: 0.0001, Training Loss: 5.84, Training Accuracy: 0.3281, Validation Loss: 2.85, Validation Accuracy: 0.2750\n",
            "Epoch 7, Learning Rate: 0.0001, Training Loss: 5.72, Training Accuracy: 0.3731, Validation Loss: 2.76, Validation Accuracy: 0.3400\n",
            "Epoch 8, Learning Rate: 0.0001, Training Loss: 5.61, Training Accuracy: 0.4100, Validation Loss: 2.59, Validation Accuracy: 0.4050\n",
            "Epoch 9, Learning Rate: 0.0001, Training Loss: 5.47, Training Accuracy: 0.4656, Validation Loss: 2.52, Validation Accuracy: 0.4000\n",
            "Epoch 10, Learning Rate: 0.0001, Training Loss: 5.38, Training Accuracy: 0.4850, Validation Loss: 2.45, Validation Accuracy: 0.4375\n",
            "Epoch 11, Learning Rate: 0.0001, Training Loss: 5.23, Training Accuracy: 0.5406, Validation Loss: 2.38, Validation Accuracy: 0.4475\n",
            "Epoch 12, Learning Rate: 0.0001, Training Loss: 5.13, Training Accuracy: 0.5806, Validation Loss: 2.31, Validation Accuracy: 0.4750\n",
            "Epoch 13, Learning Rate: 0.0001, Training Loss: 5.00, Training Accuracy: 0.6419, Validation Loss: 2.34, Validation Accuracy: 0.4675\n",
            "Epoch 14, Learning Rate: 0.0001, Training Loss: 4.88, Training Accuracy: 0.6813, Validation Loss: 2.15, Validation Accuracy: 0.5275\n",
            "Epoch 15, Learning Rate: 0.0001, Training Loss: 4.75, Training Accuracy: 0.7262, Validation Loss: 2.21, Validation Accuracy: 0.4625\n",
            "Epoch 16, Learning Rate: 0.0001, Training Loss: 4.63, Training Accuracy: 0.7488, Validation Loss: 2.13, Validation Accuracy: 0.5375\n",
            "Epoch 17, Learning Rate: 0.0001, Training Loss: 4.47, Training Accuracy: 0.8000, Validation Loss: 2.15, Validation Accuracy: 0.4375\n",
            "Epoch 18, Learning Rate: 0.0001, Training Loss: 4.42, Training Accuracy: 0.8194, Validation Loss: 2.09, Validation Accuracy: 0.5375\n",
            "Epoch 19, Learning Rate: 0.0001, Training Loss: 4.28, Training Accuracy: 0.8562, Validation Loss: 1.98, Validation Accuracy: 0.5775\n",
            "Epoch 20, Learning Rate: 0.0001, Training Loss: 4.17, Training Accuracy: 0.8819, Validation Loss: 1.93, Validation Accuracy: 0.6075\n",
            "Epoch 21, Learning Rate: 0.0001, Training Loss: 4.03, Training Accuracy: 0.9256, Validation Loss: 1.90, Validation Accuracy: 0.5850\n",
            "Epoch 00022: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch 00022: reducing learning rate of group 1 to 1.0000e-05.\n",
            "Epoch 00022: reducing learning rate of group 2 to 1.0000e-05.\n",
            "Epoch 00022: reducing learning rate of group 3 to 1.0000e-05.\n",
            "Epoch 00022: reducing learning rate of group 4 to 1.0000e-05.\n",
            "Epoch 22, Learning Rate: 1e-05, Training Loss: 3.95, Training Accuracy: 0.9375, Validation Loss: 1.90, Validation Accuracy: 0.6025\n",
            "Epoch 23, Learning Rate: 1e-05, Training Loss: 3.75, Training Accuracy: 0.9775, Validation Loss: 1.80, Validation Accuracy: 0.6750\n",
            "Epoch 24, Learning Rate: 1e-05, Training Loss: 3.68, Training Accuracy: 0.9838, Validation Loss: 1.79, Validation Accuracy: 0.6500\n",
            "Epoch 25, Learning Rate: 1e-05, Training Loss: 3.65, Training Accuracy: 0.9862, Validation Loss: 1.77, Validation Accuracy: 0.6700\n",
            "Epoch 26, Learning Rate: 1e-05, Training Loss: 3.63, Training Accuracy: 0.9906, Validation Loss: 1.77, Validation Accuracy: 0.6575\n",
            "Epoch 27, Learning Rate: 1e-05, Training Loss: 3.63, Training Accuracy: 0.9875, Validation Loss: 1.78, Validation Accuracy: 0.6600\n",
            "Epoch 28, Learning Rate: 1e-05, Training Loss: 3.59, Training Accuracy: 0.9906, Validation Loss: 1.79, Validation Accuracy: 0.6650\n",
            "Epoch 29, Learning Rate: 1e-05, Training Loss: 3.59, Training Accuracy: 0.9912, Validation Loss: 1.77, Validation Accuracy: 0.6650\n",
            "Epoch 30, Learning Rate: 1e-05, Training Loss: 3.57, Training Accuracy: 0.9931, Validation Loss: 1.80, Validation Accuracy: 0.6550\n",
            "Epoch 31, Learning Rate: 1e-05, Training Loss: 3.56, Training Accuracy: 0.9931, Validation Loss: 1.80, Validation Accuracy: 0.6700\n",
            "Epoch 32, Learning Rate: 1e-05, Training Loss: 3.55, Training Accuracy: 0.9912, Validation Loss: 1.77, Validation Accuracy: 0.6825\n",
            "Epoch 33, Learning Rate: 1e-05, Training Loss: 3.54, Training Accuracy: 0.9938, Validation Loss: 1.78, Validation Accuracy: 0.6750\n",
            "Epoch 34, Learning Rate: 1e-05, Training Loss: 3.53, Training Accuracy: 0.9931, Validation Loss: 1.80, Validation Accuracy: 0.6600\n",
            "Epoch 35, Learning Rate: 1e-05, Training Loss: 3.52, Training Accuracy: 0.9956, Validation Loss: 1.80, Validation Accuracy: 0.6500\n",
            "Epoch 36, Learning Rate: 1e-05, Training Loss: 3.51, Training Accuracy: 0.9956, Validation Loss: 1.78, Validation Accuracy: 0.6750\n",
            "Epoch 37, Learning Rate: 1e-05, Training Loss: 3.50, Training Accuracy: 0.9950, Validation Loss: 1.79, Validation Accuracy: 0.6725\n",
            "Epoch 38, Learning Rate: 1e-05, Training Loss: 3.48, Training Accuracy: 0.9975, Validation Loss: 1.79, Validation Accuracy: 0.6800\n",
            "Epoch 39, Learning Rate: 1e-05, Training Loss: 3.47, Training Accuracy: 0.9975, Validation Loss: 1.77, Validation Accuracy: 0.6675\n",
            "Epoch 40, Learning Rate: 1e-05, Training Loss: 3.47, Training Accuracy: 0.9969, Validation Loss: 1.80, Validation Accuracy: 0.6600\n",
            "Epoch 41, Learning Rate: 1e-05, Training Loss: 3.45, Training Accuracy: 0.9975, Validation Loss: 1.78, Validation Accuracy: 0.6625\n",
            "Epoch 42, Learning Rate: 1e-05, Training Loss: 3.45, Training Accuracy: 0.9969, Validation Loss: 1.78, Validation Accuracy: 0.6550\n",
            "Epoch 00043: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Epoch 00043: reducing learning rate of group 1 to 1.0000e-06.\n",
            "Epoch 00043: reducing learning rate of group 2 to 1.0000e-06.\n",
            "Epoch 00043: reducing learning rate of group 3 to 1.0000e-06.\n",
            "Epoch 00043: reducing learning rate of group 4 to 1.0000e-06.\n",
            "Epoch 43, Learning Rate: 1.0000000000000002e-06, Training Loss: 3.45, Training Accuracy: 0.9962, Validation Loss: 1.78, Validation Accuracy: 0.6675\n",
            "Epoch 44, Learning Rate: 1.0000000000000002e-06, Training Loss: 3.42, Training Accuracy: 0.9969, Validation Loss: 1.78, Validation Accuracy: 0.6750\n",
            "Epoch 45, Learning Rate: 1.0000000000000002e-06, Training Loss: 3.43, Training Accuracy: 0.9975, Validation Loss: 1.78, Validation Accuracy: 0.6650\n",
            "Epoch 46, Learning Rate: 1.0000000000000002e-06, Training Loss: 3.42, Training Accuracy: 0.9988, Validation Loss: 1.78, Validation Accuracy: 0.6700\n",
            "Epoch 47, Learning Rate: 1.0000000000000002e-06, Training Loss: 3.42, Training Accuracy: 0.9981, Validation Loss: 1.77, Validation Accuracy: 0.6675\n",
            "Epoch 48, Learning Rate: 1.0000000000000002e-06, Training Loss: 3.42, Training Accuracy: 0.9981, Validation Loss: 1.78, Validation Accuracy: 0.6675\n",
            "Epoch 49, Learning Rate: 1.0000000000000002e-06, Training Loss: 3.43, Training Accuracy: 0.9981, Validation Loss: 1.78, Validation Accuracy: 0.6625\n",
            "Epoch 50, Learning Rate: 1.0000000000000002e-06, Training Loss: 3.42, Training Accuracy: 0.9975, Validation Loss: 1.78, Validation Accuracy: 0.6775\n",
            "\n",
            " Model has been saved to tensorboard_logs/2023-11-28_122544_ResNetRaw/best_model_ResNetRaw.pth\n",
            "\n",
            "Feature Extractor: ResNetRaw\n",
            "Model Accuracy: 0.6825\n",
            "Params (M): 10.4627\n",
            "Size of model (MB): 40.0504\n",
            "Latency of model (ms): 4.1488\n",
            "Classifier type: MLP\n",
            "Training type: Teacher Student Model\n",
            "\n",
            " Test Accuracy: 68.50%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 50\n",
        "patience = 20\n",
        "early_stopping_patience=30\n",
        "load_model_name = \"ResNetRaw1\"\n",
        "classifier_type='MLP'\n",
        "\n",
        "#spec_resnet = models.resnet50(pretrained=True)\n",
        "# student_model= models.resnet18(pretrained=False)\n",
        "student_model = ResNetRaw1(img_channel=3, num_classes=50)\n",
        "student_model.fc = nn.Sequential(nn.Linear(student_model.fc.in_features,500),\n",
        "                               nn.ReLU(),\n",
        "                               nn.Dropout(),\n",
        "                               nn.Linear(500,50))\n",
        "student_model.to(device)\n",
        "print(model_size(student_model), '\\n')\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.Adam([\n",
        "                        {'params': student_model.conv1.parameters(), 'lr': 1e-4},\n",
        "                        {'params': student_model.layer1.parameters(), 'lr': 1e-4},\n",
        "                        {'params': student_model.layer2.parameters(), 'lr': 1e-4},\n",
        "                        {'params': student_model.layer3.parameters(), 'lr': 1e-4},\n",
        "                        {'params': student_model.layer4.parameters(), 'lr': 1e-4},\n",
        "                        {'params': student_model.fc.parameters(), 'lr': 1e-8}\n",
        "                        ], lr=1e-2)\n",
        "\n",
        "nn_deep = model\n",
        "new_nn_light = student_model\n",
        "\n",
        "train_knowledge_distillation(teacher=nn_deep, student=new_nn_light, optimizer=optimizer, ce_loss=loss_fn, train_loader=esc50_train_loader,\n",
        "                             val_loader=esc50_val_loader, test_loader=esc50_test_loader, load_model_name=load_model_name,\n",
        "                             epochs=epochs, T=2, soft_target_loss_weight=0.25, ce_loss_weight=0.75, device=device,\n",
        "                             log_dir = 'tensorboard_logs', patience=patience, early_stopping_patience=early_stopping_patience,\n",
        "                             classifier_type=classifier_type)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rn2QQrCoD1A5",
        "outputId": "39685ca5-2e4f-4250-eec4-5c1f37047e8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.65594482421875 \n",
            "\n",
            "Epoch 1, Learning Rate: 0.0001, Training Loss: 6.68, Training Accuracy: 0.0600, Validation Loss: 3.53, Validation Accuracy: 0.1375\n",
            "Epoch 2, Learning Rate: 0.0001, Training Loss: 6.45, Training Accuracy: 0.1212, Validation Loss: 3.38, Validation Accuracy: 0.1975\n",
            "Epoch 3, Learning Rate: 0.0001, Training Loss: 6.30, Training Accuracy: 0.1794, Validation Loss: 3.24, Validation Accuracy: 0.2375\n",
            "Epoch 4, Learning Rate: 0.0001, Training Loss: 6.18, Training Accuracy: 0.2181, Validation Loss: 3.13, Validation Accuracy: 0.2450\n",
            "Epoch 5, Learning Rate: 0.0001, Training Loss: 6.07, Training Accuracy: 0.2500, Validation Loss: 3.03, Validation Accuracy: 0.2825\n",
            "Epoch 6, Learning Rate: 0.0001, Training Loss: 5.95, Training Accuracy: 0.3231, Validation Loss: 2.94, Validation Accuracy: 0.3125\n",
            "Epoch 7, Learning Rate: 0.0001, Training Loss: 5.84, Training Accuracy: 0.3563, Validation Loss: 2.90, Validation Accuracy: 0.3300\n",
            "Epoch 8, Learning Rate: 0.0001, Training Loss: 5.76, Training Accuracy: 0.3956, Validation Loss: 2.81, Validation Accuracy: 0.3300\n",
            "Epoch 9, Learning Rate: 0.0001, Training Loss: 5.65, Training Accuracy: 0.4381, Validation Loss: 2.71, Validation Accuracy: 0.3925\n",
            "Epoch 10, Learning Rate: 0.0001, Training Loss: 5.57, Training Accuracy: 0.4681, Validation Loss: 2.74, Validation Accuracy: 0.3525\n",
            "Epoch 11, Learning Rate: 0.0001, Training Loss: 5.47, Training Accuracy: 0.5150, Validation Loss: 2.62, Validation Accuracy: 0.4375\n",
            "Epoch 12, Learning Rate: 0.0001, Training Loss: 5.39, Training Accuracy: 0.5700, Validation Loss: 2.52, Validation Accuracy: 0.4175\n",
            "Epoch 13, Learning Rate: 0.0001, Training Loss: 5.28, Training Accuracy: 0.5863, Validation Loss: 2.48, Validation Accuracy: 0.4825\n",
            "Epoch 14, Learning Rate: 0.0001, Training Loss: 5.21, Training Accuracy: 0.6300, Validation Loss: 2.45, Validation Accuracy: 0.4925\n",
            "Epoch 15, Learning Rate: 0.0001, Training Loss: 5.12, Training Accuracy: 0.6500, Validation Loss: 2.42, Validation Accuracy: 0.4900\n",
            "Epoch 16, Learning Rate: 0.0001, Training Loss: 4.99, Training Accuracy: 0.7225, Validation Loss: 2.36, Validation Accuracy: 0.5375\n",
            "Epoch 17, Learning Rate: 0.0001, Training Loss: 4.92, Training Accuracy: 0.7319, Validation Loss: 2.33, Validation Accuracy: 0.5250\n",
            "Epoch 18, Learning Rate: 0.0001, Training Loss: 4.83, Training Accuracy: 0.7638, Validation Loss: 2.32, Validation Accuracy: 0.5025\n",
            "Epoch 19, Learning Rate: 0.0001, Training Loss: 4.74, Training Accuracy: 0.7894, Validation Loss: 2.30, Validation Accuracy: 0.5125\n",
            "Epoch 20, Learning Rate: 0.0001, Training Loss: 4.66, Training Accuracy: 0.8169, Validation Loss: 2.23, Validation Accuracy: 0.5725\n",
            "Epoch 21, Learning Rate: 0.0001, Training Loss: 4.56, Training Accuracy: 0.8556, Validation Loss: 2.14, Validation Accuracy: 0.6300\n",
            "Epoch 00022: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch 00022: reducing learning rate of group 1 to 1.0000e-05.\n",
            "Epoch 00022: reducing learning rate of group 2 to 1.0000e-05.\n",
            "Epoch 00022: reducing learning rate of group 3 to 1.0000e-05.\n",
            "Epoch 00022: reducing learning rate of group 4 to 1.0000e-05.\n",
            "Epoch 22, Learning Rate: 1e-05, Training Loss: 4.47, Training Accuracy: 0.8800, Validation Loss: 2.15, Validation Accuracy: 0.5925\n",
            "Epoch 23, Learning Rate: 1e-05, Training Loss: 4.28, Training Accuracy: 0.9263, Validation Loss: 2.06, Validation Accuracy: 0.6325\n",
            "Epoch 24, Learning Rate: 1e-05, Training Loss: 4.23, Training Accuracy: 0.9406, Validation Loss: 2.06, Validation Accuracy: 0.6300\n",
            "Epoch 25, Learning Rate: 1e-05, Training Loss: 4.20, Training Accuracy: 0.9450, Validation Loss: 2.07, Validation Accuracy: 0.6300\n",
            "Epoch 26, Learning Rate: 1e-05, Training Loss: 4.19, Training Accuracy: 0.9463, Validation Loss: 2.06, Validation Accuracy: 0.6325\n",
            "Epoch 27, Learning Rate: 1e-05, Training Loss: 4.19, Training Accuracy: 0.9537, Validation Loss: 2.05, Validation Accuracy: 0.6450\n",
            "Epoch 28, Learning Rate: 1e-05, Training Loss: 4.15, Training Accuracy: 0.9581, Validation Loss: 2.05, Validation Accuracy: 0.6375\n",
            "Epoch 29, Learning Rate: 1e-05, Training Loss: 4.14, Training Accuracy: 0.9600, Validation Loss: 2.06, Validation Accuracy: 0.6450\n",
            "Epoch 30, Learning Rate: 1e-05, Training Loss: 4.13, Training Accuracy: 0.9587, Validation Loss: 2.05, Validation Accuracy: 0.6400\n",
            "Epoch 31, Learning Rate: 1e-05, Training Loss: 4.11, Training Accuracy: 0.9669, Validation Loss: 2.04, Validation Accuracy: 0.6325\n",
            "Epoch 32, Learning Rate: 1e-05, Training Loss: 4.11, Training Accuracy: 0.9631, Validation Loss: 2.05, Validation Accuracy: 0.6475\n",
            "Epoch 33, Learning Rate: 1e-05, Training Loss: 4.10, Training Accuracy: 0.9644, Validation Loss: 2.04, Validation Accuracy: 0.6525\n",
            "Epoch 34, Learning Rate: 1e-05, Training Loss: 4.07, Training Accuracy: 0.9725, Validation Loss: 2.05, Validation Accuracy: 0.6425\n",
            "Epoch 35, Learning Rate: 1e-05, Training Loss: 4.05, Training Accuracy: 0.9712, Validation Loss: 2.05, Validation Accuracy: 0.6400\n",
            "Epoch 36, Learning Rate: 1e-05, Training Loss: 4.04, Training Accuracy: 0.9750, Validation Loss: 2.03, Validation Accuracy: 0.6525\n",
            "Epoch 37, Learning Rate: 1e-05, Training Loss: 4.04, Training Accuracy: 0.9731, Validation Loss: 2.02, Validation Accuracy: 0.6650\n",
            "Epoch 38, Learning Rate: 1e-05, Training Loss: 4.03, Training Accuracy: 0.9706, Validation Loss: 2.04, Validation Accuracy: 0.6525\n",
            "Epoch 39, Learning Rate: 1e-05, Training Loss: 4.01, Training Accuracy: 0.9731, Validation Loss: 2.02, Validation Accuracy: 0.6600\n",
            "Epoch 40, Learning Rate: 1e-05, Training Loss: 4.00, Training Accuracy: 0.9725, Validation Loss: 2.03, Validation Accuracy: 0.6500\n",
            "Epoch 41, Learning Rate: 1e-05, Training Loss: 3.99, Training Accuracy: 0.9819, Validation Loss: 2.01, Validation Accuracy: 0.6425\n",
            "Epoch 42, Learning Rate: 1e-05, Training Loss: 3.97, Training Accuracy: 0.9825, Validation Loss: 2.03, Validation Accuracy: 0.6575\n",
            "Epoch 00043: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Epoch 00043: reducing learning rate of group 1 to 1.0000e-06.\n",
            "Epoch 00043: reducing learning rate of group 2 to 1.0000e-06.\n",
            "Epoch 00043: reducing learning rate of group 3 to 1.0000e-06.\n",
            "Epoch 00043: reducing learning rate of group 4 to 1.0000e-06.\n",
            "Epoch 43, Learning Rate: 1.0000000000000002e-06, Training Loss: 3.97, Training Accuracy: 0.9788, Validation Loss: 2.04, Validation Accuracy: 0.6500\n",
            "Epoch 44, Learning Rate: 1.0000000000000002e-06, Training Loss: 3.96, Training Accuracy: 0.9800, Validation Loss: 2.01, Validation Accuracy: 0.6450\n",
            "Epoch 45, Learning Rate: 1.0000000000000002e-06, Training Loss: 3.93, Training Accuracy: 0.9831, Validation Loss: 2.04, Validation Accuracy: 0.6450\n",
            "Epoch 46, Learning Rate: 1.0000000000000002e-06, Training Loss: 3.92, Training Accuracy: 0.9806, Validation Loss: 2.01, Validation Accuracy: 0.6525\n",
            "Epoch 47, Learning Rate: 1.0000000000000002e-06, Training Loss: 3.93, Training Accuracy: 0.9806, Validation Loss: 2.03, Validation Accuracy: 0.6450\n",
            "Epoch 48, Learning Rate: 1.0000000000000002e-06, Training Loss: 3.94, Training Accuracy: 0.9831, Validation Loss: 2.01, Validation Accuracy: 0.6375\n",
            "Epoch 49, Learning Rate: 1.0000000000000002e-06, Training Loss: 3.92, Training Accuracy: 0.9844, Validation Loss: 2.02, Validation Accuracy: 0.6500\n",
            "Epoch 50, Learning Rate: 1.0000000000000002e-06, Training Loss: 3.92, Training Accuracy: 0.9819, Validation Loss: 2.03, Validation Accuracy: 0.6475\n",
            "\n",
            " Model has been saved to tensorboard_logs/2023-11-29_075255_ResNetRaw1/best_model_ResNetRaw1.pth\n",
            "\n",
            "Feature Extractor: ResNetRaw1\n",
            "Model Accuracy: 0.6650\n",
            "Params (M): 9.0655\n",
            "Size of model (MB): 34.6911\n",
            "Latency of model (ms): 3.8614\n",
            "Classifier type: MLP\n",
            "Training type: Teacher Student Model\n",
            "\n",
            " Test Accuracy: 65.25%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Resnet raw without knowledge distillation"
      ],
      "metadata": {
        "id": "cZ_OiNvr_KSh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epoches = 50\n",
        "patience = 20\n",
        "early_stopping_patience=30\n",
        "load_model_name = \"ResNetRaw1\"\n",
        "classifier_type='MLP'\n",
        "\n",
        "model_raw = ResNetRaw1(img_channel=3, num_classes=50)\n",
        "\n",
        "# Replace the last fully connected layer\n",
        "num_features = model_raw.fc.in_features\n",
        "model_raw.fc = nn.Sequential(\n",
        "    nn.Linear(num_features, 500),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(),\n",
        "    nn.Linear(500, 50)\n",
        ")\n",
        "\n",
        "model_raw.to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.Adam([\n",
        "                        {'params': model_raw.conv1.parameters(), 'lr': 1e-4},\n",
        "                        {'params': model_raw.layer1.parameters(), 'lr': 1e-4},\n",
        "                        {'params': model_raw.layer2.parameters(), 'lr': 1e-4},\n",
        "                        {'params': model_raw.layer3.parameters(), 'lr': 1e-4},\n",
        "                        {'params': model_raw.layer4.parameters(), 'lr': 1e-4},\n",
        "                        {'params': model_raw.fc.parameters(), 'lr': 1e-8}\n",
        "                        ], lr=1e-2)\n",
        "\n",
        "\n",
        "train(model_raw, optimizer, loss_fn, esc50_train_loader, esc50_val_loader, esc50_test_loader, load_model_name, epochs=epoches, device=device,\n",
        "      log_dir = 'tensorboard_logs', patience=patience, early_stopping_patience=early_stopping_patience, classifier_type=classifier_type)\n"
      ],
      "metadata": {
        "id": "wXp4VIqNEfUL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14b77257-0339-4fa2-e9b4-2c4e6d99b5a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Learning Rate: 0.0001, Training Loss: 3.72, Training Accuracy: 0.0663, Validation Loss: 3.49, Validation Accuracy: 0.1200\n",
            "Epoch: 2, Learning Rate: 0.0001, Training Loss: 3.47, Training Accuracy: 0.1219, Validation Loss: 3.34, Validation Accuracy: 0.1950\n",
            "Epoch: 3, Learning Rate: 0.0001, Training Loss: 3.32, Training Accuracy: 0.1769, Validation Loss: 3.20, Validation Accuracy: 0.2250\n",
            "Epoch: 4, Learning Rate: 0.0001, Training Loss: 3.14, Training Accuracy: 0.2225, Validation Loss: 3.07, Validation Accuracy: 0.2525\n",
            "Epoch: 5, Learning Rate: 0.0001, Training Loss: 3.02, Training Accuracy: 0.2762, Validation Loss: 3.03, Validation Accuracy: 0.2650\n",
            "Epoch: 6, Learning Rate: 0.0001, Training Loss: 2.91, Training Accuracy: 0.3100, Validation Loss: 2.88, Validation Accuracy: 0.3400\n",
            "Epoch: 7, Learning Rate: 0.0001, Training Loss: 2.80, Training Accuracy: 0.3581, Validation Loss: 2.83, Validation Accuracy: 0.3625\n",
            "Epoch: 8, Learning Rate: 0.0001, Training Loss: 2.66, Training Accuracy: 0.4200, Validation Loss: 2.69, Validation Accuracy: 0.3725\n",
            "Epoch: 9, Learning Rate: 0.0001, Training Loss: 2.56, Training Accuracy: 0.4569, Validation Loss: 2.68, Validation Accuracy: 0.3625\n",
            "Epoch: 10, Learning Rate: 0.0001, Training Loss: 2.46, Training Accuracy: 0.5062, Validation Loss: 2.58, Validation Accuracy: 0.4425\n",
            "Epoch: 11, Learning Rate: 0.0001, Training Loss: 2.37, Training Accuracy: 0.5281, Validation Loss: 2.56, Validation Accuracy: 0.4350\n",
            "Epoch: 12, Learning Rate: 0.0001, Training Loss: 2.30, Training Accuracy: 0.5494, Validation Loss: 2.48, Validation Accuracy: 0.4375\n",
            "Epoch: 13, Learning Rate: 0.0001, Training Loss: 2.18, Training Accuracy: 0.6162, Validation Loss: 2.42, Validation Accuracy: 0.4825\n",
            "Epoch: 14, Learning Rate: 0.0001, Training Loss: 2.09, Training Accuracy: 0.6381, Validation Loss: 2.42, Validation Accuracy: 0.4775\n",
            "Epoch: 15, Learning Rate: 0.0001, Training Loss: 1.96, Training Accuracy: 0.6781, Validation Loss: 2.35, Validation Accuracy: 0.4825\n",
            "Epoch: 16, Learning Rate: 0.0001, Training Loss: 1.91, Training Accuracy: 0.7025, Validation Loss: 2.29, Validation Accuracy: 0.4925\n",
            "Epoch: 17, Learning Rate: 0.0001, Training Loss: 1.81, Training Accuracy: 0.7406, Validation Loss: 2.30, Validation Accuracy: 0.4975\n",
            "Epoch: 18, Learning Rate: 0.0001, Training Loss: 1.72, Training Accuracy: 0.7863, Validation Loss: 2.18, Validation Accuracy: 0.6000\n",
            "Epoch: 19, Learning Rate: 0.0001, Training Loss: 1.58, Training Accuracy: 0.8169, Validation Loss: 2.23, Validation Accuracy: 0.5600\n",
            "Epoch: 20, Learning Rate: 0.0001, Training Loss: 1.51, Training Accuracy: 0.8313, Validation Loss: 2.25, Validation Accuracy: 0.5325\n",
            "Epoch: 21, Learning Rate: 0.0001, Training Loss: 1.40, Training Accuracy: 0.8581, Validation Loss: 2.13, Validation Accuracy: 0.6050\n",
            "Epoch 00022: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch 00022: reducing learning rate of group 1 to 1.0000e-05.\n",
            "Epoch 00022: reducing learning rate of group 2 to 1.0000e-05.\n",
            "Epoch 00022: reducing learning rate of group 3 to 1.0000e-05.\n",
            "Epoch 00022: reducing learning rate of group 4 to 1.0000e-05.\n",
            "Epoch: 22, Learning Rate: 1e-05, Training Loss: 1.32, Training Accuracy: 0.8931, Validation Loss: 2.10, Validation Accuracy: 0.5625\n",
            "Epoch: 23, Learning Rate: 1e-05, Training Loss: 1.14, Training Accuracy: 0.9344, Validation Loss: 2.03, Validation Accuracy: 0.6425\n",
            "Epoch: 24, Learning Rate: 1e-05, Training Loss: 1.09, Training Accuracy: 0.9463, Validation Loss: 2.00, Validation Accuracy: 0.6250\n",
            "Epoch: 25, Learning Rate: 1e-05, Training Loss: 1.05, Training Accuracy: 0.9444, Validation Loss: 2.00, Validation Accuracy: 0.6550\n",
            "Epoch: 26, Learning Rate: 1e-05, Training Loss: 1.03, Training Accuracy: 0.9544, Validation Loss: 2.01, Validation Accuracy: 0.6550\n",
            "Epoch: 27, Learning Rate: 1e-05, Training Loss: 1.03, Training Accuracy: 0.9544, Validation Loss: 2.01, Validation Accuracy: 0.6475\n",
            "Epoch: 28, Learning Rate: 1e-05, Training Loss: 0.98, Training Accuracy: 0.9625, Validation Loss: 1.99, Validation Accuracy: 0.6375\n",
            "Epoch: 29, Learning Rate: 1e-05, Training Loss: 0.99, Training Accuracy: 0.9644, Validation Loss: 1.99, Validation Accuracy: 0.6375\n",
            "Epoch: 30, Learning Rate: 1e-05, Training Loss: 0.97, Training Accuracy: 0.9563, Validation Loss: 2.02, Validation Accuracy: 0.6525\n",
            "Epoch: 31, Learning Rate: 1e-05, Training Loss: 0.94, Training Accuracy: 0.9644, Validation Loss: 1.99, Validation Accuracy: 0.6650\n",
            "Epoch: 32, Learning Rate: 1e-05, Training Loss: 0.92, Training Accuracy: 0.9719, Validation Loss: 2.01, Validation Accuracy: 0.6525\n",
            "Epoch: 33, Learning Rate: 1e-05, Training Loss: 0.93, Training Accuracy: 0.9694, Validation Loss: 1.98, Validation Accuracy: 0.6525\n",
            "Epoch: 34, Learning Rate: 1e-05, Training Loss: 0.92, Training Accuracy: 0.9712, Validation Loss: 2.02, Validation Accuracy: 0.6600\n",
            "Epoch: 35, Learning Rate: 1e-05, Training Loss: 0.91, Training Accuracy: 0.9688, Validation Loss: 1.99, Validation Accuracy: 0.6600\n",
            "Epoch: 36, Learning Rate: 1e-05, Training Loss: 0.89, Training Accuracy: 0.9775, Validation Loss: 2.01, Validation Accuracy: 0.6575\n",
            "Epoch: 37, Learning Rate: 1e-05, Training Loss: 0.88, Training Accuracy: 0.9769, Validation Loss: 2.01, Validation Accuracy: 0.6500\n",
            "Epoch: 38, Learning Rate: 1e-05, Training Loss: 0.86, Training Accuracy: 0.9781, Validation Loss: 2.01, Validation Accuracy: 0.6700\n",
            "Epoch: 39, Learning Rate: 1e-05, Training Loss: 0.86, Training Accuracy: 0.9762, Validation Loss: 2.00, Validation Accuracy: 0.6600\n",
            "Epoch: 40, Learning Rate: 1e-05, Training Loss: 0.83, Training Accuracy: 0.9794, Validation Loss: 2.00, Validation Accuracy: 0.6525\n",
            "Epoch: 41, Learning Rate: 1e-05, Training Loss: 0.83, Training Accuracy: 0.9769, Validation Loss: 2.01, Validation Accuracy: 0.6400\n",
            "Epoch: 42, Learning Rate: 1e-05, Training Loss: 0.82, Training Accuracy: 0.9825, Validation Loss: 2.00, Validation Accuracy: 0.6725\n",
            "Epoch 00043: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Epoch 00043: reducing learning rate of group 1 to 1.0000e-06.\n",
            "Epoch 00043: reducing learning rate of group 2 to 1.0000e-06.\n",
            "Epoch 00043: reducing learning rate of group 3 to 1.0000e-06.\n",
            "Epoch 00043: reducing learning rate of group 4 to 1.0000e-06.\n",
            "Epoch: 43, Learning Rate: 1.0000000000000002e-06, Training Loss: 0.79, Training Accuracy: 0.9812, Validation Loss: 2.00, Validation Accuracy: 0.6850\n",
            "Epoch: 44, Learning Rate: 1.0000000000000002e-06, Training Loss: 0.79, Training Accuracy: 0.9831, Validation Loss: 1.99, Validation Accuracy: 0.6575\n",
            "Epoch: 45, Learning Rate: 1.0000000000000002e-06, Training Loss: 0.79, Training Accuracy: 0.9862, Validation Loss: 1.99, Validation Accuracy: 0.6525\n",
            "Epoch: 46, Learning Rate: 1.0000000000000002e-06, Training Loss: 0.77, Training Accuracy: 0.9875, Validation Loss: 2.01, Validation Accuracy: 0.6675\n",
            "Epoch: 47, Learning Rate: 1.0000000000000002e-06, Training Loss: 0.78, Training Accuracy: 0.9838, Validation Loss: 1.99, Validation Accuracy: 0.6700\n",
            "Epoch: 48, Learning Rate: 1.0000000000000002e-06, Training Loss: 0.78, Training Accuracy: 0.9856, Validation Loss: 1.98, Validation Accuracy: 0.6600\n",
            "Epoch: 49, Learning Rate: 1.0000000000000002e-06, Training Loss: 0.78, Training Accuracy: 0.9862, Validation Loss: 1.99, Validation Accuracy: 0.6800\n",
            "Epoch: 50, Learning Rate: 1.0000000000000002e-06, Training Loss: 0.77, Training Accuracy: 0.9838, Validation Loss: 1.99, Validation Accuracy: 0.6625\n",
            "\n",
            " Model has been saved to tensorboard_logs/2023-11-29_121201_ResNetRaw1/best_model_ResNetRaw1.pth \n",
            "\n",
            "Feature Extractor: ResNetRaw1\n",
            "Model Accuracy: 0.6850\n",
            "Params (M): 9.0655\n",
            "Size of model (MB): 34.6911\n",
            "Latency of model (ms): 6.6134\n",
            "Classifier type: MLP\n",
            "Training type: Normal\n",
            "\n",
            " Test Accuracy: 67.25%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "lKNopN2H43uT"
      ],
      "provenance": [],
      "gpuType": "V100",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}