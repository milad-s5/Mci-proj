{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fXpFjiht0bg"
      },
      "source": [
        "# Login to Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRXsDffet4nW",
        "outputId": "9e28110a-131a-495f-d17c-dbe892c2faf6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bikP354nuWkx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f224978a-59ce-4742-87bb-c2aa8759f8ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ],
      "source": [
        "%cd drive/MyDrive"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quantize model"
      ],
      "metadata": {
        "id": "av431rc3czAt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## codes"
      ],
      "metadata": {
        "id": "PmS163W24_mt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython.display as display\n",
        "\n",
        "import glob\n",
        "from collections import Counter\n",
        "\n",
        "import math\n",
        "import pandas as pd\n",
        "\n",
        "import librosa\n",
        "import librosa.display\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import torch\n",
        "# import torchaudio\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import soundfile as sf\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import models, transforms\n",
        "\n",
        "import tensorflow as tf\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import datetime\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "\n",
        "def print_size_of_model(model, label=\"\"):\n",
        "    torch.save(model.state_dict(), \"temp.p\")\n",
        "    size=os.path.getsize(\"temp.p\")\n",
        "    print(\"model: \",label,' \\t','Size (MB):', size/1e6)\n",
        "    os.remove('temp.p')\n",
        "    return size\n",
        "\n",
        "def model_size(model):\n",
        "    param_size = 0\n",
        "    for param in model.parameters():\n",
        "        param_size += param.nelement() * param.element_size()\n",
        "    buffer_size = 0\n",
        "    for buffer in model.buffers():\n",
        "        buffer_size += buffer.nelement() * buffer.element_size()\n",
        "\n",
        "    size_all_mb = (param_size + buffer_size) / 1024**2\n",
        "    # print('model size: {:.3f}MB'.format(size_all_mb))\n",
        "    return size_all_mb\n",
        "\n",
        "def evaluate(model, test_loader, device=\"cpu\"):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    num_correct = 0\n",
        "    num_examples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            inputs, targets = batch\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "            output = model(inputs)\n",
        "            correct = torch.eq(torch.max(F.softmax(output, dim=1), dim=1)[1], targets).view(-1)\n",
        "            num_correct += torch.sum(correct).item()\n",
        "            num_examples += correct.shape[0]\n",
        "\n",
        "    accuracy = num_correct / num_examples\n",
        "    return accuracy\n",
        "\n",
        "class FrequencyMask(object):\n",
        "    \"\"\"\n",
        "      Example:\n",
        "        >>> transforms.Compose([\n",
        "        >>>     transforms.ToTensor(),\n",
        "        >>>     FrequencyMask(max_width=10, use_mean=False),\n",
        "        >>> ])\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, max_width, use_mean=True):\n",
        "        self.max_width = max_width\n",
        "        self.use_mean = use_mean\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            tensor (Tensor): Tensor image of\n",
        "            size (C, H, W) where the frequency\n",
        "            mask is to be applied.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Transformed image with Frequency Mask.\n",
        "        \"\"\"\n",
        "        start = random.randrange(0, tensor.shape[2])\n",
        "        end = start + random.randrange(1, self.max_width)\n",
        "        if self.use_mean:\n",
        "            tensor[:, start:end, :] = tensor.mean()\n",
        "        else:\n",
        "            tensor[:, start:end, :] = 0\n",
        "        return tensor\n",
        "\n",
        "    def __repr__(self):\n",
        "        format_string = self.__class__.__name__ + \"(max_width=\"\n",
        "        format_string += str(self.max_width) + \")\"\n",
        "        format_string += 'use_mean=' + (str(self.use_mean) + ')')\n",
        "\n",
        "        return format_string\n",
        "\n",
        "\n",
        "class TimeMask(object):\n",
        "    \"\"\"\n",
        "      Example:\n",
        "        >>> transforms.Compose([\n",
        "        >>>     transforms.ToTensor(),\n",
        "        >>>     TimeMask(max_width=10, use_mean=False),\n",
        "        >>> ])\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, max_width, use_mean=True):\n",
        "        self.max_width = max_width\n",
        "        self.use_mean = use_mean\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            tensor (Tensor): Tensor image of\n",
        "            size (C, H, W) where the time mask\n",
        "            is to be applied.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Transformed image with Time Mask.\n",
        "        \"\"\"\n",
        "        start = random.randrange(0, tensor.shape[1])\n",
        "        end = start + random.randrange(0, self.max_width)\n",
        "        if self.use_mean:\n",
        "            tensor[:, :, start:end] = tensor.mean()\n",
        "        else:\n",
        "            tensor[:, :, start:end] = 0\n",
        "        return tensor\n",
        "\n",
        "    def __repr__(self):\n",
        "        format_string = self.__class__.__name__ + \"(max_width=\"\n",
        "        format_string += str(self.max_width) + \")\"\n",
        "        format_string += 'use_mean=' + (str(self.use_mean) + ')')\n",
        "        return format_string\n",
        "\n",
        "\n",
        "class PrecomputedESC50(Dataset):\n",
        "    def __init__(self,path, max_freqmask_width, max_timemask_width, use_mean=True, dpi=50):\n",
        "        files = Path(path).glob('*.png')\n",
        "        self.items = [(f,int(f.name.split(\"-\")[-1].replace(\".wav.png\",\"\"))) for f in files]\n",
        "        self.length = len(self.items)\n",
        "        self.max_freqmask_width = max_freqmask_width\n",
        "        self.max_timemask_width = max_timemask_width\n",
        "        self.use_mean = use_mean\n",
        "        self.img_transforms = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225]),\n",
        "            transforms.RandomApply([FrequencyMask(self.max_freqmask_width, self.use_mean)], p=0.5),\n",
        "            transforms.RandomApply([TimeMask(self.max_timemask_width, self.use_mean)], p=0.5)])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        filename, label = self.items[index]\n",
        "        img = Image.open(filename).convert('RGB')\n",
        "        return (self.img_transforms(img), label)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "# Define a function to plot and log confusion matrix to TensorBoard\n",
        "def plot_confusion_matrix(model, test_loader, device=\"cpu\"):\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            inputs, targets = batch\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "            output = model(inputs)\n",
        "            predictions = torch.max(F.softmax(output, dim=1), dim=1)[1].cpu().numpy()\n",
        "            all_predictions.extend(predictions)\n",
        "            all_labels.extend(targets.cpu().numpy())\n",
        "\n",
        "    # Generate confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "\n",
        "    # Create a heatmap of the confusion matrix\n",
        "    plt.figure(figsize=(20, 16))\n",
        "    sn.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=True, yticklabels=True)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Confusion Matrix')\n",
        "\n",
        "    figure = plt.gcf()\n",
        "    return figure\n",
        "\n",
        "# Define a function to log predictions vs. actuals as images to TensorBoard\n",
        "def log_predictions_vs_actuals(model, data_loader, device=\"cpu\", num_batches=5):\n",
        "    model.eval()\n",
        "\n",
        "    batch_counter = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            if batch_counter >= num_batches:\n",
        "                break\n",
        "\n",
        "            inputs, targets = batch\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "            output = model(inputs)\n",
        "            predictions = torch.max(F.softmax(output, dim=1), dim=1)\n",
        "            predicted_labels = predictions[1]\n",
        "            probabilities = predictions[0]\n",
        "\n",
        "            # Convert PyTorch tensors to NumPy arrays\n",
        "            inputs_np = inputs.permute(0, 2, 3, 1).cpu().numpy()\n",
        "\n",
        "            # Create a figure for each batch\n",
        "            fig, axes = plt.subplots(nrows=4, ncols=4, figsize=(12, 12))\n",
        "\n",
        "            for i, ax in enumerate(axes.flat):\n",
        "                ax.imshow(inputs_np[i])\n",
        "                ax.axis(\"off\")\n",
        "\n",
        "                actual_label = targets[i].item()\n",
        "                predicted_label = predicted_labels[i].item()\n",
        "                probability = probabilities[i].item()\n",
        "\n",
        "                # Color the title based on correctness\n",
        "                title_color = 'green' if actual_label == predicted_label else 'red'\n",
        "\n",
        "                ax.set_title(f\"Actual: {actual_label}\\nPredicted: {predicted_label}\\nProb: {probability:.2f}\", color=title_color)\n",
        "\n",
        "            plt.tight_layout()\n",
        "            batch_counter += 1\n",
        "    return fig\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience, verbose=False):\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_valid_accuracy = 0.0\n",
        "        self.early_stop = False\n",
        "\n",
        "    def step(self, valid_accuracy):\n",
        "        if valid_accuracy > self.best_valid_accuracy:\n",
        "            self.best_valid_accuracy = valid_accuracy\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter > self.patience:\n",
        "                self.early_stop = True\n",
        "                if self.verbose:\n",
        "                    print(\"Early stopping activated.\")\n",
        "        return self.early_stop\n",
        "\n",
        "class LearningRateScheduler(lr_scheduler._LRScheduler):\n",
        "    def __init__(self, optimizer, patience, factor=0.1, verbose=False):\n",
        "        self.optimizer = optimizer\n",
        "        self.patience = patience\n",
        "        self.factor = factor\n",
        "        self.verbose = verbose\n",
        "        self.lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, patience=self.patience, factor=self.factor, verbose=self.verbose)\n",
        "\n",
        "    def step(self, valid_accuracy):\n",
        "        self.lr_scheduler.step(valid_accuracy)\n",
        "        return self.optimizer.param_groups[0]['lr']\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "PATH_ESC50_TRAIN=\"./train1/\"\n",
        "PATH_ESC50_VALID=\"./valid1/\"\n",
        "PATH_ESC50_TEST=\"./test/\"\n",
        "\n",
        "bs=16\n",
        "esc50pre_train = PrecomputedESC50(PATH_ESC50_TRAIN, max_freqmask_width=10, max_timemask_width=10 )\n",
        "esc50pre_valid = PrecomputedESC50(PATH_ESC50_VALID,max_freqmask_width=10, max_timemask_width=10 )\n",
        "esc50pre_test = PrecomputedESC50(PATH_ESC50_TEST,max_freqmask_width=10, max_timemask_width=10 )\n",
        "\n",
        "esc50_train_loader = torch.utils.data.DataLoader(esc50pre_train, bs, shuffle=True)\n",
        "esc50_val_loader = torch.utils.data.DataLoader(esc50pre_valid, bs, shuffle=True)\n",
        "esc50_test_loader = torch.utils.data.DataLoader(esc50pre_test, bs, shuffle=True)"
      ],
      "metadata": {
        "id": "nHp1UvHe4_mt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9200441-4b39-444a-d4da-f62f90e03cee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/usr/local/lib/python3.10/dist-packages/torchvision/image.so: undefined symbol: _ZN3c104cuda9SetDeviceEi'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
            "  warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate model latency (replace with your actual latency calculation)\n",
        "def estimate_latency(model, device=\"cpu\"):\n",
        "    input_tensor = torch.randn(1, 3, 224, 224).to(device)\n",
        "    # This is a simplified estimation of model latency and may not be accurate for all models.\n",
        "    model = model.to(device)\n",
        "    input_tensor = input_tensor.to(device)\n",
        "\n",
        "    # Warm-up to reduce variability\n",
        "    for _ in range(10):\n",
        "        _ = model(input_tensor)\n",
        "\n",
        "    # Measure execution time\n",
        "    start_time = torch.cuda.Event(enable_timing=True)\n",
        "    end_time = torch.cuda.Event(enable_timing=True)\n",
        "    start_time.record()\n",
        "    _ = model(input_tensor)\n",
        "    end_time.record()\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    latency_ms = start_time.elapsed_time(end_time)\n",
        "    return latency_ms\n",
        "\n",
        "\n",
        "def train(model, optimizer, loss_fn, train_loader, val_loader, test_loader, load_model_name, epochs=20, device=\"cpu\", log_dir='tensorboard_logs', patience=5, early_stopping_patience=10, classifier_type='MLP'):\n",
        "    ## Create folders and writer\n",
        "    # Create a directory to store TensorBoard logs\n",
        "    # log_dir = 'tensorboard_logs'\n",
        "\n",
        "    # Create a TensorBoard SummaryWriter\n",
        "    # load_model_name = \"resnet50\"\n",
        "    model_name = \"best_model_\" + load_model_name + \".pth\"\n",
        "\n",
        "    current_datetime = datetime.datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n",
        "    unique_folder_name = f\"{current_datetime}_{load_model_name}\"\n",
        "    unique_log_dir = os.path.join(log_dir, unique_folder_name)\n",
        "    model_path = os.path.join(unique_log_dir, model_name)\n",
        "\n",
        "    layout = {\n",
        "        \"Train and validation at same time\": {\n",
        "            \"Loss\": [\"Multiline\", [\"Loss/Train\", \"Loss/Validation\"]],\n",
        "            \"Accuracy\": [\"Multiline\", [\"Accuracy/Train\", \"Accuracy/Validation\"]],\n",
        "        },\n",
        "    }\n",
        "\n",
        "    writer = SummaryWriter(log_dir=unique_log_dir)\n",
        "    writer.add_custom_scalars(layout)\n",
        "\n",
        "    ## use early_stopping and scheduler learning rate\n",
        "    early_stopping = EarlyStopping(patience=early_stopping_patience, verbose=True)\n",
        "    lr_scheduler = LearningRateScheduler(optimizer, patience=patience, factor=0.1, verbose=True)\n",
        "\n",
        "    ## start training\n",
        "    best_valid_accuracy = 0.0\n",
        "    best_model_state = None\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        training_loss = 0.0\n",
        "        valid_loss = 0.0\n",
        "        model.train()\n",
        "\n",
        "        # Initialize variables for train accuracy calculation\n",
        "        num_correct_train = 0\n",
        "        num_examples_train = 0\n",
        "\n",
        "        for batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            inputs, targets = batch\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "            output = model(inputs)\n",
        "            loss = loss_fn(output, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            training_loss += loss.data.item() * inputs.size(0)\n",
        "\n",
        "            # Calculate the number of correct predictions in the current batch\n",
        "            correct = torch.eq(torch.max(F.softmax(output, dim=1), dim=1)[1], targets).view(-1)\n",
        "            num_correct_train += torch.sum(correct).item()\n",
        "            num_examples_train += correct.shape[0]\n",
        "\n",
        "        training_loss /= len(train_loader.dataset)\n",
        "        train_accuracy = num_correct_train / num_examples_train\n",
        "\n",
        "        model.eval()\n",
        "        num_correct = 0\n",
        "        num_examples = 0\n",
        "\n",
        "        for batch in val_loader:\n",
        "            inputs, targets = batch\n",
        "            inputs = inputs.to(device)\n",
        "            output = model(inputs)\n",
        "            targets = targets.to(device)\n",
        "            loss = loss_fn(output, targets)\n",
        "            valid_loss += loss.data.item() * inputs.size(0)\n",
        "            correct = torch.eq(torch.max(F.softmax(output, dim=1), dim=1)[1], targets).view(-1)\n",
        "\n",
        "            num_correct += torch.sum(correct).item()\n",
        "            num_examples += correct.shape[0]\n",
        "\n",
        "        valid_loss /= len(val_loader.dataset)\n",
        "        valid_accuracy = num_correct / num_examples\n",
        "\n",
        "        # Get the current learning rate from the optimizer\n",
        "        current_lr = lr_scheduler.step(valid_accuracy)\n",
        "\n",
        "        print('Epoch: {}, Learning Rate: {}, Training Loss: {:.2f}, Training Accuracy: {:.4f}, Validation Loss: {:.2f}, Validation Accuracy: {:.4f}'.format(epoch, current_lr, training_loss, train_accuracy, valid_loss, valid_accuracy))\n",
        "\n",
        "        # Log training accuracy to TensorBoard\n",
        "        writer.add_scalar('Learning Rate', current_lr, epoch)\n",
        "        writer.add_scalar('Loss/Train', training_loss, epoch)\n",
        "        writer.add_scalar('Accuracy/Train', train_accuracy, epoch)\n",
        "        writer.add_scalar('Loss/Validation', valid_loss, epoch)\n",
        "        writer.add_scalar('Accuracy/Validation', valid_accuracy, epoch)\n",
        "\n",
        "        early_stop = early_stopping.step(valid_accuracy)\n",
        "        if early_stop:\n",
        "            break  # Stop training if early stopping is activated\n",
        "\n",
        "        # Save the best model based on validation accuracy\n",
        "        if valid_accuracy > best_valid_accuracy:\n",
        "            best_valid_accuracy = valid_accuracy\n",
        "            best_model_state = model.state_dict()\n",
        "            # Save the best model state to a file\n",
        "            torch.save(best_model_state, model_path)\n",
        "\n",
        "    print(f\"\\n Model has been saved to {model_path} \\n\")\n",
        "\n",
        "    # Inspect the model\n",
        "    writer.add_graph(model, inputs)\n",
        "    writer.add_figure('Confusion Matrix', plot_confusion_matrix(model, val_loader, device))\n",
        "    # writer.add_figure(f\"Predictions vs. Actuals\", log_predictions_vs_actuals(model, val_loader, device=device, num_batches=1))\n",
        "\n",
        "    # Add hyperparameters to TensorBoard\n",
        "    hyperparameters = {\n",
        "        'Feature Extractor': load_model_name,\n",
        "        'Model Accuracy': best_valid_accuracy,\n",
        "        'Params (M)': sum(p.numel() for p in model.parameters()) / 1e6,  # Convert to million parameters\n",
        "        'Size of model (MB)': os.path.getsize(model_path) / (1024 * 1024),  # Size in MB\n",
        "        'Latency of model (ms)': estimate_latency(model, device),  # Calculate latency with a dummy input\n",
        "        'Classifier type': classifier_type,\n",
        "        'Training type': 'Normal',\n",
        "    }\n",
        "\n",
        "    writer.add_hparams(hparam_dict=hyperparameters, metric_dict={})\n",
        "    # Print hyperparameters with .4f\n",
        "    for key, value in hyperparameters.items():\n",
        "        if isinstance(value, float):\n",
        "            print(f'{key}: {value:.4f}')\n",
        "        else:\n",
        "            print(f'{key}: {value}')\n",
        "\n",
        "    # Close the TensorBoard SummaryWriter\n",
        "    writer.close()\n",
        "\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    test_accuracy = evaluate(model, test_loader, device=device)\n",
        "    print(f\"\\n Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "--BuoBbH4_mu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load model"
      ],
      "metadata": {
        "id": "3mIL6u3M6ANI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load teacher model\n",
        "model = models.resnet18(pretrained=False)\n",
        "# Replace the last fully connected layer\n",
        "num_features = model.fc.in_features\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Linear(num_features, 500),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(),\n",
        "    nn.Linear(500, 50)\n",
        ")\n",
        "model_path = 'tensorboard_logs/2023-12-02_120014_resnet18/best_model_resnet18.pth'\n",
        "model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "model.to(device)\n",
        "print(f'model from {model_path} loaded')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58164b44-7205-495b-ac26-0fc1fcc60c7c",
        "id": "9m7J-c_96ANJ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model from tensorboard_logs/2023-12-02_120014_resnet18/best_model_resnet18.pth loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## resnet50 trained model"
      ],
      "metadata": {
        "id": "9sVnye1cH5uM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_size(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lNCNq9DH9Vy",
        "outputId": "424f57cd-9213-47fc-8459-970e9ea62b4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "93.88278198242188"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for n, p in model.named_parameters():\n",
        "  print(n, \": \", p.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juR2BhwaIEg5",
        "outputId": "85ca7950-12cb-4aae-d826-50c76434f61e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conv1.weight :  torch.float32\n",
            "bn1.weight :  torch.float32\n",
            "bn1.bias :  torch.float32\n",
            "layer1.0.conv1.weight :  torch.float32\n",
            "layer1.0.bn1.weight :  torch.float32\n",
            "layer1.0.bn1.bias :  torch.float32\n",
            "layer1.0.conv2.weight :  torch.float32\n",
            "layer1.0.bn2.weight :  torch.float32\n",
            "layer1.0.bn2.bias :  torch.float32\n",
            "layer1.0.conv3.weight :  torch.float32\n",
            "layer1.0.bn3.weight :  torch.float32\n",
            "layer1.0.bn3.bias :  torch.float32\n",
            "layer1.0.downsample.0.weight :  torch.float32\n",
            "layer1.0.downsample.1.weight :  torch.float32\n",
            "layer1.0.downsample.1.bias :  torch.float32\n",
            "layer1.1.conv1.weight :  torch.float32\n",
            "layer1.1.bn1.weight :  torch.float32\n",
            "layer1.1.bn1.bias :  torch.float32\n",
            "layer1.1.conv2.weight :  torch.float32\n",
            "layer1.1.bn2.weight :  torch.float32\n",
            "layer1.1.bn2.bias :  torch.float32\n",
            "layer1.1.conv3.weight :  torch.float32\n",
            "layer1.1.bn3.weight :  torch.float32\n",
            "layer1.1.bn3.bias :  torch.float32\n",
            "layer1.2.conv1.weight :  torch.float32\n",
            "layer1.2.bn1.weight :  torch.float32\n",
            "layer1.2.bn1.bias :  torch.float32\n",
            "layer1.2.conv2.weight :  torch.float32\n",
            "layer1.2.bn2.weight :  torch.float32\n",
            "layer1.2.bn2.bias :  torch.float32\n",
            "layer1.2.conv3.weight :  torch.float32\n",
            "layer1.2.bn3.weight :  torch.float32\n",
            "layer1.2.bn3.bias :  torch.float32\n",
            "layer2.0.conv1.weight :  torch.float32\n",
            "layer2.0.bn1.weight :  torch.float32\n",
            "layer2.0.bn1.bias :  torch.float32\n",
            "layer2.0.conv2.weight :  torch.float32\n",
            "layer2.0.bn2.weight :  torch.float32\n",
            "layer2.0.bn2.bias :  torch.float32\n",
            "layer2.0.conv3.weight :  torch.float32\n",
            "layer2.0.bn3.weight :  torch.float32\n",
            "layer2.0.bn3.bias :  torch.float32\n",
            "layer2.0.downsample.0.weight :  torch.float32\n",
            "layer2.0.downsample.1.weight :  torch.float32\n",
            "layer2.0.downsample.1.bias :  torch.float32\n",
            "layer2.1.conv1.weight :  torch.float32\n",
            "layer2.1.bn1.weight :  torch.float32\n",
            "layer2.1.bn1.bias :  torch.float32\n",
            "layer2.1.conv2.weight :  torch.float32\n",
            "layer2.1.bn2.weight :  torch.float32\n",
            "layer2.1.bn2.bias :  torch.float32\n",
            "layer2.1.conv3.weight :  torch.float32\n",
            "layer2.1.bn3.weight :  torch.float32\n",
            "layer2.1.bn3.bias :  torch.float32\n",
            "layer2.2.conv1.weight :  torch.float32\n",
            "layer2.2.bn1.weight :  torch.float32\n",
            "layer2.2.bn1.bias :  torch.float32\n",
            "layer2.2.conv2.weight :  torch.float32\n",
            "layer2.2.bn2.weight :  torch.float32\n",
            "layer2.2.bn2.bias :  torch.float32\n",
            "layer2.2.conv3.weight :  torch.float32\n",
            "layer2.2.bn3.weight :  torch.float32\n",
            "layer2.2.bn3.bias :  torch.float32\n",
            "layer2.3.conv1.weight :  torch.float32\n",
            "layer2.3.bn1.weight :  torch.float32\n",
            "layer2.3.bn1.bias :  torch.float32\n",
            "layer2.3.conv2.weight :  torch.float32\n",
            "layer2.3.bn2.weight :  torch.float32\n",
            "layer2.3.bn2.bias :  torch.float32\n",
            "layer2.3.conv3.weight :  torch.float32\n",
            "layer2.3.bn3.weight :  torch.float32\n",
            "layer2.3.bn3.bias :  torch.float32\n",
            "layer3.0.conv1.weight :  torch.float32\n",
            "layer3.0.bn1.weight :  torch.float32\n",
            "layer3.0.bn1.bias :  torch.float32\n",
            "layer3.0.conv2.weight :  torch.float32\n",
            "layer3.0.bn2.weight :  torch.float32\n",
            "layer3.0.bn2.bias :  torch.float32\n",
            "layer3.0.conv3.weight :  torch.float32\n",
            "layer3.0.bn3.weight :  torch.float32\n",
            "layer3.0.bn3.bias :  torch.float32\n",
            "layer3.0.downsample.0.weight :  torch.float32\n",
            "layer3.0.downsample.1.weight :  torch.float32\n",
            "layer3.0.downsample.1.bias :  torch.float32\n",
            "layer3.1.conv1.weight :  torch.float32\n",
            "layer3.1.bn1.weight :  torch.float32\n",
            "layer3.1.bn1.bias :  torch.float32\n",
            "layer3.1.conv2.weight :  torch.float32\n",
            "layer3.1.bn2.weight :  torch.float32\n",
            "layer3.1.bn2.bias :  torch.float32\n",
            "layer3.1.conv3.weight :  torch.float32\n",
            "layer3.1.bn3.weight :  torch.float32\n",
            "layer3.1.bn3.bias :  torch.float32\n",
            "layer3.2.conv1.weight :  torch.float32\n",
            "layer3.2.bn1.weight :  torch.float32\n",
            "layer3.2.bn1.bias :  torch.float32\n",
            "layer3.2.conv2.weight :  torch.float32\n",
            "layer3.2.bn2.weight :  torch.float32\n",
            "layer3.2.bn2.bias :  torch.float32\n",
            "layer3.2.conv3.weight :  torch.float32\n",
            "layer3.2.bn3.weight :  torch.float32\n",
            "layer3.2.bn3.bias :  torch.float32\n",
            "layer3.3.conv1.weight :  torch.float32\n",
            "layer3.3.bn1.weight :  torch.float32\n",
            "layer3.3.bn1.bias :  torch.float32\n",
            "layer3.3.conv2.weight :  torch.float32\n",
            "layer3.3.bn2.weight :  torch.float32\n",
            "layer3.3.bn2.bias :  torch.float32\n",
            "layer3.3.conv3.weight :  torch.float32\n",
            "layer3.3.bn3.weight :  torch.float32\n",
            "layer3.3.bn3.bias :  torch.float32\n",
            "layer3.4.conv1.weight :  torch.float32\n",
            "layer3.4.bn1.weight :  torch.float32\n",
            "layer3.4.bn1.bias :  torch.float32\n",
            "layer3.4.conv2.weight :  torch.float32\n",
            "layer3.4.bn2.weight :  torch.float32\n",
            "layer3.4.bn2.bias :  torch.float32\n",
            "layer3.4.conv3.weight :  torch.float32\n",
            "layer3.4.bn3.weight :  torch.float32\n",
            "layer3.4.bn3.bias :  torch.float32\n",
            "layer3.5.conv1.weight :  torch.float32\n",
            "layer3.5.bn1.weight :  torch.float32\n",
            "layer3.5.bn1.bias :  torch.float32\n",
            "layer3.5.conv2.weight :  torch.float32\n",
            "layer3.5.bn2.weight :  torch.float32\n",
            "layer3.5.bn2.bias :  torch.float32\n",
            "layer3.5.conv3.weight :  torch.float32\n",
            "layer3.5.bn3.weight :  torch.float32\n",
            "layer3.5.bn3.bias :  torch.float32\n",
            "layer4.0.conv1.weight :  torch.float32\n",
            "layer4.0.bn1.weight :  torch.float32\n",
            "layer4.0.bn1.bias :  torch.float32\n",
            "layer4.0.conv2.weight :  torch.float32\n",
            "layer4.0.bn2.weight :  torch.float32\n",
            "layer4.0.bn2.bias :  torch.float32\n",
            "layer4.0.conv3.weight :  torch.float32\n",
            "layer4.0.bn3.weight :  torch.float32\n",
            "layer4.0.bn3.bias :  torch.float32\n",
            "layer4.0.downsample.0.weight :  torch.float32\n",
            "layer4.0.downsample.1.weight :  torch.float32\n",
            "layer4.0.downsample.1.bias :  torch.float32\n",
            "layer4.1.conv1.weight :  torch.float32\n",
            "layer4.1.bn1.weight :  torch.float32\n",
            "layer4.1.bn1.bias :  torch.float32\n",
            "layer4.1.conv2.weight :  torch.float32\n",
            "layer4.1.bn2.weight :  torch.float32\n",
            "layer4.1.bn2.bias :  torch.float32\n",
            "layer4.1.conv3.weight :  torch.float32\n",
            "layer4.1.bn3.weight :  torch.float32\n",
            "layer4.1.bn3.bias :  torch.float32\n",
            "layer4.2.conv1.weight :  torch.float32\n",
            "layer4.2.bn1.weight :  torch.float32\n",
            "layer4.2.bn1.bias :  torch.float32\n",
            "layer4.2.conv2.weight :  torch.float32\n",
            "layer4.2.bn2.weight :  torch.float32\n",
            "layer4.2.bn2.bias :  torch.float32\n",
            "layer4.2.conv3.weight :  torch.float32\n",
            "layer4.2.bn3.weight :  torch.float32\n",
            "layer4.2.bn3.bias :  torch.float32\n",
            "fc.0.weight :  torch.float32\n",
            "fc.0.bias :  torch.float32\n",
            "fc.3.weight :  torch.float32\n",
            "fc.3.bias :  torch.float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## pytorch"
      ],
      "metadata": {
        "id": "bPeDg8qtZ3Gg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Post-Training Dynamic/Weight-only Quantization"
      ],
      "metadata": {
        "id": "Hb1Vfkq9Ej8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Post-Training Dynamic/Weight-only Quantization\n",
        "'''\n",
        "torch.quantization.quantize_dynamic. Currently only Linear and Recurrent (LSTM, GRU, RNN)\n",
        "'''\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "model.eval()\n",
        "\n",
        "## EAGER MODE\n",
        "from torch.quantization import quantize_dynamic\n",
        "model_quantized = quantize_dynamic(\n",
        "    model=model, qconfig_spec={nn.LSTM, nn.Linear}, dtype=torch.qint8, inplace=False\n",
        ")\n",
        "\n",
        "## FX MODE\n",
        "from torch.quantization import quantize_fx\n",
        "qconfig_dict = {\"\": torch.quantization.default_dynamic_qconfig}  # An empty key denotes the default applied to all modules\n",
        "example_inputs = (torch.randn(1, 3, 224, 224),)\n",
        "model_prepared = quantize_fx.prepare_fx(model, qconfig_dict, example_inputs)\n",
        "model_quantized = quantize_fx.convert_fx(model_prepared)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xuNvYkWZ4O_",
        "outputId": "5bcb984f-7452-43d3-d121-183dfa43b19d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/fx/prepare.py:1755: UserWarning: Passing a QConfig dictionary to prepare is deprecated and will not be supported in a future version. Please pass in a QConfigMapping instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_size_of_model(model, label=\"\"):\n",
        "    torch.save(model.state_dict(), \"temp.p\")\n",
        "    size=os.path.getsize(\"temp.p\")\n",
        "    print(\"model: \",label,' \\t','Size (MB):', size/1e6)\n",
        "    os.remove('temp.p')\n",
        "    return size\n",
        "\n",
        "print_size_of_model(model, 'float32')\n",
        "print_size_of_model(model_quantized, 'Dynamic/Weight-only Quantization')\n",
        "\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kuZ2VTzFa9ZL",
        "outputId": "623a6d5b-1d99-4c47-d46f-bd9a0cffa20f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model:  float32  \t Size (MB): 98.526044\n",
            "model:  Dynamic/Weight-only Quantization  \t Size (MB): 95.010924\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(model_quantized, esc50_test_loader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzYqGiqFdOkg",
        "outputId": "4af6da13-cf54-491d-9d5c-9172a5e82846"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8675"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Post-Training Static Quantization (PTQ) 1 - resnet18"
      ],
      "metadata": {
        "id": "vG4nupKk8Gmv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "## FX GRAPH\n",
        "from torch.quantization import quantize_fx\n",
        "\n",
        "backend = \"fbgemm\"\n",
        "m = copy.deepcopy(model)\n",
        "m.eval()\n",
        "qconfig_dict = {\"\": torch.quantization.get_default_qconfig(backend)}\n",
        "# Prepare\n",
        "example_inputs = (torch.randn(1, 3, 224, 224),)\n",
        "model_prepared = quantize_fx.prepare_fx(m, qconfig_dict, example_inputs)\n",
        "# Calibrate - Use representative (validation) data.\n",
        "# with torch.inference_mode():\n",
        "#   for _ in range(10):\n",
        "#     x = torch.rand(1,2,28, 28)\n",
        "#     model_prepared(x)\n",
        "\n",
        "model_prepared.to(torch.device(\"cpu:0\"))\n",
        "with torch.inference_mode():\n",
        "    for batch in esc50_test_loader:\n",
        "        inputs, targets = batch\n",
        "        inputs = inputs.to('cpu')\n",
        "        model_prepared(inputs)\n",
        "# quantize\n",
        "model_quantized = quantize_fx.convert_fx(model_prepared)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e58d43e-cccc-474b-c58d-743231bfcce0",
        "id": "5IZqEgTc8Gmx"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/fx/prepare.py:1755: UserWarning: Passing a QConfig dictionary to prepare is deprecated and will not be supported in a future version. Please pass in a QConfigMapping instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(model_quantized, esc50_test_loader, 'cpu')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df396b5a-a62c-4dbb-e9dc-93e4cca659c8",
        "id": "SGa2Hc-B8Gmx"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8525"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_size_of_model(model_quantized, 'Post-Training Static Quantization (PTQ)')\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d137dee-d633-4087-d177-87f19de0e78f",
        "id": "5o4Cv1kn8Gmy"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model:  Post-Training Static Quantization (PTQ)  \t Size (MB): 11.593976\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# Prepare input data (replace this with your actual input data)\n",
        "input_data = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "# Warm-up the model (optional but recommended)\n",
        "warmup_iterations = 10\n",
        "for _ in range(warmup_iterations):\n",
        "    _ = model_quantized(input_data)\n",
        "\n",
        "# Measure inference time\n",
        "num_iterations = 100  # Adjust as needed\n",
        "total_time = 0.0\n",
        "\n",
        "for _ in range(num_iterations):\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        _ = model_quantized(input_data)\n",
        "    end_time = time.time()\n",
        "\n",
        "    iteration_time = end_time - start_time\n",
        "    total_time += iteration_time\n",
        "\n",
        "average_latency = total_time / num_iterations\n",
        "\n",
        "print(f\"Average Latency: {(average_latency * 1000):.2f} ms seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96c79ec8-de18-484f-ee33-522af6865a4e",
        "id": "sfhKezQ78Gmy"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Latency: 33.08 ms seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# Prepare input data (replace this with your actual input data)\n",
        "input_data = torch.randn(1, 3, 224, 224).to(torch.device('cuda'))\n",
        "\n",
        "# Warm-up the model (optional but recommended)\n",
        "warmup_iterations = 10\n",
        "for _ in range(warmup_iterations):\n",
        "    _ = model(input_data)\n",
        "\n",
        "# Measure inference time\n",
        "num_iterations = 100  # Adjust as needed\n",
        "total_time = 0.0\n",
        "\n",
        "for _ in range(num_iterations):\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        _ = model(input_data)\n",
        "    end_time = time.time()\n",
        "\n",
        "    iteration_time = end_time - start_time\n",
        "    total_time += iteration_time\n",
        "\n",
        "average_latency = total_time / num_iterations\n",
        "\n",
        "print(f\"Average Latency: {(average_latency * 1000):.2f} ms seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd993fd5-a1d4-45d1-d34f-681c5f8c1862",
        "id": "K19aCIFm8Gmz"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Latency: 12.83 ms seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quantization Aware Training with FX Graph Mode"
      ],
      "metadata": {
        "id": "K4usn86EdK3f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epoches = 25\n",
        "patience = 10\n",
        "early_stopping_patience=15\n",
        "load_model_name = \"resnet18_quantized\"\n",
        "classifier_type='MLP'\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "h0iK1pUHW21u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sample site\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "from torch.ao.quantization import get_default_qat_qconfig_mapping\n",
        "from torch.ao.quantization.quantize_fx import prepare_qat_fx, convert_fx\n",
        "\n",
        "import torch.onnx\n",
        "\n",
        "import time\n",
        "import copy\n",
        "import numpy as np\n",
        "\n",
        "from torchvision.models import resnet18\n",
        "\n",
        "def prepare_dataloader(num_workers=8, train_batch_size=128, eval_batch_size=256):\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
        "    ])\n",
        "\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
        "    ])\n",
        "\n",
        "    train_set = torchvision.datasets.CIFAR10(root=\"data\", train=True, download=True, transform=train_transform)\n",
        "    test_set = torchvision.datasets.CIFAR10(root=\"data\", train=False, download=True, transform=test_transform)\n",
        "\n",
        "    train_sampler = torch.utils.data.RandomSampler(train_set)\n",
        "    test_sampler = torch.utils.data.SequentialSampler(test_set)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        dataset=train_set, batch_size=train_batch_size,\n",
        "        sampler=train_sampler, num_workers=num_workers)\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        dataset=test_set, batch_size=eval_batch_size,\n",
        "        sampler=test_sampler, num_workers=num_workers)\n",
        "\n",
        "    return train_loader, test_loader\n",
        "\n",
        "def evaluate_model(model, test_loader, device, criterion=None):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    running_loss = 0\n",
        "    running_corrects = 0\n",
        "\n",
        "    for inputs, labels in test_loader:\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        if criterion is not None:\n",
        "            loss = criterion(outputs, labels).item()\n",
        "        else:\n",
        "            loss = 0\n",
        "\n",
        "        # statistics\n",
        "        running_loss += loss * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    eval_loss = running_loss / len(test_loader.dataset)\n",
        "    eval_accuracy = running_corrects / len(test_loader.dataset)\n",
        "\n",
        "    return eval_loss, eval_accuracy\n",
        "\n",
        "def train_model(model, train_loader, test_loader, device, learning_rate=1e-1, num_epochs=200):\n",
        "    # The training configurations were not carefully selected.\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    # It seems that SGD optimizer is better than Adam optimizer for ResNet18 training on CIFAR10.\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-4)\n",
        "    # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=500)\n",
        "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100, 150], gamma=0.1, last_epoch=-1)\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    eval_loss, eval_accuracy = evaluate_model(model=model, test_loader=test_loader, device=device, criterion=criterion)\n",
        "    print(\"Epoch: {:02d} Eval Loss: {:.3f} Eval Acc: {:.3f}\".format(-1, eval_loss, eval_accuracy))\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        # Training\n",
        "        model.train()\n",
        "\n",
        "        running_loss = 0\n",
        "        running_corrects = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # statistics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        train_loss = running_loss / len(train_loader.dataset)\n",
        "        train_accuracy = running_corrects / len(train_loader.dataset)\n",
        "\n",
        "        # Evaluation\n",
        "        model.eval()\n",
        "        eval_loss, eval_accuracy = evaluate_model(model=model, test_loader=test_loader, device=device, criterion=criterion)\n",
        "\n",
        "        # Set learning rate scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        print(\"Epoch: {:03d} Train Loss: {:.3f} Train Acc: {:.3f} Eval Loss: {:.3f} Eval Acc: {:.3f}\".format(epoch, train_loss, train_accuracy, eval_loss, eval_accuracy))\n",
        "\n",
        "    return model\n",
        "\n",
        "def measure_inference_latency(model,\n",
        "                              device,\n",
        "                              input_size=(1, 3, 32, 32),\n",
        "                              num_samples=100,\n",
        "                              num_warmups=10):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    x = torch.rand(size=input_size).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_warmups):\n",
        "            _ = model(x)\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        start_time = time.time()\n",
        "        for _ in range(num_samples):\n",
        "            _ = model(x)\n",
        "            torch.cuda.synchronize()\n",
        "        end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_time_ave = elapsed_time / num_samples\n",
        "\n",
        "    return elapsed_time_ave\n",
        "\n",
        "def save_model(model, model_dir, model_filename):\n",
        "    if not os.path.exists(model_dir):\n",
        "        os.makedirs(model_dir)\n",
        "    model_filepath = os.path.join(model_dir, model_filename)\n",
        "    torch.save(model.state_dict(), model_filepath)\n",
        "\n",
        "def load_model(model, model_filepath, device):\n",
        "    model.load_state_dict(torch.load(model_filepath, map_location=device))\n",
        "    return model\n",
        "\n",
        "def save_torchscript_model(model, model_dir, model_filename):\n",
        "    if not os.path.exists(model_dir):\n",
        "        os.makedirs(model_dir)\n",
        "    model_filepath = os.path.join(model_dir, model_filename)\n",
        "    torch.jit.save(torch.jit.script(model), model_filepath)\n",
        "\n",
        "def load_torchscript_model(model_filepath, device):\n",
        "    model = torch.jit.load(model_filepath, map_location=device)\n",
        "    return model\n",
        "\n",
        "def model_equivalence(model_1, model_2, device, rtol=1e-05, atol=1e-08, num_tests=100, input_size=(1,3,32,32)):\n",
        "    model_1.to(device)\n",
        "    model_2.to(device)\n",
        "\n",
        "    for _ in range(num_tests):\n",
        "        x = torch.rand(size=input_size).to(device)\n",
        "        y1 = model_1(x).detach().cpu().numpy()\n",
        "        y2 = model_2(x).detach().cpu().numpy()\n",
        "        if np.allclose(a=y1, b=y2, rtol=rtol, atol=atol, equal_nan=False) == False:\n",
        "            print(\"Model equivalence test sample failed: \")\n",
        "            print(y1)\n",
        "            print(y2)\n",
        "            return False\n",
        "    return True"
      ],
      "metadata": {
        "id": "ESMCPclvPtKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# num_classes = 10\n",
        "cuda_device = torch.device(\"cuda:0\")\n",
        "cpu_device = torch.device(\"cpu:0\")\n",
        "\n",
        "model_dir = \"saved_models\"\n",
        "model_filename = \"resnet18.pt\"\n",
        "prepared_model_filename = \"resnet18_prepared_model.pt\"\n",
        "quantized_model_filename = \"resnet18_quantized.pt\"\n",
        "model_filepath = os.path.join(model_dir, model_filename)\n",
        "quantized_model_filepath = os.path.join(model_dir, quantized_model_filename)"
      ],
      "metadata": {
        "id": "Jq2uphHXyYUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Create an untrained model.\n",
        "# model = model = resnet18(num_classes=num_classes, pretrained=False)\n",
        "\n",
        "# train_loader, test_loader = prepare_dataloader(num_workers=8, train_batch_size=128, eval_batch_size=256)\n",
        "\n",
        "# Train model.\n",
        "# print(\"Training Model...\")\n",
        "# model = train_model(model=model, train_loader=train_loader, test_loader=test_loader, device=cuda_device, learning_rate=1e-1, num_epochs=5)\n",
        "# Save model.\n",
        "save_model(model=model, model_dir=model_dir, model_filename=model_filename)\n",
        "\n",
        "# Prepare a model for quantization aware training\n",
        "model.to(cpu_device)\n",
        "model_to_quantize = copy.deepcopy(model)\n",
        "qconfig_mapping = get_default_qat_qconfig_mapping(\"fbgemm\")\n",
        "example_inputs = torch.rand(size=(1,3,224,224)).to(cpu_device)\n",
        "prepared_model = prepare_qat_fx(model_to_quantize, qconfig_mapping, example_inputs)\n",
        "\n",
        "# Print FP32 model.\n",
        "# print(model)\n",
        "# Print fused model.\n",
        "# print(prepared_model)\n",
        "\n",
        "# Model and fused model should be equivalent.\n",
        "model.eval()\n",
        "prepared_model.eval()\n",
        "assert model_equivalence(model_1=model, model_2=prepared_model, device=cpu_device, rtol=1e-01, atol=3, num_tests=100, input_size=(1,3,224,224)), \"Fused model is not equivalent to the original model!\"\n",
        "\n",
        "# Quantization aware training\n",
        "print(\"Training QAT Model...\")\n",
        "prepared_model.train()\n",
        "prepared_model.to(cuda_device)\n",
        "# train_model(model=prepared_model, train_loader=train_loader, test_loader=test_loader, device=cuda_device, learning_rate=1e-3, num_epochs=5)\n",
        "optimizer = optim.Adam([\n",
        "                        {'params': prepared_model.conv1.parameters(), 'lr': 1e-4},\n",
        "                        {'params': prepared_model.layer1.parameters(), 'lr': 1e-4},\n",
        "                        {'params': prepared_model.layer2.parameters(), 'lr': 1e-4},\n",
        "                        {'params': prepared_model.layer3.parameters(), 'lr': 1e-4},\n",
        "                        {'params': prepared_model.layer4.parameters(), 'lr': 1e-4},\n",
        "                        {'params': prepared_model.fc.parameters(), 'lr': 1e-8}\n",
        "                        ], lr=1e-2)\n",
        "\n",
        "\n",
        "prepared_model = train(prepared_model, optimizer, loss_fn, esc50_train_loader, esc50_val_loader, esc50_test_loader, load_model_name, epochs=epoches, device=device,\n",
        "      log_dir = 'tensorboard_logs', patience=patience, early_stopping_patience=early_stopping_patience, classifier_type=classifier_type)\n",
        "prepared_model.to(cpu_device)\n",
        "\n",
        "print('evaluate: ', evaluate(prepared_model, esc50_test_loader, cpu_device))\n",
        "\n",
        "# Save model.\n",
        "save_model(model=prepared_model, model_dir=model_dir, model_filename=prepared_model_filename)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aPNaILDXz9K",
        "outputId": "cccd2ce8-2d74-43ab-b924-43ed9ee73321"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:110: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
            "  torch.has_cuda,\n",
            "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:111: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'\n",
            "  torch.has_cudnn,\n",
            "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:117: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
            "  torch.has_mps,\n",
            "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:118: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'\n",
            "  torch.has_mkldnn,\n",
            "/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training QAT Model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/fake_quantize.py:343: UserWarning: _aminmax is deprecated as of PyTorch 1.11 and will be removed in a future release. Use aminmax instead. This warning will only appear once per process. (Triggered internally at ../aten/src/ATen/native/ReduceAllOps.cpp:72.)\n",
            "  return torch.fused_moving_avg_obs_fake_quant(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/fake_quantize.py:343: UserWarning: _aminmax is deprecated as of PyTorch 1.11 and will be removed in a future release. Use aminmax instead. This warning will only appear once per process. (Triggered internally at ../aten/src/ATen/native/TensorCompare.cpp:677.)\n",
            "  return torch.fused_moving_avg_obs_fake_quant(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Learning Rate: 0.0001, Training Loss: 2.66, Training Accuracy: 0.9000, Validation Loss: 2.02, Validation Accuracy: 0.7775\n",
            "Epoch: 2, Learning Rate: 0.0001, Training Loss: 1.10, Training Accuracy: 0.9831, Validation Loss: 1.45, Validation Accuracy: 0.8025\n",
            "Epoch: 3, Learning Rate: 0.0001, Training Loss: 0.71, Training Accuracy: 0.9944, Validation Loss: 1.23, Validation Accuracy: 0.8225\n",
            "Epoch: 4, Learning Rate: 0.0001, Training Loss: 0.60, Training Accuracy: 0.9969, Validation Loss: 1.21, Validation Accuracy: 0.8150\n",
            "Epoch: 5, Learning Rate: 0.0001, Training Loss: 0.53, Training Accuracy: 0.9962, Validation Loss: 1.22, Validation Accuracy: 0.8000\n",
            "Epoch: 6, Learning Rate: 0.0001, Training Loss: 0.48, Training Accuracy: 0.9988, Validation Loss: 1.20, Validation Accuracy: 0.8175\n",
            "Epoch: 7, Learning Rate: 0.0001, Training Loss: 0.46, Training Accuracy: 1.0000, Validation Loss: 1.14, Validation Accuracy: 0.8325\n",
            "Epoch: 8, Learning Rate: 0.0001, Training Loss: 0.44, Training Accuracy: 0.9994, Validation Loss: 1.20, Validation Accuracy: 0.8000\n",
            "Epoch: 9, Learning Rate: 0.0001, Training Loss: 0.41, Training Accuracy: 0.9994, Validation Loss: 1.11, Validation Accuracy: 0.8200\n",
            "Epoch: 10, Learning Rate: 0.0001, Training Loss: 0.39, Training Accuracy: 0.9994, Validation Loss: 1.15, Validation Accuracy: 0.8225\n",
            "Epoch: 11, Learning Rate: 0.0001, Training Loss: 0.38, Training Accuracy: 0.9994, Validation Loss: 1.04, Validation Accuracy: 0.8375\n",
            "Epoch 00012: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch 00012: reducing learning rate of group 1 to 1.0000e-05.\n",
            "Epoch 00012: reducing learning rate of group 2 to 1.0000e-05.\n",
            "Epoch 00012: reducing learning rate of group 3 to 1.0000e-05.\n",
            "Epoch 00012: reducing learning rate of group 4 to 1.0000e-05.\n",
            "Epoch: 12, Learning Rate: 1e-05, Training Loss: 0.37, Training Accuracy: 0.9975, Validation Loss: 1.10, Validation Accuracy: 0.8150\n",
            "Epoch: 13, Learning Rate: 1e-05, Training Loss: 0.34, Training Accuracy: 0.9994, Validation Loss: 1.02, Validation Accuracy: 0.8400\n",
            "Epoch: 14, Learning Rate: 1e-05, Training Loss: 0.31, Training Accuracy: 1.0000, Validation Loss: 1.03, Validation Accuracy: 0.8425\n",
            "Epoch: 15, Learning Rate: 1e-05, Training Loss: 0.31, Training Accuracy: 1.0000, Validation Loss: 1.01, Validation Accuracy: 0.8400\n",
            "Epoch: 16, Learning Rate: 1e-05, Training Loss: 0.30, Training Accuracy: 1.0000, Validation Loss: 1.02, Validation Accuracy: 0.8425\n",
            "Epoch: 17, Learning Rate: 1e-05, Training Loss: 0.30, Training Accuracy: 0.9994, Validation Loss: 1.01, Validation Accuracy: 0.8375\n",
            "Epoch: 18, Learning Rate: 1e-05, Training Loss: 0.28, Training Accuracy: 1.0000, Validation Loss: 1.00, Validation Accuracy: 0.8475\n",
            "Epoch: 19, Learning Rate: 1e-05, Training Loss: 0.30, Training Accuracy: 1.0000, Validation Loss: 1.02, Validation Accuracy: 0.8450\n",
            "Epoch: 20, Learning Rate: 1e-05, Training Loss: 0.29, Training Accuracy: 1.0000, Validation Loss: 1.01, Validation Accuracy: 0.8450\n",
            "Epoch: 21, Learning Rate: 1e-05, Training Loss: 0.28, Training Accuracy: 1.0000, Validation Loss: 1.02, Validation Accuracy: 0.8300\n",
            "Epoch: 22, Learning Rate: 1e-05, Training Loss: 0.28, Training Accuracy: 1.0000, Validation Loss: 1.02, Validation Accuracy: 0.8500\n",
            "Epoch 00023: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Epoch 00023: reducing learning rate of group 1 to 1.0000e-06.\n",
            "Epoch 00023: reducing learning rate of group 2 to 1.0000e-06.\n",
            "Epoch 00023: reducing learning rate of group 3 to 1.0000e-06.\n",
            "Epoch 00023: reducing learning rate of group 4 to 1.0000e-06.\n",
            "Epoch: 23, Learning Rate: 1.0000000000000002e-06, Training Loss: 0.28, Training Accuracy: 1.0000, Validation Loss: 1.00, Validation Accuracy: 0.8400\n",
            "Epoch: 24, Learning Rate: 1.0000000000000002e-06, Training Loss: 0.27, Training Accuracy: 1.0000, Validation Loss: 1.01, Validation Accuracy: 0.8400\n",
            "Epoch: 25, Learning Rate: 1.0000000000000002e-06, Training Loss: 0.28, Training Accuracy: 1.0000, Validation Loss: 1.01, Validation Accuracy: 0.8350\n",
            "\n",
            " Model has been saved to tensorboard_logs/2023-12-04_114300_resnet18_quantized/best_model_resnet18_quantized.pth \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/jit/_trace.py:1093: TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function. Detailed error:\n",
            "Tensor-likes are not close!\n",
            "\n",
            "Mismatched elements: 764 / 800 (95.5%)\n",
            "Greatest absolute difference: 0.1461324691772461 at index (6, 35) (up to 1e-05 allowed)\n",
            "Greatest relative difference: inf at index (2, 9) (up to 1e-05 allowed)\n",
            "  _check_trace(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Extractor: resnet18_quantized\n",
            "Model Accuracy: 0.8500\n",
            "Params (M): 11.4581\n",
            "Size of model (MB): 44.0067\n",
            "Latency of model (ms): 25.7802\n",
            "Classifier type: MLP\n",
            "Training type: Normal\n",
            "\n",
            " Test Accuracy: 84.25%\n",
            "evaluate:  0.8325\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert trained model to quantized model\n",
        "quantized_model = convert_fx(prepared_model)\n",
        "\n",
        "quantized_model.eval()\n",
        "# Save quantized model.\n",
        "save_torchscript_model(model=quantized_model, model_dir=model_dir, model_filename=quantized_model_filename)"
      ],
      "metadata": {
        "id": "I0ql9Fg55-p0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dad82d2-44c3-4fbc-aad2-468760f49894"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load quantized model.\n",
        "quantized_jit_model = load_torchscript_model(model_filepath=quantized_model_filepath, device=cpu_device)\n",
        "\n",
        "fp32_eval_accuracy = evaluate(model, esc50_test_loader, device=cpu_device)\n",
        "int8_eval_accuracy = evaluate(quantized_jit_model, esc50_test_loader, device=cpu_device)\n",
        "\n",
        "print(\"FP32 evaluation accuracy: {:.3f}\".format(fp32_eval_accuracy))\n",
        "print(\"INT8 evaluation accuracy: {:.3f}\".format(int8_eval_accuracy))\n"
      ],
      "metadata": {
        "id": "Dbkfme_kYBdT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8db27c1-0a32-4103-a41e-1c8d36c3d770"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FP32 evaluation accuracy: 0.835\n",
            "INT8 evaluation accuracy: 0.825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fp32_cpu_inference_latency = measure_inference_latency(model=model, device=cpu_device, input_size=(1,3,224,224), num_samples=100)\n",
        "int8_cpu_inference_latency = measure_inference_latency(model=quantized_model, device=cpu_device, input_size=(1,3,224,224), num_samples=100)\n",
        "int8_jit_cpu_inference_latency = measure_inference_latency(model=quantized_jit_model, device=cpu_device, input_size=(1,3,224,224), num_samples=100)\n",
        "fp32_gpu_inference_latency = measure_inference_latency(model=model, device=cuda_device, input_size=(1,3,224,224), num_samples=100)\n",
        "\n",
        "print(\"FP32 CPU Inference Latency: {:.2f} ms / sample\".format(fp32_cpu_inference_latency * 1000))\n",
        "print(\"FP32 CUDA Inference Latency: {:.2f} ms / sample\".format(fp32_gpu_inference_latency * 1000))\n",
        "print(\"INT8 CPU Inference Latency: {:.2f} ms / sample\".format(int8_cpu_inference_latency * 1000))\n",
        "print(\"INT8 JIT CPU Inference Latency: {:.2f} ms / sample\".format(int8_jit_cpu_inference_latency * 1000))"
      ],
      "metadata": {
        "id": "WYtJLzKxiSEF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29e7c0df-b620-4b6e-c82a-2b66abdc2e7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FP32 CPU Inference Latency: 81.72 ms / sample\n",
            "FP32 CUDA Inference Latency: 3.86 ms / sample\n",
            "INT8 CPU Inference Latency: 34.76 ms / sample\n",
            "INT8 JIT CPU Inference Latency: 45.23 ms / sample\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_size_of_model(model, 'model')\n",
        "print_size_of_model(prepared_model, 'prepared_model')\n",
        "print_size_of_model(quantized_model, 'quantized_model')\n",
        "print()"
      ],
      "metadata": {
        "id": "hMsuiQ9tpDXV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e82a9642-5f70-4161-e79f-4adf276e66a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model:  model  \t Size (MB): 45.90489\n",
            "model:  prepared_model  \t Size (MB): 46.004802\n",
            "model:  quantized_model  \t Size (MB): 11.593976\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}